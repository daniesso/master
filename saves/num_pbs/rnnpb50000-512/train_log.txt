2019-06-26 02:20:46.156612: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-26 02:20:46.165156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-26 02:20:46.165971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-26 02:20:46.166103: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 02:20:46.167306: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-26 02:20:46.168602: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-26 02:20:46.168862: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-26 02:20:46.170198: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-26 02:20:46.171459: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-26 02:20:46.174423: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 02:20:46.177277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
Starting training procedure.
Loading training set...
2019-06-26 02:20:46.985203: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-06-26 02:20:47.307643: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x34e5730 executing computations on platform CUDA. Devices:
2019-06-26 02:20:47.307698: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-26 02:20:47.333021: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2019-06-26 02:20:47.336097: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x34dbbe0 executing computations on platform Host. Devices:
2019-06-26 02:20:47.336132: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-06-26 02:20:47.337324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-26 02:20:47.337380: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 02:20:47.337389: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-26 02:20:47.337396: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-26 02:20:47.337402: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-26 02:20:47.337409: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-26 02:20:47.337416: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-26 02:20:47.337424: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 02:20:47.339062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 1
2019-06-26 02:20:47.339090: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 02:20:47.340890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-26 02:20:47.340904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      1 
2019-06-26 02:20:47.340909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N 
2019-06-26 02:20:47.342932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30071 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
Saving vocab defined by training set...
Loading development set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


=== Model ===
Name: rnnpbnmt
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0
Num PBs: 512
Bind hard: True
Binding strength: 1.0
Autoencode: False
PB learning rate: 0.01
Sigma: 0
p_reset: 0
Max recog epochs: 100


=== Training ===
Max epochs: 0
Early stopping steps: 20
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-06-26 02:20:52.119436: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 02:20:53.344614: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0626 02:20:53.653663 139653492778816 deprecation.py:323] From /lhome/daniesso/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 58.2065
  Batch 100 Loss 38.9512
  Batch 200 Loss 33.9471
  Batch 300 Loss 35.5037
  Batch 400 Loss 30.4486
  Batch 500 Loss 27.8301
  Batch 600 Loss 31.4680
  Batch 700 Loss 26.5904
Finished epoch 1 in 58.0 seconds
Perplexity training: 81.060
Measuring development set...
Recognition iteration 0 Loss 28.338
Recognition finished, iteration 100 Loss 26.010
Recognition iteration 0 Loss 29.337
Recognition finished, iteration 100 Loss 26.363
Recognition iteration 0 Loss 26.567
Recognition finished, iteration 100 Loss 23.949
Recognition iteration 0 Loss 29.509
Recognition finished, iteration 100 Loss 26.760
Perplexity dev: 35.742

==== Starting epoch 2 ====
  Batch 0 Loss 25.3762
  Batch 100 Loss 27.3956
  Batch 200 Loss 26.1317
  Batch 300 Loss 28.1871
  Batch 400 Loss 24.9836
  Batch 500 Loss 22.7757
  Batch 600 Loss 26.1305
  Batch 700 Loss 22.1269
Finished epoch 2 in 51.0 seconds
Perplexity training: 24.703

==== Starting epoch 3 ====
  Batch 0 Loss 22.9643
  Batch 100 Loss 23.3376
  Batch 200 Loss 22.3480
  Batch 300 Loss 24.8417
  Batch 400 Loss 21.2937
  Batch 500 Loss 19.8613
  Batch 600 Loss 23.0805
  Batch 700 Loss 18.7023
Finished epoch 3 in 51.0 seconds
Perplexity training: 16.569
Measuring development set...
Recognition iteration 0 Loss 28.338
Recognition finished, iteration 100 Loss 15.617
Recognition iteration 0 Loss 28.963
Recognition finished, iteration 100 Loss 15.274
Recognition iteration 0 Loss 25.854
Recognition finished, iteration 100 Loss 13.882
Recognition iteration 0 Loss 29.324
Recognition finished, iteration 100 Loss 15.468
Perplexity dev: 23.102

==== Starting epoch 4 ====
  Batch 0 Loss 18.7903
  Batch 100 Loss 19.9713
  Batch 200 Loss 19.2026
  Batch 300 Loss 21.7094
  Batch 400 Loss 18.5383
  Batch 500 Loss 17.0578
  Batch 600 Loss 20.0061
  Batch 700 Loss 15.8579
Finished epoch 4 in 51.0 seconds
Perplexity training: 11.360

==== Starting epoch 5 ====
  Batch 0 Loss 15.5583
  Batch 100 Loss 17.5105
  Batch 200 Loss 16.2271
  Batch 300 Loss 18.6123
  Batch 400 Loss 16.0406
  Batch 500 Loss 14.2388
  Batch 600 Loss 17.3116
  Batch 700 Loss 13.3546
Finished epoch 5 in 52.0 seconds
Perplexity training: 8.160
Measuring development set...
Recognition iteration 0 Loss 32.991
Recognition finished, iteration 100 Loss 10.135
Recognition iteration 0 Loss 33.692
Recognition finished, iteration 100 Loss 9.588
Recognition iteration 0 Loss 30.432
Recognition finished, iteration 100 Loss 8.603
Recognition iteration 0 Loss 33.881
Recognition finished, iteration 100 Loss 9.820
Perplexity dev: 6.761

==== Starting epoch 6 ====
  Batch 0 Loss 13.1053
  Batch 100 Loss 15.2478
  Batch 200 Loss 13.9154
  Batch 300 Loss 16.0400
  Batch 400 Loss 13.4409
  Batch 500 Loss 11.8897
  Batch 600 Loss 14.8494
  Batch 700 Loss 11.8736
Finished epoch 6 in 52.0 seconds
Perplexity training: 6.031

==== Starting epoch 7 ====
  Batch 0 Loss 10.5158
  Batch 100 Loss 12.7916
  Batch 200 Loss 12.0181
  Batch 300 Loss 14.1260
  Batch 400 Loss 11.5847
  Batch 500 Loss 10.5253
  Batch 600 Loss 12.8213
  Batch 700 Loss 9.8266
Finished epoch 7 in 52.0 seconds
Perplexity training: 4.734
Measuring development set...
Recognition iteration 0 Loss 40.366
Recognition finished, iteration 100 Loss 6.652
Recognition iteration 0 Loss 41.144
Recognition finished, iteration 100 Loss 5.951
Recognition iteration 0 Loss 35.654
Recognition finished, iteration 100 Loss 5.327
Recognition iteration 0 Loss 41.095
Recognition finished, iteration 100 Loss 6.210
Perplexity dev: 5.136

==== Starting epoch 8 ====
  Batch 0 Loss 8.5417
  Batch 100 Loss 10.6917
  Batch 200 Loss 10.5774
  Batch 300 Loss 12.3111
  Batch 400 Loss 10.1409
  Batch 500 Loss 9.1323
  Batch 600 Loss 11.0952
  Batch 700 Loss 8.6291
Finished epoch 8 in 51.0 seconds
Perplexity training: 3.772

==== Starting epoch 9 ====
  Batch 0 Loss 7.1706
  Batch 100 Loss 9.3063
  Batch 200 Loss 8.8758
  Batch 300 Loss 11.1047
  Batch 400 Loss 8.5478
  Batch 500 Loss 7.3404
  Batch 600 Loss 9.7563
  Batch 700 Loss 6.9647
Finished epoch 9 in 52.0 seconds
Perplexity training: 3.098
Measuring development set...
Recognition iteration 0 Loss 45.992
Recognition finished, iteration 100 Loss 4.635
Recognition iteration 0 Loss 46.506
Recognition finished, iteration 100 Loss 3.958
Recognition iteration 0 Loss 41.158
Recognition finished, iteration 100 Loss 3.469
Recognition iteration 0 Loss 47.420
Recognition finished, iteration 100 Loss 4.177
Perplexity dev: 4.985

==== Starting epoch 10 ====
  Batch 0 Loss 6.0448
  Batch 100 Loss 7.5435
  Batch 200 Loss 7.6574
  Batch 300 Loss 9.4540
  Batch 400 Loss 7.3230
  Batch 500 Loss 6.3825
  Batch 600 Loss 8.2621
  Batch 700 Loss 5.9479
Finished epoch 10 in 52.0 seconds
Perplexity training: 2.598

==== Starting epoch 11 ====
  Batch 0 Loss 5.1536
  Batch 100 Loss 6.4201
  Batch 200 Loss 6.6745
  Batch 300 Loss 8.0027
  Batch 400 Loss 6.0018
  Batch 500 Loss 5.3613
  Batch 600 Loss 6.6587
  Batch 700 Loss 4.9870
Finished epoch 11 in 52.0 seconds
Perplexity training: 2.222
Measuring development set...
Recognition iteration 0 Loss 50.104
Recognition finished, iteration 100 Loss 2.998
Recognition iteration 0 Loss 50.661
Recognition finished, iteration 100 Loss 2.636
Recognition iteration 0 Loss 45.788
Recognition finished, iteration 100 Loss 2.414
Recognition iteration 0 Loss 52.636
Recognition finished, iteration 100 Loss 2.796
Perplexity dev: 5.384

==== Starting epoch 12 ====
  Batch 0 Loss 4.0867
  Batch 100 Loss 5.4723
  Batch 200 Loss 5.6394
  Batch 300 Loss 7.2621
  Batch 400 Loss 5.4482
  Batch 500 Loss 4.3127
  Batch 600 Loss 5.6931
  Batch 700 Loss 4.4626
Finished epoch 12 in 52.0 seconds
Perplexity training: 1.935

==== Starting epoch 13 ====
  Batch 0 Loss 3.1980
  Batch 100 Loss 4.6581
  Batch 200 Loss 4.5030
  Batch 300 Loss 6.4037
  Batch 400 Loss 4.6502
  Batch 500 Loss 3.5972
  Batch 600 Loss 4.6991
  Batch 700 Loss 3.5736
Finished epoch 13 in 53.0 seconds
Perplexity training: 1.718
Measuring development set...
Recognition iteration 0 Loss 56.376
Recognition finished, iteration 100 Loss 2.199
Recognition iteration 0 Loss 57.650
Recognition finished, iteration 100 Loss 1.900
Recognition iteration 0 Loss 50.544
Recognition finished, iteration 100 Loss 1.615
Recognition iteration 0 Loss 58.508
Recognition finished, iteration 100 Loss 2.062
Perplexity dev: 7.623

==== Starting epoch 14 ====
  Batch 0 Loss 2.4246
  Batch 100 Loss 3.8246
  Batch 200 Loss 3.5460
  Batch 300 Loss 5.2996
  Batch 400 Loss 3.8725
  Batch 500 Loss 2.8672
  Batch 600 Loss 3.7162
  Batch 700 Loss 2.7213
Finished epoch 14 in 54.0 seconds
Perplexity training: 1.551

==== Starting epoch 15 ====
  Batch 0 Loss 1.8886
  Batch 100 Loss 3.2801
  Batch 200 Loss 2.8621
  Batch 300 Loss 4.2714
  Batch 400 Loss 3.2037
  Batch 500 Loss 2.2298
  Batch 600 Loss 2.9195
  Batch 700 Loss 2.1382
Finished epoch 15 in 53.0 seconds
Perplexity training: 1.424
Measuring development set...
Recognition iteration 0 Loss 62.476
Recognition finished, iteration 100 Loss 1.644
Recognition iteration 0 Loss 63.750
Recognition finished, iteration 100 Loss 1.556
Recognition iteration 0 Loss 56.796
Recognition finished, iteration 100 Loss 1.355
Recognition iteration 0 Loss 65.437
Recognition finished, iteration 100 Loss 1.731
Perplexity dev: 11.326

==== Starting epoch 16 ====
  Batch 0 Loss 1.5305
  Batch 100 Loss 2.8417
  Batch 200 Loss 2.2413
  Batch 300 Loss 3.4283
  Batch 400 Loss 2.6419
  Batch 500 Loss 1.7476
  Batch 600 Loss 2.4726
  Batch 700 Loss 1.6598
Finished epoch 16 in 54.0 seconds
Perplexity training: 1.325

==== Starting epoch 17 ====
  Batch 0 Loss 1.0813
  Batch 100 Loss 2.1758
  Batch 200 Loss 1.8940
  Batch 300 Loss 2.9571
  Batch 400 Loss 2.2555
  Batch 500 Loss 1.2201
  Batch 600 Loss 1.9715
  Batch 700 Loss 1.2723
Finished epoch 17 in 54.0 seconds
Perplexity training: 1.247
Measuring development set...
Recognition iteration 0 Loss 67.937
Recognition finished, iteration 100 Loss 1.489
Recognition iteration 0 Loss 70.434
Recognition finished, iteration 100 Loss 1.432
Recognition iteration 0 Loss 62.259
Recognition finished, iteration 100 Loss 1.237
Recognition iteration 0 Loss 71.518
Recognition finished, iteration 100 Loss 1.646
Perplexity dev: 18.469

==== Starting epoch 18 ====
  Batch 0 Loss 0.7367
  Batch 100 Loss 1.8099
  Batch 200 Loss 1.6116
  Batch 300 Loss 2.4010
  Batch 400 Loss 1.7179
  Batch 500 Loss 0.8592
  Batch 600 Loss 1.5255
  Batch 700 Loss 0.9880
Finished epoch 18 in 54.0 seconds
Perplexity training: 1.186

==== Starting epoch 19 ====
  Batch 0 Loss 0.5030
  Batch 100 Loss 1.5033
  Batch 200 Loss 1.1845
  Batch 300 Loss 1.9076
  Batch 400 Loss 1.1786
  Batch 500 Loss 0.6489
  Batch 600 Loss 1.2006
  Batch 700 Loss 0.7296
Finished epoch 19 in 54.0 seconds
Perplexity training: 1.138
Measuring development set...
Recognition iteration 0 Loss 72.427
Recognition finished, iteration 100 Loss 1.071
Recognition iteration 0 Loss 74.489
Recognition finished, iteration 100 Loss 1.103
Recognition iteration 0 Loss 66.807
Recognition finished, iteration 100 Loss 0.959
Recognition iteration 0 Loss 75.806
Recognition finished, iteration 100 Loss 1.115
Perplexity dev: 54.428

==== Starting epoch 20 ====
  Batch 0 Loss 0.4000
  Batch 100 Loss 1.2261
  Batch 200 Loss 0.8681
  Batch 300 Loss 1.3352
  Batch 400 Loss 0.8922
  Batch 500 Loss 0.5253
  Batch 600 Loss 0.9521
  Batch 700 Loss 0.5554
Finished epoch 20 in 54.0 seconds
Perplexity training: 1.102

==== Starting epoch 21 ====
  Batch 0 Loss 0.3123
  Batch 100 Loss 0.9051
  Batch 200 Loss 0.6674
  Batch 300 Loss 0.9584
  Batch 400 Loss 0.6513
  Batch 500 Loss 0.3997
  Batch 600 Loss 0.7004
  Batch 700 Loss 0.4140
Finished epoch 21 in 55.0 seconds
Perplexity training: 1.075
Measuring development set...
Recognition iteration 0 Loss 79.451
Recognition finished, iteration 100 Loss 1.186
Recognition iteration 0 Loss 81.351
Recognition finished, iteration 100 Loss 1.233
Recognition iteration 0 Loss 72.864
Recognition finished, iteration 100 Loss 1.082
Recognition iteration 0 Loss 81.996
Recognition finished, iteration 100 Loss 1.043
Perplexity dev: 82.150

==== Starting epoch 22 ====
  Batch 0 Loss 0.3039
  Batch 100 Loss 0.6597
  Batch 200 Loss 0.4617
  Batch 300 Loss 0.7513
  Batch 400 Loss 0.4693
  Batch 500 Loss 0.3396
  Batch 600 Loss 0.5127
  Batch 700 Loss 0.3158
Finished epoch 22 in 56.0 seconds
Perplexity training: 1.055

==== Starting epoch 23 ====
  Batch 0 Loss 0.1967
  Batch 100 Loss 0.5042
  Batch 200 Loss 0.3605
  Batch 300 Loss 0.5226
  Batch 400 Loss 0.3144
  Batch 500 Loss 0.2216
  Batch 600 Loss 0.3643
  Batch 700 Loss 0.2160
Finished epoch 23 in 55.0 seconds
Perplexity training: 1.040
Measuring development set...
Recognition iteration 0 Loss 83.131
Recognition finished, iteration 100 Loss 0.923
Recognition iteration 0 Loss 84.877
Recognition finished, iteration 100 Loss 0.972
Recognition iteration 0 Loss 75.093
Recognition finished, iteration 100 Loss 1.060
Recognition iteration 0 Loss 85.532
Recognition finished, iteration 100 Loss 0.828
Perplexity dev: 101.849

==== Starting epoch 24 ====
  Batch 0 Loss 0.1515
  Batch 100 Loss 0.3860
  Batch 200 Loss 0.2642
  Batch 300 Loss 0.3852
  Batch 400 Loss 0.2582
  Batch 500 Loss 0.2144
  Batch 600 Loss 0.2727
  Batch 700 Loss 0.1450
Finished epoch 24 in 55.0 seconds
Perplexity training: 1.030

==== Starting epoch 25 ====
  Batch 0 Loss 0.1095
  Batch 100 Loss 0.2806
  Batch 200 Loss 0.2117
  Batch 300 Loss 0.4340
  Batch 400 Loss 0.2080
  Batch 500 Loss 0.1702
  Batch 600 Loss 0.2473
  Batch 700 Loss 0.1284
Finished epoch 25 in 59.0 seconds
Perplexity training: 1.023
Measuring development set...
Recognition iteration 0 Loss 87.420
Recognition finished, iteration 100 Loss 0.835
Recognition iteration 0 Loss 89.760
Recognition finished, iteration 100 Loss 1.125
Recognition iteration 0 Loss 78.719
Recognition finished, iteration 100 Loss 0.844
Recognition iteration 0 Loss 90.295
Recognition finished, iteration 100 Loss 0.957
Perplexity dev: 107.073

==== Starting epoch 26 ====
  Batch 0 Loss 0.0783
  Batch 100 Loss 0.2039
  Batch 200 Loss 0.1717
  Batch 300 Loss 0.2797
  Batch 400 Loss 0.1868
  Batch 500 Loss 0.1036
  Batch 600 Loss 0.1763
  Batch 700 Loss 0.1310
Finished epoch 26 in 58.0 seconds
Perplexity training: 1.017

==== Starting epoch 27 ====
  Batch 0 Loss 0.0696
  Batch 100 Loss 0.1400
  Batch 200 Loss 0.1436
  Batch 300 Loss 0.1789
  Batch 400 Loss 0.1454
  Batch 500 Loss 0.0833
  Batch 600 Loss 0.2028
  Batch 700 Loss 0.0724
Finished epoch 27 in 58.0 seconds
Perplexity training: 1.013
Measuring development set...
Recognition iteration 0 Loss 92.489
Recognition finished, iteration 100 Loss 0.978
Recognition iteration 0 Loss 94.871
Recognition finished, iteration 100 Loss 1.057
Recognition iteration 0 Loss 82.872
Recognition finished, iteration 100 Loss 0.815
Recognition iteration 0 Loss 95.904
Recognition finished, iteration 100 Loss 0.920
Perplexity dev: 3692.700

==== Starting epoch 28 ====
  Batch 0 Loss 0.0475
  Batch 100 Loss 0.1011
  Batch 200 Loss 0.0868
  Batch 300 Loss 0.1283
  Batch 400 Loss 0.1144
  Batch 500 Loss 0.0624
  Batch 600 Loss 0.1245
  Batch 700 Loss 0.1046
Finished epoch 28 in 58.0 seconds
Perplexity training: 1.010

==== Starting epoch 29 ====
  Batch 0 Loss 0.0521
  Batch 100 Loss 0.0922
  Batch 200 Loss 0.0809
  Batch 300 Loss 0.1100
  Batch 400 Loss 0.0869
  Batch 500 Loss 0.0616
  Batch 600 Loss 0.0999
  Batch 700 Loss 0.1131
Finished epoch 29 in 60.0 seconds
Perplexity training: 1.008
Measuring development set...
Recognition iteration 0 Loss 94.697
Recognition finished, iteration 100 Loss 0.910
Recognition iteration 0 Loss 96.326
Recognition finished, iteration 100 Loss 0.923
Recognition iteration 0 Loss 85.123
Recognition finished, iteration 100 Loss 0.873
Recognition iteration 0 Loss 97.760
Recognition finished, iteration 100 Loss 0.764
Perplexity dev: 378.909
Finished training in 1728.83 seconds
Finished training after development set stopped improving.
