Starting training procedure.
Loading training set...
2019-06-26 02:19:55.019453: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-26 02:19:56.520481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-26 02:19:56.521270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-26 02:19:56.521463: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 02:19:56.522696: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-26 02:19:56.523892: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-26 02:19:56.524159: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-26 02:19:56.525437: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-26 02:19:56.526630: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-26 02:19:56.529519: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 02:19:56.532555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-06-26 02:19:56.533033: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-06-26 02:19:57.150966: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c8b8d0 executing computations on platform CUDA. Devices:
2019-06-26 02:19:57.151018: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-26 02:19:57.151025: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-26 02:19:57.173036: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2019-06-26 02:19:57.176453: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1bda210 executing computations on platform Host. Devices:
2019-06-26 02:19:57.176492: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-06-26 02:19:57.180703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-26 02:19:57.181633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-26 02:19:57.181677: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 02:19:57.181687: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-26 02:19:57.181695: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-26 02:19:57.181702: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-26 02:19:57.181710: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-26 02:19:57.181717: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-26 02:19:57.181736: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 02:19:57.185107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-06-26 02:19:57.185147: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 02:19:57.187372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-26 02:19:57.187387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 
2019-06-26 02:19:57.187393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y 
2019-06-26 02:19:57.187397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N 
2019-06-26 02:19:57.190835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30458 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
2019-06-26 02:19:57.192093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30458 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
Saving vocab defined by training set...
Loading development set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


=== Model ===
Name: rnnpbnmt
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0
Num PBs: 64
Bind hard: True
Binding strength: 1.0
Autoencode: False
PB learning rate: 0.01
Sigma: 0
p_reset: 0
Max recog epochs: 100


=== Training ===
Max epochs: 0
Early stopping steps: 20
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-06-26 02:20:02.066828: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 02:20:03.325620: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0626 02:20:03.633831 140082244527936 deprecation.py:323] From /lhome/daniesso/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 60.2426
  Batch 100 Loss 38.2197
  Batch 200 Loss 35.1759
  Batch 300 Loss 35.7980
  Batch 400 Loss 31.8687
  Batch 500 Loss 28.5913
  Batch 600 Loss 28.1129
  Batch 700 Loss 27.8110
Finished epoch 1 in 51.0 seconds
Perplexity training: 81.389
Measuring development set...
Recognition iteration 0 Loss 29.465
Recognition finished, iteration 100 Loss 28.469
Recognition iteration 0 Loss 25.679
Recognition finished, iteration 100 Loss 24.863
Recognition iteration 0 Loss 31.426
Recognition finished, iteration 100 Loss 30.397
Recognition iteration 0 Loss 27.328
Recognition finished, iteration 100 Loss 26.419
Perplexity dev: 35.526

==== Starting epoch 2 ====
  Batch 0 Loss 26.1453
  Batch 100 Loss 27.2611
  Batch 200 Loss 26.4707
  Batch 300 Loss 29.4791
  Batch 400 Loss 26.4232
  Batch 500 Loss 24.5432
  Batch 600 Loss 24.5154
  Batch 700 Loss 23.9636
Finished epoch 2 in 46.0 seconds
Perplexity training: 27.176

==== Starting epoch 3 ====
  Batch 0 Loss 23.8155
  Batch 100 Loss 24.4897
  Batch 200 Loss 23.8991
  Batch 300 Loss 26.7740
  Batch 400 Loss 23.8081
  Batch 500 Loss 22.2701
  Batch 600 Loss 21.9071
  Batch 700 Loss 21.2538
Finished epoch 3 in 46.0 seconds
Perplexity training: 20.142
Measuring development set...
Recognition iteration 0 Loss 27.649
Recognition finished, iteration 100 Loss 19.300
Recognition iteration 0 Loss 24.273
Recognition finished, iteration 100 Loss 16.866
Recognition iteration 0 Loss 29.542
Recognition finished, iteration 100 Loss 21.265
Recognition iteration 0 Loss 25.644
Recognition finished, iteration 100 Loss 17.786
Perplexity dev: 15.815

==== Starting epoch 4 ====
  Batch 0 Loss 22.0382
  Batch 100 Loss 21.5512
  Batch 200 Loss 21.1281
  Batch 300 Loss 23.7597
  Batch 400 Loss 21.0599
  Batch 500 Loss 19.6016
  Batch 600 Loss 19.4017
  Batch 700 Loss 19.0561
Finished epoch 4 in 48.0 seconds
Perplexity training: 14.634

==== Starting epoch 5 ====
  Batch 0 Loss 18.9461
  Batch 100 Loss 18.8513
  Batch 200 Loss 18.7694
  Batch 300 Loss 21.1342
  Batch 400 Loss 19.1727
  Batch 500 Loss 17.7139
  Batch 600 Loss 17.0530
  Batch 700 Loss 16.4841
Finished epoch 5 in 48.0 seconds
Perplexity training: 11.018
Measuring development set...
Recognition iteration 0 Loss 29.540
Recognition finished, iteration 100 Loss 14.616
Recognition iteration 0 Loss 26.744
Recognition finished, iteration 100 Loss 12.557
Recognition iteration 0 Loss 31.496
Recognition finished, iteration 100 Loss 16.268
Recognition iteration 0 Loss 29.111
Recognition finished, iteration 100 Loss 13.312
Perplexity dev: 8.990

==== Starting epoch 6 ====
  Batch 0 Loss 16.8254
  Batch 100 Loss 16.7992
  Batch 200 Loss 16.7618
  Batch 300 Loss 18.8759
  Batch 400 Loss 17.6760
  Batch 500 Loss 16.1242
  Batch 600 Loss 15.3820
  Batch 700 Loss 15.2742
Finished epoch 6 in 48.0 seconds
Perplexity training: 8.809

==== Starting epoch 7 ====
  Batch 0 Loss 14.8090
  Batch 100 Loss 15.4525
  Batch 200 Loss 14.7937
  Batch 300 Loss 16.6815
  Batch 400 Loss 15.7533
  Batch 500 Loss 14.0268
  Batch 600 Loss 14.0991
  Batch 700 Loss 14.4153
Finished epoch 7 in 49.0 seconds
Perplexity training: 7.144
Measuring development set...
Recognition iteration 0 Loss 34.212
Recognition finished, iteration 100 Loss 11.571
Recognition iteration 0 Loss 31.308
Recognition finished, iteration 100 Loss 9.837
Recognition iteration 0 Loss 36.340
Recognition finished, iteration 100 Loss 13.188
Recognition iteration 0 Loss 34.295
Recognition finished, iteration 100 Loss 10.282
Perplexity dev: 6.959

==== Starting epoch 8 ====
  Batch 0 Loss 13.3504
  Batch 100 Loss 14.1320
  Batch 200 Loss 13.2626
  Batch 300 Loss 15.0851
  Batch 400 Loss 13.9800
  Batch 500 Loss 12.5937
  Batch 600 Loss 12.8286
  Batch 700 Loss 12.6473
Finished epoch 8 in 49.0 seconds
Perplexity training: 5.915

==== Starting epoch 9 ====
  Batch 0 Loss 12.1805
  Batch 100 Loss 12.8147
  Batch 200 Loss 11.7567
  Batch 300 Loss 13.7959
  Batch 400 Loss 13.1329
  Batch 500 Loss 11.8763
  Batch 600 Loss 11.4841
  Batch 700 Loss 11.0409
Finished epoch 9 in 50.0 seconds
Perplexity training: 5.019
Measuring development set...
Recognition iteration 0 Loss 41.267
Recognition finished, iteration 100 Loss 9.502
Recognition iteration 0 Loss 36.967
Recognition finished, iteration 100 Loss 7.994
Recognition iteration 0 Loss 42.672
Recognition finished, iteration 100 Loss 11.097
Recognition iteration 0 Loss 40.713
Recognition finished, iteration 100 Loss 8.414
Perplexity dev: 6.209

==== Starting epoch 10 ====
  Batch 0 Loss 11.1456
  Batch 100 Loss 11.4165
  Batch 200 Loss 10.4957
  Batch 300 Loss 12.5059
  Batch 400 Loss 12.5324
  Batch 500 Loss 10.8607
  Batch 600 Loss 10.5830
  Batch 700 Loss 9.8063
Finished epoch 10 in 51.0 seconds
Perplexity training: 4.342

==== Starting epoch 11 ====
  Batch 0 Loss 10.0645
  Batch 100 Loss 9.8026
  Batch 200 Loss 9.7305
  Batch 300 Loss 11.5845
  Batch 400 Loss 10.8516
  Batch 500 Loss 9.6661
  Batch 600 Loss 9.8678
  Batch 700 Loss 8.9199
Finished epoch 11 in 51.0 seconds
Perplexity training: 3.811
Measuring development set...
Recognition iteration 0 Loss 45.307
Recognition finished, iteration 100 Loss 8.018
Recognition iteration 0 Loss 40.234
Recognition finished, iteration 100 Loss 6.905
Recognition iteration 0 Loss 47.120
Recognition finished, iteration 100 Loss 9.331
Recognition iteration 0 Loss 44.289
Recognition finished, iteration 100 Loss 6.925
Perplexity dev: 6.644

==== Starting epoch 12 ====
  Batch 0 Loss 9.1594
  Batch 100 Loss 8.5517
  Batch 200 Loss 9.2265
  Batch 300 Loss 10.9510
  Batch 400 Loss 9.8471
  Batch 500 Loss 8.8759
  Batch 600 Loss 9.2137
  Batch 700 Loss 8.3860
Finished epoch 12 in 50.0 seconds
Perplexity training: 3.389

==== Starting epoch 13 ====
  Batch 0 Loss 8.4569
  Batch 100 Loss 7.9792
  Batch 200 Loss 8.3877
  Batch 300 Loss 10.1729
  Batch 400 Loss 9.4253
  Batch 500 Loss 8.3251
  Batch 600 Loss 8.8174
  Batch 700 Loss 7.6284
Finished epoch 13 in 53.0 seconds
Perplexity training: 3.052
Measuring development set...
Recognition iteration 0 Loss 49.311
Recognition finished, iteration 100 Loss 7.027
Recognition iteration 0 Loss 44.811
Recognition finished, iteration 100 Loss 6.124
Recognition iteration 0 Loss 52.331
Recognition finished, iteration 100 Loss 8.300
Recognition iteration 0 Loss 48.648
Recognition finished, iteration 100 Loss 6.024
Perplexity dev: 6.651

==== Starting epoch 14 ====
  Batch 0 Loss 7.9455
  Batch 100 Loss 7.4149
  Batch 200 Loss 7.6215
  Batch 300 Loss 9.3272
  Batch 400 Loss 8.9020
  Batch 500 Loss 7.4475
  Batch 600 Loss 8.2136
  Batch 700 Loss 6.5721
Finished epoch 14 in 52.0 seconds
Perplexity training: 2.772

==== Starting epoch 15 ====
  Batch 0 Loss 7.5394
  Batch 100 Loss 6.4346
  Batch 200 Loss 7.1625
  Batch 300 Loss 8.5375
  Batch 400 Loss 7.9897
  Batch 500 Loss 6.7121
  Batch 600 Loss 7.4769
  Batch 700 Loss 5.9438
Finished epoch 15 in 52.0 seconds
Perplexity training: 2.533
Measuring development set...
Recognition iteration 0 Loss 55.034
Recognition finished, iteration 100 Loss 6.372
Recognition iteration 0 Loss 51.294
Recognition finished, iteration 100 Loss 6.062
Recognition iteration 0 Loss 57.980
Recognition finished, iteration 100 Loss 8.073
Recognition iteration 0 Loss 54.633
Recognition finished, iteration 100 Loss 5.537
Perplexity dev: 7.846

==== Starting epoch 16 ====
  Batch 0 Loss 6.7460
  Batch 100 Loss 5.7315
  Batch 200 Loss 6.6412
  Batch 300 Loss 8.2519
  Batch 400 Loss 7.3009
  Batch 500 Loss 5.9580
  Batch 600 Loss 6.8189
  Batch 700 Loss 5.4563
Finished epoch 16 in 53.0 seconds
Perplexity training: 2.331

==== Starting epoch 17 ====
  Batch 0 Loss 5.8009
  Batch 100 Loss 5.3283
  Batch 200 Loss 5.8270
  Batch 300 Loss 7.9549
  Batch 400 Loss 6.8991
  Batch 500 Loss 5.2647
  Batch 600 Loss 6.3705
  Batch 700 Loss 4.8206
Finished epoch 17 in 53.0 seconds
Perplexity training: 2.157
Measuring development set...
Recognition iteration 0 Loss 59.821
Recognition finished, iteration 100 Loss 6.094
Recognition iteration 0 Loss 54.713
Recognition finished, iteration 100 Loss 5.341
Recognition iteration 0 Loss 62.490
Recognition finished, iteration 100 Loss 7.576
Recognition iteration 0 Loss 59.075
Recognition finished, iteration 100 Loss 5.141
Perplexity dev: 7.848

==== Starting epoch 18 ====
  Batch 0 Loss 5.1957
  Batch 100 Loss 4.6292
  Batch 200 Loss 5.1343
  Batch 300 Loss 7.3952
  Batch 400 Loss 6.4889
  Batch 500 Loss 4.6887
  Batch 600 Loss 6.0137
  Batch 700 Loss 4.2186
Finished epoch 18 in 54.0 seconds
Perplexity training: 2.005

==== Starting epoch 19 ====
  Batch 0 Loss 4.8144
  Batch 100 Loss 3.9477
  Batch 200 Loss 4.5983
  Batch 300 Loss 6.5451
  Batch 400 Loss 5.9379
  Batch 500 Loss 4.3471
  Batch 600 Loss 5.3442
  Batch 700 Loss 3.6760
Finished epoch 19 in 54.0 seconds
Perplexity training: 1.868
Measuring development set...
Recognition iteration 0 Loss 63.862
Recognition finished, iteration 100 Loss 5.654
Recognition iteration 0 Loss 57.522
Recognition finished, iteration 100 Loss 5.234
Recognition iteration 0 Loss 66.082
Recognition finished, iteration 100 Loss 6.778
Recognition iteration 0 Loss 62.536
Recognition finished, iteration 100 Loss 4.738
Perplexity dev: 14.236

==== Starting epoch 20 ====
  Batch 0 Loss 4.5008
  Batch 100 Loss 3.6241
  Batch 200 Loss 4.2967
  Batch 300 Loss 5.7187
  Batch 400 Loss 5.1207
  Batch 500 Loss 4.1933
  Batch 600 Loss 4.6952
  Batch 700 Loss 3.1996
Finished epoch 20 in 54.0 seconds
Perplexity training: 1.749

==== Starting epoch 21 ====
  Batch 0 Loss 4.0452
  Batch 100 Loss 3.2301
  Batch 200 Loss 3.8428
  Batch 300 Loss 5.4206
  Batch 400 Loss 4.4314
  Batch 500 Loss 3.9239
  Batch 600 Loss 4.0146
  Batch 700 Loss 2.7941
Finished epoch 21 in 56.0 seconds
Perplexity training: 1.647
Measuring development set...
Recognition iteration 0 Loss 67.455
Recognition finished, iteration 100 Loss 5.680
Recognition iteration 0 Loss 60.716
Recognition finished, iteration 100 Loss 4.898
Recognition iteration 0 Loss 69.878
Recognition finished, iteration 100 Loss 6.685
Recognition iteration 0 Loss 66.385
Recognition finished, iteration 100 Loss 4.910
Perplexity dev: 22.188

==== Starting epoch 22 ====
  Batch 0 Loss 3.3879
  Batch 100 Loss 2.6658
  Batch 200 Loss 3.3219
  Batch 300 Loss 5.2426
  Batch 400 Loss 3.9399
  Batch 500 Loss 3.5012
  Batch 600 Loss 3.5456
  Batch 700 Loss 2.4782
Finished epoch 22 in 53.0 seconds
Perplexity training: 1.558

==== Starting epoch 23 ====
  Batch 0 Loss 2.9206
  Batch 100 Loss 2.2143
  Batch 200 Loss 2.9513
  Batch 300 Loss 4.8490
  Batch 400 Loss 3.5868
  Batch 500 Loss 2.9450
  Batch 600 Loss 3.2106
  Batch 700 Loss 2.1116
Finished epoch 23 in 55.0 seconds
Perplexity training: 1.478
Measuring development set...
Recognition iteration 0 Loss 70.109
Recognition finished, iteration 100 Loss 5.268
Recognition iteration 0 Loss 63.566
Recognition finished, iteration 100 Loss 4.693
Recognition iteration 0 Loss 74.117
Recognition finished, iteration 100 Loss 6.598
Recognition iteration 0 Loss 70.627
Recognition finished, iteration 100 Loss 4.583
Perplexity dev: 23.721

==== Starting epoch 24 ====
  Batch 0 Loss 2.6127
  Batch 100 Loss 1.9447
  Batch 200 Loss 2.7895
  Batch 300 Loss 4.1734
  Batch 400 Loss 3.2164
  Batch 500 Loss 2.5773
  Batch 600 Loss 2.9683
  Batch 700 Loss 1.8369
Finished epoch 24 in 53.0 seconds
Perplexity training: 1.408

==== Starting epoch 25 ====
  Batch 0 Loss 2.2694
  Batch 100 Loss 1.6481
  Batch 200 Loss 2.5838
  Batch 300 Loss 3.7217
  Batch 400 Loss 2.7990
  Batch 500 Loss 2.3545
  Batch 600 Loss 2.6464
  Batch 700 Loss 1.6069
Finished epoch 25 in 55.0 seconds
Perplexity training: 1.347
Measuring development set...
Recognition iteration 0 Loss 73.070
Recognition finished, iteration 100 Loss 5.051
Recognition iteration 0 Loss 67.290
Recognition finished, iteration 100 Loss 4.910
Recognition iteration 0 Loss 78.150
Recognition finished, iteration 100 Loss 6.633
Recognition iteration 0 Loss 74.490
Recognition finished, iteration 100 Loss 4.404
Perplexity dev: 62.155

==== Starting epoch 26 ====
  Batch 0 Loss 1.9621
  Batch 100 Loss 1.2858
  Batch 200 Loss 2.1786
  Batch 300 Loss 3.3774
  Batch 400 Loss 2.4374
  Batch 500 Loss 2.1671
  Batch 600 Loss 2.2565
  Batch 700 Loss 1.3900
Finished epoch 26 in 54.0 seconds
Perplexity training: 1.293

==== Starting epoch 27 ====
  Batch 0 Loss 1.6841
  Batch 100 Loss 1.0409
  Batch 200 Loss 1.7941
  Batch 300 Loss 3.1216
  Batch 400 Loss 2.1913
  Batch 500 Loss 2.0292
  Batch 600 Loss 1.8565
  Batch 700 Loss 1.1910
Finished epoch 27 in 56.0 seconds
Perplexity training: 1.246
Measuring development set...
Recognition iteration 0 Loss 77.388
Recognition finished, iteration 100 Loss 5.148
Recognition iteration 0 Loss 71.845
Recognition finished, iteration 100 Loss 4.773
Recognition iteration 0 Loss 82.009
Recognition finished, iteration 100 Loss 6.437
Recognition iteration 0 Loss 78.708
Recognition finished, iteration 100 Loss 4.425
Perplexity dev: 62.357

==== Starting epoch 28 ====
  Batch 0 Loss 1.4567
  Batch 100 Loss 0.8554
  Batch 200 Loss 1.4421
  Batch 300 Loss 2.8497
  Batch 400 Loss 1.9702
  Batch 500 Loss 1.7670
  Batch 600 Loss 1.6146
  Batch 700 Loss 0.9460
Finished epoch 28 in 55.0 seconds
Perplexity training: 1.206

==== Starting epoch 29 ====
  Batch 0 Loss 1.2177
  Batch 100 Loss 0.7343
  Batch 200 Loss 1.3149
  Batch 300 Loss 2.4426
  Batch 400 Loss 1.6633
  Batch 500 Loss 1.4902
  Batch 600 Loss 1.3736
  Batch 700 Loss 0.8109
Finished epoch 29 in 55.0 seconds
Perplexity training: 1.173
Measuring development set...
Recognition iteration 0 Loss 82.435
Recognition finished, iteration 100 Loss 5.822
Recognition iteration 0 Loss 75.774
Recognition finished, iteration 100 Loss 5.021
Recognition iteration 0 Loss 86.322
Recognition finished, iteration 100 Loss 7.322
Recognition iteration 0 Loss 82.880
Recognition finished, iteration 100 Loss 4.582
Perplexity dev: 98.627
Finished training in 1669.80 seconds
Finished training after development set stopped improving.
