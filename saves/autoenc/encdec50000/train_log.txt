Starting training procedure.
Loading training set...
2019-06-25 11:24:34.273993: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-25 11:24:34.704344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:24:34.707124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:00:05.0
2019-06-25 11:24:34.718585: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-25 11:24:35.058330: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-25 11:24:35.094504: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-25 11:24:35.106802: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-25 11:24:35.197991: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-25 11:24:35.246187: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-25 11:24:35.391802: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-25 11:24:35.391963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:24:35.392577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:24:35.393099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-06-25 11:24:35.394825: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-06-25 11:24:35.490255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:24:35.490881: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ce9a50 executing computations on platform CUDA. Devices:
2019-06-25 11:24:35.490903: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Quadro P4000, Compute Capability 6.1
2019-06-25 11:24:35.512066: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600020000 Hz
2019-06-25 11:24:35.512762: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c94dc0 executing computations on platform Host. Devices:
2019-06-25 11:24:35.512788: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-06-25 11:24:35.512978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:24:35.513499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:00:05.0
2019-06-25 11:24:35.513534: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-25 11:24:35.513545: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-25 11:24:35.513554: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-25 11:24:35.513574: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-25 11:24:35.513583: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-25 11:24:35.513591: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-25 11:24:35.513600: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-25 11:24:35.513636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:24:35.514095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:24:35.514548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-06-25 11:24:35.516177: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-25 11:24:35.517465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-25 11:24:35.517481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-06-25 11:24:35.517487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-06-25 11:24:35.518850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:24:35.519325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:24:35.520482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7629 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 0000:00:05.0, compute capability: 6.1)
Saving vocab defined by training set...
Loading development set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: True

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: True

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


=== Model ===
Name: encdec
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0


=== Training ===
Max epochs: 0
Early stopping steps: 10
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-06-25 11:24:43.892410: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-25 11:24:46.085156: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0625 11:24:46.699627 140199766603584 deprecation.py:323] From /home/paperspace/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 60.6923
  Batch 100 Loss 33.3968
  Batch 200 Loss 32.6018
  Batch 300 Loss 25.1333
  Batch 400 Loss 25.2252
  Batch 500 Loss 24.4305
  Batch 600 Loss 22.4755
  Batch 700 Loss 19.0821
Finished epoch 1 in 59.0 seconds
Perplexity training: 49.525
Measuring development set...
Perplexity dev: 12.059

==== Starting epoch 2 ====
  Batch 0 Loss 18.1610
  Batch 100 Loss 17.7011
  Batch 200 Loss 16.4442
  Batch 300 Loss 13.0251
  Batch 400 Loss 14.0302
  Batch 500 Loss 13.6850
  Batch 600 Loss 12.5866
  Batch 700 Loss 10.3758
Finished epoch 2 in 51.0 seconds
Perplexity training: 6.849

==== Starting epoch 3 ====
  Batch 0 Loss 9.9626
  Batch 100 Loss 10.2445
  Batch 200 Loss 9.7059
  Batch 300 Loss 7.5118
  Batch 400 Loss 8.2025
  Batch 500 Loss 8.2479
  Batch 600 Loss 7.7700
  Batch 700 Loss 6.0919
Finished epoch 3 in 52.0 seconds
Perplexity training: 3.099
Measuring development set...
Perplexity dev: 2.507

==== Starting epoch 4 ====
  Batch 0 Loss 5.7264
  Batch 100 Loss 6.0720
  Batch 200 Loss 5.8762
  Batch 300 Loss 4.3549
  Batch 400 Loss 4.9019
  Batch 500 Loss 4.9165
  Batch 600 Loss 4.8242
  Batch 700 Loss 3.6310
Finished epoch 4 in 56.0 seconds
Perplexity training: 1.917

==== Starting epoch 5 ====
  Batch 0 Loss 3.1921
  Batch 100 Loss 3.6224
  Batch 200 Loss 3.4690
  Batch 300 Loss 2.4711
  Batch 400 Loss 3.0002
  Batch 500 Loss 2.9039
  Batch 600 Loss 3.1320
  Batch 700 Loss 2.1891
Finished epoch 5 in 56.0 seconds
Perplexity training: 1.442
Measuring development set...
Perplexity dev: 1.420

==== Starting epoch 6 ====
  Batch 0 Loss 1.9342
  Batch 100 Loss 2.3241
  Batch 200 Loss 2.1019
  Batch 300 Loss 1.5133
  Batch 400 Loss 1.9579
  Batch 500 Loss 1.7977
  Batch 600 Loss 2.1587
  Batch 700 Loss 1.3816
Finished epoch 6 in 54.0 seconds
Perplexity training: 1.239

==== Starting epoch 7 ====
  Batch 0 Loss 1.1230
  Batch 100 Loss 1.5554
  Batch 200 Loss 1.3607
  Batch 300 Loss 0.9161
  Batch 400 Loss 1.2117
  Batch 500 Loss 1.3199
  Batch 600 Loss 1.4202
  Batch 700 Loss 0.7815
Finished epoch 7 in 55.0 seconds
Perplexity training: 1.141
Measuring development set...
Perplexity dev: 1.189

==== Starting epoch 8 ====
  Batch 0 Loss 0.6903
  Batch 100 Loss 1.0918
  Batch 200 Loss 0.9862
  Batch 300 Loss 0.6186
  Batch 400 Loss 0.7622
  Batch 500 Loss 1.1460
  Batch 600 Loss 1.0947
  Batch 700 Loss 0.6268
Finished epoch 8 in 53.0 seconds
Perplexity training: 1.092

==== Starting epoch 9 ====
  Batch 0 Loss 0.4326
  Batch 100 Loss 0.7149
  Batch 200 Loss 0.7050
  Batch 300 Loss 0.3992
  Batch 400 Loss 0.6176
  Batch 500 Loss 0.7925
  Batch 600 Loss 1.1824
  Batch 700 Loss 0.5366
Finished epoch 9 in 53.0 seconds
Perplexity training: 1.063
Measuring development set...
Perplexity dev: 1.117

==== Starting epoch 10 ====
  Batch 0 Loss 0.4686
  Batch 100 Loss 0.7872
  Batch 200 Loss 0.5374
  Batch 300 Loss 0.3182
  Batch 400 Loss 0.4254
  Batch 500 Loss 0.5175
  Batch 600 Loss 0.7851
  Batch 700 Loss 0.3619
Finished epoch 10 in 53.0 seconds
Perplexity training: 1.046

==== Starting epoch 11 ====
  Batch 0 Loss 0.2648
  Batch 100 Loss 0.5315
  Batch 200 Loss 0.4667
  Batch 300 Loss 0.1848
  Batch 400 Loss 0.3512
  Batch 500 Loss 0.4794
  Batch 600 Loss 0.6859
  Batch 700 Loss 0.2836
Finished epoch 11 in 53.0 seconds
Perplexity training: 1.034
Measuring development set...
Perplexity dev: 1.107

==== Starting epoch 12 ====
  Batch 0 Loss 0.2160
  Batch 100 Loss 0.5303
  Batch 200 Loss 0.3502
  Batch 300 Loss 0.0901
  Batch 400 Loss 0.1865
  Batch 500 Loss 0.4792
  Batch 600 Loss 0.5610
  Batch 700 Loss 0.1742
Finished epoch 12 in 53.0 seconds
Perplexity training: 1.027

==== Starting epoch 13 ====
  Batch 0 Loss 0.2073
  Batch 100 Loss 0.4693
  Batch 200 Loss 0.2295
  Batch 300 Loss 0.1038
  Batch 400 Loss 0.1299
  Batch 500 Loss 0.2913
  Batch 600 Loss 0.5741
  Batch 700 Loss 0.1755
Finished epoch 13 in 54.0 seconds
Perplexity training: 1.022
Measuring development set...
Perplexity dev: 1.083

==== Starting epoch 14 ====
  Batch 0 Loss 0.1214
  Batch 100 Loss 0.3450
  Batch 200 Loss 0.1651
  Batch 300 Loss 0.0627
  Batch 400 Loss 0.1668
  Batch 500 Loss 0.3101
  Batch 600 Loss 0.3771
  Batch 700 Loss 0.1534
Finished epoch 14 in 53.0 seconds
Perplexity training: 1.018

==== Starting epoch 15 ====
  Batch 0 Loss 0.1262
  Batch 100 Loss 0.3649
  Batch 200 Loss 0.1944
  Batch 300 Loss 0.0890
  Batch 400 Loss 0.1541
  Batch 500 Loss 0.2575
  Batch 600 Loss 0.4393
  Batch 700 Loss 0.1700
Finished epoch 15 in 53.0 seconds
Perplexity training: 1.015
Measuring development set...
Perplexity dev: 1.076

==== Starting epoch 16 ====
  Batch 0 Loss 0.0873
  Batch 100 Loss 0.3564
  Batch 200 Loss 0.1408
  Batch 300 Loss 0.0590
  Batch 400 Loss 0.0791
  Batch 500 Loss 0.2105
  Batch 600 Loss 0.2398
  Batch 700 Loss 0.1240
Finished epoch 16 in 53.0 seconds
Perplexity training: 1.013

==== Starting epoch 17 ====
  Batch 0 Loss 0.0710
  Batch 100 Loss 0.3181
  Batch 200 Loss 0.1311
  Batch 300 Loss 0.0639
  Batch 400 Loss 0.1310
  Batch 500 Loss 0.3253
  Batch 600 Loss 0.2652
  Batch 700 Loss 0.0981
Finished epoch 17 in 54.0 seconds
Perplexity training: 1.011
Measuring development set...
Perplexity dev: 1.065

==== Starting epoch 18 ====
  Batch 0 Loss 0.0996
  Batch 100 Loss 0.2768
  Batch 200 Loss 0.0783
  Batch 300 Loss 0.0444
  Batch 400 Loss 0.0565
  Batch 500 Loss 0.1606
  Batch 600 Loss 0.2316
  Batch 700 Loss 0.0413
Finished epoch 18 in 52.0 seconds
Perplexity training: 1.010

==== Starting epoch 19 ====
  Batch 0 Loss 0.0847
  Batch 100 Loss 0.1961
  Batch 200 Loss 0.1965
  Batch 300 Loss 0.0837
  Batch 400 Loss 0.1004
  Batch 500 Loss 0.1565
  Batch 600 Loss 0.1806
  Batch 700 Loss 0.0776
Finished epoch 19 in 55.0 seconds
Perplexity training: 1.009
Measuring development set...
Perplexity dev: 1.061

==== Starting epoch 20 ====
  Batch 0 Loss 0.0315
  Batch 100 Loss 0.0960
  Batch 200 Loss 0.1027
  Batch 300 Loss 0.0490
  Batch 400 Loss 0.0617
  Batch 500 Loss 0.1411
  Batch 600 Loss 0.2537
  Batch 700 Loss 0.2560
Finished epoch 20 in 54.0 seconds
Perplexity training: 1.008

==== Starting epoch 21 ====
  Batch 0 Loss 0.0363
  Batch 100 Loss 0.1674
  Batch 200 Loss 0.0622
  Batch 300 Loss 0.0295
  Batch 400 Loss 0.0260
  Batch 500 Loss 0.1819
  Batch 600 Loss 0.1046
  Batch 700 Loss 0.0648
Finished epoch 21 in 55.0 seconds
Perplexity training: 1.007
Measuring development set...
Perplexity dev: 1.079

==== Starting epoch 22 ====
  Batch 0 Loss 0.1056
  Batch 100 Loss 0.1380
  Batch 200 Loss 0.1225
  Batch 300 Loss 0.0402
  Batch 400 Loss 0.0292
  Batch 500 Loss 0.2269
  Batch 600 Loss 0.1056
  Batch 700 Loss 0.0420
Finished epoch 22 in 53.0 seconds
Perplexity training: 1.007

==== Starting epoch 23 ====
  Batch 0 Loss 0.0598
  Batch 100 Loss 0.0645
  Batch 200 Loss 0.0708
  Batch 300 Loss 0.0455
  Batch 400 Loss 0.0325
  Batch 500 Loss 0.0874
  Batch 600 Loss 0.2725
  Batch 700 Loss 0.0421
Finished epoch 23 in 55.0 seconds
Perplexity training: 1.006
Measuring development set...
Perplexity dev: 1.055

==== Starting epoch 24 ====
  Batch 0 Loss 0.0713
  Batch 100 Loss 0.1952
  Batch 200 Loss 0.0983
  Batch 300 Loss 0.0221
  Batch 400 Loss 0.0519
  Batch 500 Loss 0.1276
  Batch 600 Loss 0.1137
  Batch 700 Loss 0.0417
Finished epoch 24 in 53.0 seconds
Perplexity training: 1.006

==== Starting epoch 25 ====
  Batch 0 Loss 0.0314
  Batch 100 Loss 0.1166
  Batch 200 Loss 0.0571
  Batch 300 Loss 0.0187
  Batch 400 Loss 0.0408
  Batch 500 Loss 0.0656
  Batch 600 Loss 0.1274
  Batch 700 Loss 0.0252
Finished epoch 25 in 54.0 seconds
Perplexity training: 1.005
Measuring development set...
Perplexity dev: 1.052

==== Starting epoch 26 ====
  Batch 0 Loss 0.0165
  Batch 100 Loss 0.0750
  Batch 200 Loss 0.1062
  Batch 300 Loss 0.0412
  Batch 400 Loss 0.0992
  Batch 500 Loss 0.0976
  Batch 600 Loss 0.0698
  Batch 700 Loss 0.0468
Finished epoch 26 in 54.0 seconds
Perplexity training: 1.005

==== Starting epoch 27 ====
  Batch 0 Loss 0.0443
  Batch 100 Loss 0.0922
  Batch 200 Loss 0.1316
  Batch 300 Loss 0.0205
  Batch 400 Loss 0.0914
  Batch 500 Loss 0.2267
  Batch 600 Loss 0.0430
  Batch 700 Loss 0.0391
Finished epoch 27 in 55.0 seconds
Perplexity training: 1.005
Measuring development set...
Perplexity dev: 1.042

==== Starting epoch 28 ====
  Batch 0 Loss 0.0364
  Batch 100 Loss 0.0433
  Batch 200 Loss 0.0485
  Batch 300 Loss 0.0091
  Batch 400 Loss 0.1192
  Batch 500 Loss 0.0705
  Batch 600 Loss 0.0923
  Batch 700 Loss 0.0327
Finished epoch 28 in 56.0 seconds
Perplexity training: 1.005

==== Starting epoch 29 ====
  Batch 0 Loss 0.0519
  Batch 100 Loss 0.1531
  Batch 200 Loss 0.0384
  Batch 300 Loss 0.0111
  Batch 400 Loss 0.0316
  Batch 500 Loss 0.0797
  Batch 600 Loss 0.1507
  Batch 700 Loss 0.0454
Finished epoch 29 in 54.0 seconds
Perplexity training: 1.005
Measuring development set...
Perplexity dev: 1.053

==== Starting epoch 30 ====
  Batch 0 Loss 0.0300
  Batch 100 Loss 0.0550
  Batch 200 Loss 0.0507
  Batch 300 Loss 0.0087
  Batch 400 Loss 0.0540
  Batch 500 Loss 0.0479
  Batch 600 Loss 0.0443
  Batch 700 Loss 0.0187
Finished epoch 30 in 54.0 seconds
Perplexity training: 1.004

==== Starting epoch 31 ====
  Batch 0 Loss 0.0788
  Batch 100 Loss 0.1040
  Batch 200 Loss 0.0499
  Batch 300 Loss 0.0458
  Batch 400 Loss 0.0348
  Batch 500 Loss 0.1041
  Batch 600 Loss 0.0971
  Batch 700 Loss 0.0271
Finished epoch 31 in 54.0 seconds
Perplexity training: 1.004
Measuring development set...
Perplexity dev: 1.060

==== Starting epoch 32 ====
  Batch 0 Loss 0.0703
  Batch 100 Loss 0.0634
  Batch 200 Loss 0.0351
  Batch 300 Loss 0.0116
  Batch 400 Loss 0.0991
  Batch 500 Loss 0.0746
  Batch 600 Loss 0.0926
  Batch 700 Loss 0.0158
Finished epoch 32 in 56.0 seconds
Perplexity training: 1.004

==== Starting epoch 33 ====
  Batch 0 Loss 0.0320
  Batch 100 Loss 0.0731
  Batch 200 Loss 0.0223
  Batch 300 Loss 0.0115
  Batch 400 Loss 0.0781
  Batch 500 Loss 0.0422
  Batch 600 Loss 0.0432
  Batch 700 Loss 0.0328
Finished epoch 33 in 56.0 seconds
Perplexity training: 1.004
Measuring development set...
Perplexity dev: 1.060

==== Starting epoch 34 ====
  Batch 0 Loss 0.0328
  Batch 100 Loss 0.0174
  Batch 200 Loss 0.0476
  Batch 300 Loss 0.0145
  Batch 400 Loss 0.0594
  Batch 500 Loss 0.0759
  Batch 600 Loss 0.0162
  Batch 700 Loss 0.0461
Finished epoch 34 in 55.0 seconds
Perplexity training: 1.003

==== Starting epoch 35 ====
  Batch 0 Loss 0.0471
  Batch 100 Loss 0.0814
  Batch 200 Loss 0.0204
  Batch 300 Loss 0.0232
  Batch 400 Loss 0.0286
  Batch 500 Loss 0.0406
  Batch 600 Loss 0.0419
  Batch 700 Loss 0.0352
Finished epoch 35 in 55.0 seconds
Perplexity training: 1.004
Measuring development set...
Perplexity dev: 1.077

==== Starting epoch 36 ====
  Batch 0 Loss 0.0364
  Batch 100 Loss 0.0500
  Batch 200 Loss 0.0325
  Batch 300 Loss 0.0140
  Batch 400 Loss 0.0222
  Batch 500 Loss 0.0269
  Batch 600 Loss 0.0208
  Batch 700 Loss 0.0237
Finished epoch 36 in 55.0 seconds
Perplexity training: 1.003

==== Starting epoch 37 ====
  Batch 0 Loss 0.0258
  Batch 100 Loss 0.0177
  Batch 200 Loss 0.0306
  Batch 300 Loss 0.0183
  Batch 400 Loss 0.0335
  Batch 500 Loss 0.0214
  Batch 600 Loss 0.0839
  Batch 700 Loss 0.0156
Finished epoch 37 in 54.0 seconds
Perplexity training: 1.003
Measuring development set...
Perplexity dev: 1.081
Finished training in 2026.22 seconds
Finished training after development set stopped improving.
