Starting training procedure.
Loading training set...
2019-06-25 11:58:29.806027: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-25 11:58:29.823444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:58:29.823957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:00:05.0
2019-06-25 11:58:29.824180: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-25 11:58:29.825398: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-25 11:58:29.826333: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-25 11:58:29.826575: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-25 11:58:29.827830: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-25 11:58:29.828819: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-25 11:58:29.831968: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-25 11:58:29.832066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:58:29.832576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:58:29.833009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-06-25 11:58:29.833341: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-06-25 11:58:29.911497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:58:29.912186: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d876e0 executing computations on platform CUDA. Devices:
2019-06-25 11:58:29.912209: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Quadro P4000, Compute Capability 6.1
2019-06-25 11:58:29.914420: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600020000 Hz
2019-06-25 11:58:29.915113: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d592b0 executing computations on platform Host. Devices:
2019-06-25 11:58:29.915132: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-06-25 11:58:29.915317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:58:29.915800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:00:05.0
2019-06-25 11:58:29.915838: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-25 11:58:29.915850: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-25 11:58:29.915859: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-25 11:58:29.915879: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-25 11:58:29.915889: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-25 11:58:29.915898: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-25 11:58:29.915909: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-25 11:58:29.915944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:58:29.916424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:58:29.916880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-06-25 11:58:29.916905: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-25 11:58:29.917612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-25 11:58:29.917624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-06-25 11:58:29.917630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-06-25 11:58:29.917704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:58:29.918211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-25 11:58:29.918757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7629 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 0000:00:05.0, compute capability: 6.1)
Saving vocab defined by training set...
Loading development set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


=== Model ===
Name: rnnpbnmt
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0
Num PBs: 128
Bind hard: True
Binding strength: 1.0
Autoencode: True
PB learning rate: 0.01
Sigma: 0
p_reset: 0


=== Training ===
Max epochs: 0
Early stopping steps: 10
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-06-25 11:58:35.312758: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-25 11:58:36.394001: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0625 11:58:36.667440 140283744835392 deprecation.py:323] From /home/paperspace/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 63.6311
  Batch 100 Loss 34.4083
  Batch 200 Loss 31.4154
  Batch 300 Loss 31.9147
  Batch 400 Loss 25.9789
  Batch 500 Loss 28.2613
  Batch 600 Loss 25.7339
  Batch 700 Loss 24.7828
Finished epoch 1 in 51.0 seconds
Perplexity training: 61.840
Measuring development set...
Recognition iteration 0 Loss 26.799
Recognition iteration 100 Loss 25.235
Recognition iteration 200 Loss 24.457
Recognition iteration 300 Loss 23.948
Recognition iteration 400 Loss 23.569
Recognition finished, iteration 500 Loss 23.269
Recognition iteration 0 Loss 26.456
Recognition iteration 100 Loss 24.977
Recognition iteration 200 Loss 24.129
Recognition iteration 300 Loss 23.563
Recognition iteration 400 Loss 23.155
Recognition finished, iteration 500 Loss 22.854
Recognition iteration 0 Loss 28.436
Recognition iteration 100 Loss 26.772
Recognition iteration 200 Loss 25.849
Recognition iteration 300 Loss 25.274
Recognition iteration 400 Loss 24.922
Recognition finished, iteration 500 Loss 24.665
Recognition iteration 0 Loss 26.159
Recognition iteration 100 Loss 24.660
Recognition iteration 200 Loss 23.796
Recognition iteration 300 Loss 23.252
Recognition iteration 400 Loss 22.935
Recognition finished, iteration 500 Loss 22.713
Perplexity dev: 18.459

==== Starting epoch 2 ====
  Batch 0 Loss 27.7414
  Batch 100 Loss 24.5230
  Batch 200 Loss 23.8903
  Batch 300 Loss 25.9167
  Batch 400 Loss 21.5066
  Batch 500 Loss 23.5709
  Batch 600 Loss 21.7390
  Batch 700 Loss 21.2972
Finished epoch 2 in 51.0 seconds
Perplexity training: 21.752

==== Starting epoch 3 ====
  Batch 0 Loss 26.0706
  Batch 100 Loss 21.7630
  Batch 200 Loss 20.5462
  Batch 300 Loss 22.6821
  Batch 400 Loss 17.9676
  Batch 500 Loss 19.6851
  Batch 600 Loss 17.9162
  Batch 700 Loss 18.2582
Finished epoch 3 in 49.0 seconds
Perplexity training: 14.780
Measuring development set...
Recognition iteration 0 Loss 26.204
Recognition iteration 100 Loss 14.352
Recognition iteration 200 Loss 13.411
Recognition iteration 300 Loss 12.937
Recognition iteration 400 Loss 12.633
Recognition finished, iteration 489 Loss 12.422
Recognition iteration 0 Loss 26.482
Recognition iteration 100 Loss 13.992
Recognition iteration 200 Loss 13.189
Recognition iteration 300 Loss 12.744
Recognition finished, iteration 310 Loss 12.723
Recognition iteration 0 Loss 27.600
Recognition iteration 100 Loss 15.456
Recognition iteration 200 Loss 14.535
Recognition iteration 300 Loss 14.016
Recognition iteration 400 Loss 13.663
Recognition finished, iteration 500 Loss 13.396
Recognition iteration 0 Loss 26.104
Recognition iteration 100 Loss 13.478
Recognition iteration 200 Loss 12.743
Recognition iteration 300 Loss 12.340
Recognition iteration 400 Loss 12.049
Recognition finished, iteration 500 Loss 11.815
Perplexity dev: 4.933

==== Starting epoch 4 ====
  Batch 0 Loss 21.3370
  Batch 100 Loss 18.4318
  Batch 200 Loss 17.2046
  Batch 300 Loss 19.5399
  Batch 400 Loss 15.7784
  Batch 500 Loss 17.0153
  Batch 600 Loss 15.3468
  Batch 700 Loss 15.8278
Finished epoch 4 in 49.0 seconds
Perplexity training: 10.163

==== Starting epoch 5 ====
  Batch 0 Loss 18.4709
  Batch 100 Loss 16.1150
  Batch 200 Loss 14.3902
  Batch 300 Loss 16.2260
  Batch 400 Loss 12.8733
  Batch 500 Loss 14.5799
  Batch 600 Loss 12.7551
  Batch 700 Loss 13.5334
Finished epoch 5 in 50.0 seconds
Perplexity training: 7.297
Measuring development set...
Recognition iteration 0 Loss 31.445
Recognition iteration 100 Loss 9.110
Recognition iteration 200 Loss 7.963
Recognition iteration 300 Loss 7.331
Recognition iteration 400 Loss 6.907
Recognition finished, iteration 500 Loss 6.605
Recognition iteration 0 Loss 33.296
Recognition iteration 100 Loss 8.955
Recognition iteration 200 Loss 7.940
Recognition iteration 300 Loss 7.391
Recognition iteration 400 Loss 6.984
Recognition finished, iteration 500 Loss 6.680
Recognition iteration 0 Loss 33.793
Recognition iteration 100 Loss 10.391
Recognition iteration 200 Loss 9.127
Recognition iteration 300 Loss 8.407
Recognition iteration 400 Loss 7.925
Recognition finished, iteration 500 Loss 7.576
Recognition iteration 0 Loss 32.744
Recognition iteration 100 Loss 8.338
Recognition iteration 200 Loss 7.236
Recognition iteration 300 Loss 6.671
Recognition iteration 400 Loss 6.334
Recognition finished, iteration 500 Loss 6.074
Perplexity dev: 2.380

==== Starting epoch 6 ====
  Batch 0 Loss 15.8038
  Batch 100 Loss 13.4026
  Batch 200 Loss 12.1528
  Batch 300 Loss 13.9589
  Batch 400 Loss 10.3907
  Batch 500 Loss 12.8685
  Batch 600 Loss 10.6308
  Batch 700 Loss 11.3924
Finished epoch 6 in 50.0 seconds
Perplexity training: 5.471

==== Starting epoch 7 ====
  Batch 0 Loss 13.5694
  Batch 100 Loss 11.4500
  Batch 200 Loss 10.1125
  Batch 300 Loss 11.8090
  Batch 400 Loss 8.4149
  Batch 500 Loss 11.1630
  Batch 600 Loss 9.3814
  Batch 700 Loss 9.4514
Finished epoch 7 in 50.0 seconds
Perplexity training: 4.272
Measuring development set...
Recognition iteration 0 Loss 37.583
Recognition iteration 100 Loss 5.858
Recognition iteration 200 Loss 4.521
Recognition iteration 300 Loss 3.929
Recognition iteration 400 Loss 3.530
Recognition finished, iteration 500 Loss 3.211
Recognition iteration 0 Loss 39.742
Recognition iteration 100 Loss 6.038
Recognition iteration 200 Loss 4.798
Recognition iteration 300 Loss 4.261
Recognition iteration 400 Loss 3.858
Recognition finished, iteration 500 Loss 3.598
Recognition iteration 0 Loss 39.685
Recognition iteration 100 Loss 6.659
Recognition iteration 200 Loss 5.261
Recognition iteration 300 Loss 4.578
Recognition iteration 400 Loss 4.126
Recognition finished, iteration 500 Loss 3.829
Recognition iteration 0 Loss 39.346
Recognition iteration 100 Loss 5.582
Recognition iteration 200 Loss 4.356
Recognition iteration 300 Loss 3.748
Recognition iteration 400 Loss 3.417
Recognition finished, iteration 500 Loss 3.160
Perplexity dev: 1.544

==== Starting epoch 8 ====
  Batch 0 Loss 11.0458
  Batch 100 Loss 9.8112
  Batch 200 Loss 8.1692
  Batch 300 Loss 10.0154
  Batch 400 Loss 6.7546
  Batch 500 Loss 9.5258
  Batch 600 Loss 7.8010
  Batch 700 Loss 7.7622
Finished epoch 8 in 51.0 seconds
Perplexity training: 3.434

==== Starting epoch 9 ====
  Batch 0 Loss 9.3553
  Batch 100 Loss 8.3400
  Batch 200 Loss 6.9842
  Batch 300 Loss 8.6625
  Batch 400 Loss 5.8555
  Batch 500 Loss 8.1842
  Batch 600 Loss 6.2252
  Batch 700 Loss 6.3040
Finished epoch 9 in 52.0 seconds
Perplexity training: 2.834
Measuring development set...
Recognition iteration 0 Loss 43.651
Recognition iteration 100 Loss 4.115
Recognition iteration 200 Loss 2.800
Recognition iteration 300 Loss 2.210
Recognition iteration 400 Loss 1.888
Recognition finished, iteration 500 Loss 1.691
Recognition iteration 0 Loss 47.195
Recognition iteration 100 Loss 4.718
Recognition iteration 200 Loss 3.226
Recognition iteration 300 Loss 2.646
Recognition iteration 400 Loss 2.330
Recognition finished, iteration 500 Loss 2.133
Recognition iteration 0 Loss 46.632
Recognition iteration 100 Loss 5.041
Recognition iteration 200 Loss 3.461
Recognition iteration 300 Loss 2.788
Recognition iteration 400 Loss 2.393
Recognition finished, iteration 500 Loss 2.104
Recognition iteration 0 Loss 46.313
Recognition iteration 100 Loss 4.295
Recognition iteration 200 Loss 2.748
Recognition iteration 300 Loss 2.130
Recognition iteration 400 Loss 1.816
Recognition finished, iteration 500 Loss 1.601
Perplexity dev: 1.262

==== Starting epoch 10 ====
  Batch 0 Loss 7.9090
  Batch 100 Loss 7.3032
  Batch 200 Loss 5.9931
  Batch 300 Loss 7.1816
  Batch 400 Loss 4.6095
  Batch 500 Loss 6.7325
  Batch 600 Loss 5.0494
  Batch 700 Loss 4.9588
Finished epoch 10 in 53.0 seconds
Perplexity training: 2.395

==== Starting epoch 11 ====
  Batch 0 Loss 6.5779
  Batch 100 Loss 5.8277
  Batch 200 Loss 4.7796
  Batch 300 Loss 6.0350
  Batch 400 Loss 3.4280
  Batch 500 Loss 5.7056
  Batch 600 Loss 4.2631
  Batch 700 Loss 3.8771
Finished epoch 11 in 53.0 seconds
Perplexity training: 2.062
Measuring development set...
Recognition iteration 0 Loss 49.135
Recognition iteration 100 Loss 3.599
Recognition iteration 200 Loss 1.966
Recognition iteration 300 Loss 1.425
Recognition iteration 400 Loss 1.167
Recognition finished, iteration 500 Loss 0.989
Recognition iteration 0 Loss 53.220
Recognition iteration 100 Loss 3.703
Recognition iteration 200 Loss 2.188
Recognition iteration 300 Loss 1.647
Recognition iteration 400 Loss 1.347
Recognition finished, iteration 500 Loss 1.176
Recognition iteration 0 Loss 53.149
Recognition iteration 100 Loss 3.841
Recognition iteration 200 Loss 2.139
Recognition iteration 300 Loss 1.529
Recognition iteration 400 Loss 1.248
Recognition finished, iteration 500 Loss 1.084
Recognition iteration 0 Loss 52.170
Recognition iteration 100 Loss 3.341
Recognition iteration 200 Loss 1.699
Recognition iteration 300 Loss 1.198
Recognition iteration 400 Loss 0.915
Recognition finished, iteration 500 Loss 0.758
Perplexity dev: 1.127

==== Starting epoch 12 ====
  Batch 0 Loss 5.3918
  Batch 100 Loss 4.6369
  Batch 200 Loss 3.7489
  Batch 300 Loss 5.0892
  Batch 400 Loss 2.5980
  Batch 500 Loss 4.6911
  Batch 600 Loss 3.4434
  Batch 700 Loss 2.9196
Finished epoch 12 in 54.0 seconds
Perplexity training: 1.804

==== Starting epoch 13 ====
  Batch 0 Loss 4.7623
  Batch 100 Loss 4.0104
  Batch 200 Loss 2.9780
  Batch 300 Loss 4.5420
  Batch 400 Loss 2.0487
  Batch 500 Loss 3.7803
  Batch 600 Loss 2.7915
  Batch 700 Loss 2.2548
Finished epoch 13 in 55.0 seconds
Perplexity training: 1.610
Measuring development set...
Recognition iteration 0 Loss 55.369
Recognition iteration 100 Loss 3.180
Recognition iteration 200 Loss 1.497
Recognition iteration 300 Loss 0.887
Recognition iteration 400 Loss 0.669
Recognition finished, iteration 500 Loss 0.560
Recognition iteration 0 Loss 59.303
Recognition iteration 100 Loss 2.871
Recognition iteration 200 Loss 1.480
Recognition iteration 300 Loss 1.067
Recognition iteration 400 Loss 0.852
Recognition finished, iteration 500 Loss 0.734
Recognition iteration 0 Loss 59.504
Recognition iteration 100 Loss 2.813
Recognition iteration 200 Loss 1.325
Recognition iteration 300 Loss 0.888
Recognition iteration 400 Loss 0.683
Recognition finished, iteration 500 Loss 0.533
Recognition iteration 0 Loss 59.319
Recognition iteration 100 Loss 2.833
Recognition iteration 200 Loss 1.209
Recognition iteration 300 Loss 0.780
Recognition iteration 400 Loss 0.525
Recognition finished, iteration 500 Loss 0.389
Perplexity dev: 1.064

==== Starting epoch 14 ====
  Batch 0 Loss 3.4780
  Batch 100 Loss 3.6022
  Batch 200 Loss 2.5283
  Batch 300 Loss 3.8527
  Batch 400 Loss 1.5901
  Batch 500 Loss 3.0585
  Batch 600 Loss 2.2260
  Batch 700 Loss 1.9452
Finished epoch 14 in 56.0 seconds
Perplexity training: 1.460

==== Starting epoch 15 ====
  Batch 0 Loss 2.9711
  Batch 100 Loss 2.8671
  Batch 200 Loss 2.1481
  Batch 300 Loss 3.1287
  Batch 400 Loss 1.1063
  Batch 500 Loss 2.5050
  Batch 600 Loss 1.8280
  Batch 700 Loss 1.2744
Finished epoch 15 in 55.0 seconds
Perplexity training: 1.348
Measuring development set...
Recognition iteration 0 Loss 61.008
Recognition iteration 100 Loss 2.478
Recognition iteration 200 Loss 1.137
Recognition iteration 300 Loss 0.719
Recognition iteration 400 Loss 0.542
Recognition finished, iteration 500 Loss 0.446
Recognition iteration 0 Loss 65.077
Recognition iteration 100 Loss 2.464
Recognition iteration 200 Loss 1.121
Recognition iteration 300 Loss 0.686
Recognition iteration 400 Loss 0.541
Recognition finished, iteration 500 Loss 0.461
Recognition iteration 0 Loss 64.142
Recognition iteration 100 Loss 2.293
Recognition iteration 200 Loss 0.890
Recognition iteration 300 Loss 0.563
Recognition iteration 400 Loss 0.419
Recognition finished, iteration 500 Loss 0.340
Recognition iteration 0 Loss 64.142
Recognition iteration 100 Loss 2.366
Recognition iteration 200 Loss 1.049
Recognition iteration 300 Loss 0.714
Recognition iteration 400 Loss 0.491
Recognition finished, iteration 500 Loss 0.385
Perplexity dev: 1.048

==== Starting epoch 16 ====
  Batch 0 Loss 2.6740
  Batch 100 Loss 2.2588
  Batch 200 Loss 1.4780
  Batch 300 Loss 2.6047
  Batch 400 Loss 0.7719
  Batch 500 Loss 2.0475
  Batch 600 Loss 1.2834
  Batch 700 Loss 0.9587
Finished epoch 16 in 54.0 seconds
Perplexity training: 1.262

==== Starting epoch 17 ====
  Batch 0 Loss 2.2269
  Batch 100 Loss 1.7601
  Batch 200 Loss 1.0868
  Batch 300 Loss 2.0553
  Batch 400 Loss 0.5826
  Batch 500 Loss 1.6520
  Batch 600 Loss 0.9840
  Batch 700 Loss 0.6887
Finished epoch 17 in 55.0 seconds
Perplexity training: 1.199
Measuring development set...
Recognition iteration 0 Loss 66.912
Recognition iteration 100 Loss 2.678
Recognition iteration 200 Loss 1.008
Recognition iteration 300 Loss 0.604
Recognition iteration 400 Loss 0.412
Recognition finished, iteration 500 Loss 0.280
Recognition iteration 0 Loss 71.247
Recognition iteration 100 Loss 2.922
Recognition iteration 200 Loss 1.134
Recognition iteration 300 Loss 0.651
Recognition iteration 400 Loss 0.467
Recognition finished, iteration 500 Loss 0.353
Recognition iteration 0 Loss 70.285
Recognition iteration 100 Loss 2.368
Recognition iteration 200 Loss 0.819
Recognition iteration 300 Loss 0.463
Recognition iteration 400 Loss 0.336
Recognition finished, iteration 500 Loss 0.260
Recognition iteration 0 Loss 69.310
Recognition iteration 100 Loss 2.399
Recognition iteration 200 Loss 0.896
Recognition iteration 300 Loss 0.547
Recognition iteration 400 Loss 0.399
Recognition finished, iteration 500 Loss 0.303
Perplexity dev: 1.034

==== Starting epoch 18 ====
  Batch 0 Loss 1.7122
  Batch 100 Loss 1.3867
  Batch 200 Loss 0.9062
  Batch 300 Loss 1.6350
  Batch 400 Loss 0.4658
  Batch 500 Loss 1.5907
  Batch 600 Loss 0.6906
  Batch 700 Loss 0.5365
Finished epoch 18 in 58.0 seconds
Perplexity training: 1.155

==== Starting epoch 19 ====
  Batch 0 Loss 1.4563
  Batch 100 Loss 1.0590
  Batch 200 Loss 0.6078
  Batch 300 Loss 1.3274
  Batch 400 Loss 0.3761
  Batch 500 Loss 1.1111
  Batch 600 Loss 0.6186
  Batch 700 Loss 0.6125
Finished epoch 19 in 60.0 seconds
Perplexity training: 1.121
Measuring development set...
Recognition iteration 0 Loss 71.146
Recognition iteration 100 Loss 2.531
Recognition iteration 200 Loss 0.741
Recognition iteration 300 Loss 0.448
Recognition iteration 400 Loss 0.312
Recognition finished, iteration 500 Loss 0.203
Recognition iteration 0 Loss 76.451
Recognition iteration 100 Loss 3.128
Recognition iteration 200 Loss 1.233
Recognition iteration 300 Loss 0.748
Recognition iteration 400 Loss 0.559
Recognition finished, iteration 500 Loss 0.404
Recognition iteration 0 Loss 75.047
Recognition iteration 100 Loss 2.416
Recognition iteration 200 Loss 0.663
Recognition iteration 300 Loss 0.316
Recognition iteration 400 Loss 0.217
Recognition finished, iteration 500 Loss 0.170
Recognition iteration 0 Loss 73.704
Recognition iteration 100 Loss 2.832
Recognition iteration 200 Loss 0.896
Recognition iteration 300 Loss 0.550
Recognition iteration 400 Loss 0.413
Recognition finished, iteration 500 Loss 0.323
Perplexity dev: 1.032

==== Starting epoch 20 ====
  Batch 0 Loss 0.9967
  Batch 100 Loss 0.7358
  Batch 200 Loss 0.6362
  Batch 300 Loss 1.0397
  Batch 400 Loss 0.2915
  Batch 500 Loss 0.9143
  Batch 600 Loss 0.5516
  Batch 700 Loss 0.4078
Finished epoch 20 in 59.0 seconds
Perplexity training: 1.096

==== Starting epoch 21 ====
  Batch 0 Loss 0.7628
  Batch 100 Loss 0.6396
  Batch 200 Loss 0.4775
  Batch 300 Loss 0.7875
  Batch 400 Loss 0.2764
  Batch 500 Loss 0.8014
  Batch 600 Loss 0.5480
  Batch 700 Loss 0.3061
Finished epoch 21 in 59.0 seconds
Perplexity training: 1.077
Measuring development set...
Recognition iteration 0 Loss 74.450
Recognition iteration 100 Loss 2.185
Recognition iteration 200 Loss 0.845
Recognition iteration 300 Loss 0.488
Recognition iteration 400 Loss 0.329
Recognition finished, iteration 500 Loss 0.253
Recognition iteration 0 Loss 81.699
Recognition iteration 100 Loss 3.195
Recognition iteration 200 Loss 1.068
Recognition iteration 300 Loss 0.555
Recognition iteration 400 Loss 0.402
Recognition finished, iteration 500 Loss 0.310
Recognition iteration 0 Loss 80.659
Recognition iteration 100 Loss 2.314
Recognition iteration 200 Loss 0.650
Recognition iteration 300 Loss 0.322
Recognition iteration 400 Loss 0.196
Recognition finished, iteration 500 Loss 0.145
Recognition iteration 0 Loss 79.266
Recognition iteration 100 Loss 2.637
Recognition iteration 200 Loss 0.827
Recognition iteration 300 Loss 0.485
Recognition iteration 400 Loss 0.349
Recognition finished, iteration 500 Loss 0.247
Perplexity dev: 1.029

==== Starting epoch 22 ====
  Batch 0 Loss 0.7847
  Batch 100 Loss 0.5433
  Batch 200 Loss 0.5086
  Batch 300 Loss 0.7519
  Batch 400 Loss 0.2826
  Batch 500 Loss 0.7388
  Batch 600 Loss 0.3425
  Batch 700 Loss 0.2991
Finished epoch 22 in 59.0 seconds
Perplexity training: 1.064

==== Starting epoch 23 ====
  Batch 0 Loss 0.5137
  Batch 100 Loss 0.5260
  Batch 200 Loss 0.3730
  Batch 300 Loss 0.6431
  Batch 400 Loss 0.2129
  Batch 500 Loss 0.5878
  Batch 600 Loss 0.2795
  Batch 700 Loss 0.1993
Finished epoch 23 in 61.0 seconds
Perplexity training: 1.054
Measuring development set...
Recognition iteration 0 Loss 81.571
Recognition iteration 100 Loss 2.660
Recognition iteration 200 Loss 0.671
Recognition iteration 300 Loss 0.245
Recognition iteration 400 Loss 0.154
Recognition finished, iteration 500 Loss 0.111
Recognition iteration 0 Loss 87.412
Recognition iteration 100 Loss 2.688
Recognition iteration 200 Loss 0.811
Recognition iteration 300 Loss 0.446
Recognition iteration 400 Loss 0.308
Recognition finished, iteration 500 Loss 0.220
Recognition iteration 0 Loss 87.096
Recognition iteration 100 Loss 2.537
Recognition iteration 200 Loss 0.441
Recognition iteration 300 Loss 0.226
Recognition iteration 400 Loss 0.148
Recognition finished, iteration 500 Loss 0.112
Recognition iteration 0 Loss 86.228
Recognition iteration 100 Loss 2.979
Recognition iteration 200 Loss 0.693
Recognition iteration 300 Loss 0.344
Recognition iteration 400 Loss 0.231
Recognition finished, iteration 500 Loss 0.158
Perplexity dev: 1.016

==== Starting epoch 24 ====
  Batch 0 Loss 0.3845
  Batch 100 Loss 0.3293
  Batch 200 Loss 0.3483
  Batch 300 Loss 0.6019
  Batch 400 Loss 0.2335
  Batch 500 Loss 0.4195
  Batch 600 Loss 0.3774
  Batch 700 Loss 0.1916
Finished epoch 24 in 63.0 seconds
Perplexity training: 1.046

==== Starting epoch 25 ====
  Batch 0 Loss 0.4830
  Batch 100 Loss 0.3565
  Batch 200 Loss 0.3079
  Batch 300 Loss 0.5839
  Batch 400 Loss 0.1985
  Batch 500 Loss 0.5113
  Batch 600 Loss 0.3489
  Batch 700 Loss 0.2133
Finished epoch 25 in 64.0 seconds
Perplexity training: 1.041
Measuring development set...
Recognition iteration 0 Loss 83.761
Recognition iteration 100 Loss 2.713
Recognition iteration 200 Loss 0.958
Recognition iteration 300 Loss 0.555
Recognition iteration 400 Loss 0.356
Recognition finished, iteration 500 Loss 0.222
Recognition iteration 0 Loss 89.779
Recognition iteration 100 Loss 3.295
Recognition iteration 200 Loss 0.770
Recognition iteration 300 Loss 0.328
Recognition iteration 400 Loss 0.187
Recognition finished, iteration 500 Loss 0.137
Recognition iteration 0 Loss 87.754
Recognition iteration 100 Loss 2.543
Recognition iteration 200 Loss 0.419
Recognition iteration 300 Loss 0.182
Recognition iteration 400 Loss 0.124
Recognition finished, iteration 500 Loss 0.095
Recognition iteration 0 Loss 88.420
Recognition iteration 100 Loss 3.200
Recognition iteration 200 Loss 0.837
Recognition iteration 300 Loss 0.478
Recognition iteration 400 Loss 0.402
Recognition finished, iteration 500 Loss 0.324
Perplexity dev: 1.040

==== Starting epoch 26 ====
  Batch 0 Loss 0.3272
  Batch 100 Loss 0.3151
  Batch 200 Loss 0.3564
  Batch 300 Loss 0.3816
  Batch 400 Loss 0.1876
  Batch 500 Loss 0.3326
  Batch 600 Loss 0.2274
  Batch 700 Loss 0.2158
Finished epoch 26 in 64.0 seconds
Perplexity training: 1.037

==== Starting epoch 27 ====
  Batch 0 Loss 0.3537
  Batch 100 Loss 0.4597
  Batch 200 Loss 0.3901
  Batch 300 Loss 0.4587
  Batch 400 Loss 0.1331
  Batch 500 Loss 0.3833
  Batch 600 Loss 0.1580
  Batch 700 Loss 0.2333
Finished epoch 27 in 65.0 seconds
Perplexity training: 1.034
Measuring development set...
Recognition iteration 0 Loss 87.952
Recognition iteration 100 Loss 2.151
Recognition iteration 200 Loss 0.616
Recognition iteration 300 Loss 0.301
Recognition iteration 400 Loss 0.161
Recognition finished, iteration 500 Loss 0.085
Recognition iteration 0 Loss 93.065
Recognition iteration 100 Loss 3.187
Recognition iteration 200 Loss 0.839
Recognition iteration 300 Loss 0.396
Recognition iteration 400 Loss 0.277
Recognition finished, iteration 500 Loss 0.198
Recognition iteration 0 Loss 90.207
Recognition iteration 100 Loss 2.800
Recognition iteration 200 Loss 0.593
Recognition iteration 300 Loss 0.254
Recognition iteration 400 Loss 0.143
Recognition finished, iteration 500 Loss 0.102
Recognition iteration 0 Loss 91.942
Recognition iteration 100 Loss 3.458
Recognition iteration 200 Loss 1.062
Recognition iteration 300 Loss 0.462
Recognition iteration 400 Loss 0.271
Recognition finished, iteration 500 Loss 0.123
Perplexity dev: 1.014

==== Starting epoch 28 ====
  Batch 0 Loss 0.2678
  Batch 100 Loss 0.3109
  Batch 200 Loss 0.3150
  Batch 300 Loss 0.3978
  Batch 400 Loss 0.1768
  Batch 500 Loss 0.4686
  Batch 600 Loss 0.2394
  Batch 700 Loss 0.1837
Finished epoch 28 in 66.0 seconds
Perplexity training: 1.032

==== Starting epoch 29 ====
  Batch 0 Loss 0.3653
  Batch 100 Loss 0.1482
  Batch 200 Loss 0.2812
  Batch 300 Loss 0.3079
  Batch 400 Loss 0.0968
  Batch 500 Loss 0.4600
  Batch 600 Loss 0.1876
  Batch 700 Loss 0.1958
Finished epoch 29 in 67.0 seconds
Perplexity training: 1.030
Measuring development set...
Recognition iteration 0 Loss 87.942
Recognition iteration 100 Loss 2.662
Recognition iteration 200 Loss 0.767
Recognition iteration 300 Loss 0.458
Recognition finished, iteration 394 Loss 0.363
Recognition iteration 0 Loss 96.045
Recognition iteration 100 Loss 3.207
Recognition iteration 200 Loss 0.907
Recognition iteration 300 Loss 0.428
Recognition iteration 400 Loss 0.222
Recognition finished, iteration 500 Loss 0.146
Recognition iteration 0 Loss 94.161
Recognition iteration 100 Loss 2.463
Recognition iteration 200 Loss 0.507
Recognition iteration 300 Loss 0.177
Recognition iteration 400 Loss 0.114
Recognition finished, iteration 500 Loss 0.082
Recognition iteration 0 Loss 92.660
Recognition iteration 100 Loss 2.897
Recognition iteration 200 Loss 0.771
Recognition iteration 300 Loss 0.452
Recognition iteration 400 Loss 0.329
Recognition finished, iteration 500 Loss 0.239
Perplexity dev: 1.035

==== Starting epoch 30 ====
  Batch 0 Loss 0.3422
  Batch 100 Loss 0.2498
  Batch 200 Loss 0.3832
  Batch 300 Loss 0.4512
  Batch 400 Loss 0.0892
  Batch 500 Loss 0.4970
  Batch 600 Loss 0.2235
  Batch 700 Loss 0.1191
Finished epoch 30 in 67.0 seconds
Perplexity training: 1.028

==== Starting epoch 31 ====
  Batch 0 Loss 0.2923
  Batch 100 Loss 0.3287
  Batch 200 Loss 0.2582
  Batch 300 Loss 0.4810
  Batch 400 Loss 0.1532
  Batch 500 Loss 0.4902
  Batch 600 Loss 0.2352
  Batch 700 Loss 0.1310
Finished epoch 31 in 67.0 seconds
Perplexity training: 1.027
Measuring development set...
Recognition iteration 0 Loss 92.221
Recognition iteration 100 Loss 2.828
Recognition iteration 200 Loss 0.808
Recognition iteration 300 Loss 0.544
Recognition iteration 400 Loss 0.354
Recognition finished, iteration 500 Loss 0.269
Recognition iteration 0 Loss 97.515
Recognition iteration 100 Loss 3.231
Recognition iteration 200 Loss 0.894
Recognition iteration 300 Loss 0.401
Recognition iteration 400 Loss 0.217
Recognition finished, iteration 500 Loss 0.148
Recognition iteration 0 Loss 96.498
Recognition iteration 100 Loss 3.034
Recognition iteration 200 Loss 0.463
Recognition iteration 300 Loss 0.137
Recognition iteration 400 Loss 0.081
Recognition finished, iteration 500 Loss 0.059
Recognition iteration 0 Loss 95.516
Recognition iteration 100 Loss 3.539
Recognition iteration 200 Loss 0.973
Recognition iteration 300 Loss 0.610
Recognition iteration 400 Loss 0.427
Recognition finished, iteration 500 Loss 0.322
Perplexity dev: 1.045

==== Starting epoch 32 ====
  Batch 0 Loss 0.2387
  Batch 100 Loss 0.1646
  Batch 200 Loss 0.2668
  Batch 300 Loss 0.3830
  Batch 400 Loss 0.1343
  Batch 500 Loss 0.2975
  Batch 600 Loss 0.1062
  Batch 700 Loss 0.1091
Finished epoch 32 in 68.0 seconds
Perplexity training: 1.026

==== Starting epoch 33 ====
  Batch 0 Loss 0.2459
  Batch 100 Loss 0.1968
  Batch 200 Loss 0.2648
  Batch 300 Loss 0.3649
  Batch 400 Loss 0.0717
  Batch 500 Loss 0.2493
  Batch 600 Loss 0.2555
  Batch 700 Loss 0.1455
Finished epoch 33 in 69.0 seconds
Perplexity training: 1.025
Measuring development set...
Recognition iteration 0 Loss 93.819
Recognition iteration 100 Loss 2.302
Recognition iteration 200 Loss 0.460
Recognition iteration 300 Loss 0.201
Recognition iteration 400 Loss 0.127
Recognition finished, iteration 500 Loss 0.089
Recognition iteration 0 Loss 98.426
Recognition iteration 100 Loss 3.233
Recognition iteration 200 Loss 0.725
Recognition iteration 300 Loss 0.428
Recognition iteration 400 Loss 0.311
Recognition finished, iteration 500 Loss 0.209
Recognition iteration 0 Loss 96.919
Recognition iteration 100 Loss 3.399
Recognition iteration 200 Loss 0.524
Recognition iteration 300 Loss 0.189
Recognition iteration 400 Loss 0.115
Recognition finished, iteration 500 Loss 0.082
Recognition iteration 0 Loss 96.905
Recognition iteration 100 Loss 3.609
Recognition iteration 200 Loss 0.968
Recognition iteration 300 Loss 0.541
Recognition iteration 400 Loss 0.255
Recognition finished, iteration 500 Loss 0.115
Perplexity dev: 1.013

==== Starting epoch 34 ====
  Batch 0 Loss 0.2174
  Batch 100 Loss 0.2114
  Batch 200 Loss 0.2130
  Batch 300 Loss 0.4569
  Batch 400 Loss 0.1080
  Batch 500 Loss 0.2528
  Batch 600 Loss 0.1363
  Batch 700 Loss 0.1170
Finished epoch 34 in 70.0 seconds
Perplexity training: 1.025

==== Starting epoch 35 ====
  Batch 0 Loss 0.1869
  Batch 100 Loss 0.2125
  Batch 200 Loss 0.2928
  Batch 300 Loss 0.3988
  Batch 400 Loss 0.1265
  Batch 500 Loss 0.2837
  Batch 600 Loss 0.2262
  Batch 700 Loss 0.1181
Finished epoch 35 in 81.0 seconds
Perplexity training: 1.023
Measuring development set...
Recognition iteration 0 Loss 96.367
Recognition iteration 100 Loss 3.155
Recognition iteration 200 Loss 0.831
Recognition finished, iteration 241 Loss 0.658
Recognition iteration 0 Loss 103.030
Recognition iteration 100 Loss 3.506
Recognition iteration 200 Loss 0.713
Recognition iteration 300 Loss 0.341
Recognition iteration 400 Loss 0.210
Recognition finished, iteration 500 Loss 0.132
Recognition iteration 0 Loss 100.303
Recognition iteration 100 Loss 3.495
Recognition iteration 200 Loss 0.703
Recognition iteration 300 Loss 0.402
Recognition iteration 400 Loss 0.220
Recognition finished, iteration 500 Loss 0.093
Recognition iteration 0 Loss 99.079
Recognition iteration 100 Loss 3.801
Recognition iteration 200 Loss 0.920
Recognition iteration 300 Loss 0.560
Recognition iteration 400 Loss 0.429
Recognition finished, iteration 500 Loss 0.297
Perplexity dev: 1.094

==== Starting epoch 36 ====
  Batch 0 Loss 0.2229
  Batch 100 Loss 0.1670
  Batch 200 Loss 0.1611
  Batch 300 Loss 0.3390
  Batch 400 Loss 0.1073
  Batch 500 Loss 0.2576
  Batch 600 Loss 0.2216
  Batch 700 Loss 0.1248
Finished epoch 36 in 75.0 seconds
Perplexity training: 1.023

==== Starting epoch 37 ====
  Batch 0 Loss 0.2875
  Batch 100 Loss 0.2604
  Batch 200 Loss 0.2079
  Batch 300 Loss 0.1724
  Batch 400 Loss 0.1239
  Batch 500 Loss 0.2316
  Batch 600 Loss 0.1182
  Batch 700 Loss 0.1506
Finished epoch 37 in 76.0 seconds
Perplexity training: 1.024
Measuring development set...
Recognition iteration 0 Loss 99.279
Recognition iteration 100 Loss 3.254
Recognition iteration 200 Loss 1.008
Recognition iteration 300 Loss 0.635
Recognition iteration 400 Loss 0.458
Recognition finished, iteration 500 Loss 0.351
Recognition iteration 0 Loss 102.253
Recognition iteration 100 Loss 3.190
Recognition iteration 200 Loss 0.961
Recognition iteration 300 Loss 0.580
Recognition iteration 400 Loss 0.245
Recognition finished, iteration 500 Loss 0.144
Recognition iteration 0 Loss 102.525
Recognition iteration 100 Loss 3.689
Recognition iteration 200 Loss 0.422
Recognition iteration 300 Loss 0.134
Recognition iteration 400 Loss 0.082
Recognition finished, iteration 500 Loss 0.062
Recognition iteration 0 Loss 101.510
Recognition iteration 100 Loss 3.453
Recognition iteration 200 Loss 0.780
Recognition iteration 300 Loss 0.418
Recognition iteration 400 Loss 0.299
Recognition finished, iteration 500 Loss 0.239
Perplexity dev: 1.039

==== Starting epoch 38 ====
  Batch 0 Loss 0.1818
  Batch 100 Loss 0.1008
  Batch 200 Loss 0.1362
  Batch 300 Loss 0.2620
  Batch 400 Loss 0.1110
  Batch 500 Loss 0.2006
  Batch 600 Loss 0.1705
  Batch 700 Loss 0.1830
Finished epoch 38 in 76.0 seconds
Perplexity training: 1.024

==== Starting epoch 39 ====
  Batch 0 Loss 0.2228
  Batch 100 Loss 0.2407
  Batch 200 Loss 0.1369
  Batch 300 Loss 0.2793
  Batch 400 Loss 0.0531
  Batch 500 Loss 0.2633
  Batch 600 Loss 0.1002
  Batch 700 Loss 0.1403
Finished epoch 39 in 76.0 seconds
Perplexity training: 1.023
Measuring development set...
Recognition iteration 0 Loss 98.333
Recognition iteration 100 Loss 2.266
Recognition iteration 200 Loss 0.711
Recognition iteration 300 Loss 0.512
Recognition iteration 400 Loss 0.429
Recognition finished, iteration 425 Loss 0.420
Recognition iteration 0 Loss 105.491
Recognition iteration 100 Loss 3.429
Recognition iteration 200 Loss 0.722
Recognition iteration 300 Loss 0.323
Recognition iteration 400 Loss 0.201
Recognition finished, iteration 500 Loss 0.117
Recognition iteration 0 Loss 104.248
Recognition iteration 100 Loss 3.711
Recognition iteration 200 Loss 0.538
Recognition iteration 300 Loss 0.168
Recognition iteration 400 Loss 0.089
Recognition finished, iteration 500 Loss 0.057
Recognition iteration 0 Loss 102.714
Recognition iteration 100 Loss 3.143
Recognition iteration 200 Loss 0.943
Recognition iteration 300 Loss 0.573
Recognition iteration 400 Loss 0.388
Recognition finished, iteration 500 Loss 0.293
Perplexity dev: 1.057

==== Starting epoch 40 ====
  Batch 0 Loss 0.1695
  Batch 100 Loss 0.2679
  Batch 200 Loss 0.1965
  Batch 300 Loss 0.2763
  Batch 400 Loss 0.0842
  Batch 500 Loss 0.3191
  Batch 600 Loss 0.1135
  Batch 700 Loss 0.1496
Finished epoch 40 in 78.0 seconds
Perplexity training: 1.023

==== Starting epoch 41 ====
  Batch 0 Loss 0.1272
  Batch 100 Loss 0.2536
  Batch 200 Loss 0.2536
  Batch 300 Loss 0.1719
  Batch 400 Loss 0.0658
  Batch 500 Loss 0.4748
  Batch 600 Loss 0.1513
  Batch 700 Loss 0.0991
Finished epoch 41 in 79.0 seconds
Perplexity training: 1.022
Measuring development set...
Recognition iteration 0 Loss 102.808
Recognition iteration 100 Loss 2.916
Recognition iteration 200 Loss 0.835
Recognition iteration 300 Loss 0.543
Recognition iteration 400 Loss 0.345
Recognition finished, iteration 500 Loss 0.158
Recognition iteration 0 Loss 107.755
Recognition iteration 100 Loss 3.871
Recognition iteration 200 Loss 0.698
Recognition iteration 300 Loss 0.336
Recognition iteration 400 Loss 0.167
Recognition finished, iteration 500 Loss 0.104
Recognition iteration 0 Loss 107.914
Recognition iteration 100 Loss 3.767
Recognition iteration 200 Loss 0.317
Recognition iteration 300 Loss 0.107
Recognition iteration 400 Loss 0.069
Recognition finished, iteration 500 Loss 0.049
Recognition iteration 0 Loss 105.528
Recognition iteration 100 Loss 4.454
Recognition iteration 200 Loss 0.950
Recognition iteration 300 Loss 0.494
Recognition iteration 400 Loss 0.321
Recognition finished, iteration 500 Loss 0.269
Perplexity dev: 1.024

==== Starting epoch 42 ====
  Batch 0 Loss 0.1301
  Batch 100 Loss 0.1687
  Batch 200 Loss 0.1029
  Batch 300 Loss 0.1969
  Batch 400 Loss 0.1269
  Batch 500 Loss 0.3269
  Batch 600 Loss 0.2714
  Batch 700 Loss 0.2102
Finished epoch 42 in 80.0 seconds
Perplexity training: 1.022

==== Starting epoch 43 ====
  Batch 0 Loss 0.0995
  Batch 100 Loss 0.1716
  Batch 200 Loss 0.1243
  Batch 300 Loss 0.2539
  Batch 400 Loss 0.1036
  Batch 500 Loss 0.1669
  Batch 600 Loss 0.0856
  Batch 700 Loss 0.1719
Finished epoch 43 in 79.0 seconds
Perplexity training: 1.023
Measuring development set...
Recognition iteration 0 Loss 102.959
Recognition iteration 100 Loss 2.482
Recognition iteration 200 Loss 0.841
Recognition iteration 300 Loss 0.537
Recognition finished, iteration 349 Loss 0.480
Recognition iteration 0 Loss 107.689
Recognition iteration 100 Loss 3.670
Recognition iteration 200 Loss 0.544
Recognition iteration 300 Loss 0.255
Recognition iteration 400 Loss 0.184
Recognition finished, iteration 500 Loss 0.153
Recognition iteration 0 Loss 108.524
Recognition iteration 100 Loss 3.066
Recognition iteration 200 Loss 0.375
Recognition iteration 300 Loss 0.171
Recognition iteration 400 Loss 0.106
Recognition finished, iteration 500 Loss 0.076
Recognition iteration 0 Loss 106.432
Recognition iteration 100 Loss 3.790
Recognition iteration 200 Loss 1.094
Recognition iteration 300 Loss 0.598
Recognition iteration 400 Loss 0.338
Recognition finished, iteration 500 Loss 0.130
Perplexity dev: 1.056
Finished training in 4287.10 seconds
Finished training after development set stopped improving.
