Starting training procedure.
Loading training set...
2019-06-26 17:30:59.130888: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-26 17:30:59.140562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-26 17:30:59.141261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-26 17:30:59.141438: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 17:30:59.142754: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-26 17:30:59.143807: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-26 17:30:59.144060: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-26 17:30:59.145537: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-26 17:30:59.146718: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-26 17:30:59.150006: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 17:30:59.152833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-06-26 17:30:59.153276: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-06-26 17:30:59.799564: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ecc740 executing computations on platform CUDA. Devices:
2019-06-26 17:30:59.799608: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-26 17:30:59.799614: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-26 17:30:59.820913: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2019-06-26 17:30:59.824544: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ec2bf0 executing computations on platform Host. Devices:
2019-06-26 17:30:59.824604: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-06-26 17:30:59.829576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-26 17:30:59.830174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-26 17:30:59.830210: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 17:30:59.830218: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-26 17:30:59.830225: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-26 17:30:59.830231: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-26 17:30:59.830238: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-26 17:30:59.830244: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-26 17:30:59.830262: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 17:30:59.832836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-06-26 17:30:59.832868: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 17:30:59.834641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-26 17:30:59.834655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 
2019-06-26 17:30:59.834660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y 
2019-06-26 17:30:59.834664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N 
2019-06-26 17:30:59.837823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30458 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
2019-06-26 17:30:59.838867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 927 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
Saving vocab defined by training set...
Loading development set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


=== Model ===
Name: rnnpbnmt
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0.15
Num PBs: 1024
Bind hard: True
Binding strength: 1.0
Autoencode: False
PB learning rate: 0.01
Sigma: 0
p_reset: 0
Max recog epochs: 100


=== Training ===
Max epochs: 0
Early stopping steps: 20
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-06-26 17:31:05.231330: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 17:31:06.613982: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0626 17:31:06.935717 140253666527040 deprecation.py:323] From /lhome/daniesso/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 59.4498
  Batch 100 Loss 37.9047
  Batch 200 Loss 35.4245
  Batch 300 Loss 31.8982
  Batch 400 Loss 29.1803
  Batch 500 Loss 29.3628
  Batch 600 Loss 28.1937
  Batch 700 Loss 26.9365
Finished epoch 1 in 69.0 seconds
Perplexity training: 81.880
Measuring development set...
Recognition iteration 0 Loss 28.345
Recognition finished, iteration 100 Loss 25.338
Recognition iteration 0 Loss 26.687
Recognition finished, iteration 100 Loss 23.780
Recognition iteration 0 Loss 30.717
Recognition finished, iteration 100 Loss 27.253
Recognition iteration 0 Loss 28.401
Recognition finished, iteration 100 Loss 25.276
Perplexity dev: 36.605

==== Starting epoch 2 ====
  Batch 0 Loss 26.8674
  Batch 100 Loss 27.2911
  Batch 200 Loss 27.9696
  Batch 300 Loss 25.8804
  Batch 400 Loss 23.5792
  Batch 500 Loss 24.4784
  Batch 600 Loss 23.0415
  Batch 700 Loss 22.6955
Finished epoch 2 in 66.0 seconds
Perplexity training: 25.859

==== Starting epoch 3 ====
  Batch 0 Loss 24.3304
  Batch 100 Loss 23.5566
  Batch 200 Loss 24.1881
  Batch 300 Loss 22.1173
  Batch 400 Loss 19.9913
  Batch 500 Loss 20.7187
  Batch 600 Loss 19.6777
  Batch 700 Loss 19.7844
Finished epoch 3 in 66.0 seconds
Perplexity training: 17.086
Measuring development set...
Recognition iteration 0 Loss 28.365
Recognition finished, iteration 100 Loss 14.929
Recognition iteration 0 Loss 26.458
Recognition finished, iteration 100 Loss 13.664
Recognition iteration 0 Loss 29.715
Recognition finished, iteration 100 Loss 15.442
Recognition iteration 0 Loss 28.099
Recognition finished, iteration 100 Loss 14.668
Perplexity dev: 14.564

==== Starting epoch 4 ====
  Batch 0 Loss 20.6327
  Batch 100 Loss 19.9916
  Batch 200 Loss 21.2504
  Batch 300 Loss 19.5217
  Batch 400 Loss 17.0713
  Batch 500 Loss 17.5317
  Batch 600 Loss 16.5156
  Batch 700 Loss 16.5426
Finished epoch 4 in 66.0 seconds
Perplexity training: 11.825

==== Starting epoch 5 ====
  Batch 0 Loss 16.7418
  Batch 100 Loss 17.1860
  Batch 200 Loss 18.3746
  Batch 300 Loss 16.7603
  Batch 400 Loss 15.0021
  Batch 500 Loss 14.7804
  Batch 600 Loss 14.0590
  Batch 700 Loss 14.3103
Finished epoch 5 in 66.0 seconds
Perplexity training: 8.462
Measuring development set...
Recognition iteration 0 Loss 35.181
Recognition finished, iteration 100 Loss 9.299
Recognition iteration 0 Loss 32.880
Recognition finished, iteration 100 Loss 8.388
Recognition iteration 0 Loss 35.272
Recognition finished, iteration 100 Loss 9.347
Recognition iteration 0 Loss 34.701
Recognition finished, iteration 100 Loss 8.829
Perplexity dev: 7.637

==== Starting epoch 6 ====
  Batch 0 Loss 14.6535
  Batch 100 Loss 14.8912
  Batch 200 Loss 15.9239
  Batch 300 Loss 14.3759
  Batch 400 Loss 12.3831
  Batch 500 Loss 12.8573
  Batch 600 Loss 12.0890
  Batch 700 Loss 12.2042
Finished epoch 6 in 67.0 seconds
Perplexity training: 6.341

==== Starting epoch 7 ====
  Batch 0 Loss 12.4014
  Batch 100 Loss 13.0460
  Batch 200 Loss 14.0196
  Batch 300 Loss 12.3318
  Batch 400 Loss 10.6726
  Batch 500 Loss 11.3006
  Batch 600 Loss 10.3840
  Batch 700 Loss 10.2620
Finished epoch 7 in 67.0 seconds
Perplexity training: 4.998
Measuring development set...
Recognition iteration 0 Loss 41.074
Recognition finished, iteration 100 Loss 5.838
Recognition iteration 0 Loss 39.655
Recognition finished, iteration 100 Loss 5.348
Recognition iteration 0 Loss 41.843
Recognition finished, iteration 100 Loss 5.638
Recognition iteration 0 Loss 41.121
Recognition finished, iteration 100 Loss 5.714
Perplexity dev: 5.623

==== Starting epoch 8 ====
  Batch 0 Loss 10.7772
  Batch 100 Loss 11.5236
  Batch 200 Loss 12.4667
  Batch 300 Loss 10.9148
  Batch 400 Loss 9.0206
  Batch 500 Loss 9.4252
  Batch 600 Loss 8.9865
  Batch 700 Loss 9.3099
Finished epoch 8 in 67.0 seconds
Perplexity training: 4.057

==== Starting epoch 9 ====
  Batch 0 Loss 9.3466
  Batch 100 Loss 9.8127
  Batch 200 Loss 10.8652
  Batch 300 Loss 9.5587
  Batch 400 Loss 7.9020
  Batch 500 Loss 7.8759
  Batch 600 Loss 7.3578
  Batch 700 Loss 7.7929
Finished epoch 9 in 68.0 seconds
Perplexity training: 3.364
Measuring development set...
Recognition iteration 0 Loss 46.460
Recognition finished, iteration 100 Loss 3.820
Recognition iteration 0 Loss 44.847
Recognition finished, iteration 100 Loss 3.579
Recognition iteration 0 Loss 46.901
Recognition finished, iteration 100 Loss 3.565
Recognition iteration 0 Loss 45.501
Recognition finished, iteration 100 Loss 3.670
Perplexity dev: 5.068

==== Starting epoch 10 ====
  Batch 0 Loss 7.8894
  Batch 100 Loss 8.4131
  Batch 200 Loss 9.5938
  Batch 300 Loss 8.4115
  Batch 400 Loss 6.8125
  Batch 500 Loss 6.7069
  Batch 600 Loss 6.5182
  Batch 700 Loss 6.6580
Finished epoch 10 in 70.0 seconds
Perplexity training: 2.863

==== Starting epoch 11 ====
  Batch 0 Loss 6.8713
  Batch 100 Loss 7.1810
  Batch 200 Loss 8.7046
  Batch 300 Loss 7.4693
  Batch 400 Loss 5.8404
  Batch 500 Loss 5.6208
  Batch 600 Loss 5.6595
  Batch 700 Loss 5.7031
Finished epoch 11 in 71.0 seconds
Perplexity training: 2.486
Measuring development set...
Recognition iteration 0 Loss 50.984
Recognition finished, iteration 100 Loss 2.728
Recognition iteration 0 Loss 49.513
Recognition finished, iteration 100 Loss 2.537
Recognition iteration 0 Loss 51.611
Recognition finished, iteration 100 Loss 2.226
Recognition iteration 0 Loss 50.593
Recognition finished, iteration 100 Loss 2.515
Perplexity dev: 5.019

==== Starting epoch 12 ====
  Batch 0 Loss 6.0568
  Batch 100 Loss 6.1790
  Batch 200 Loss 7.7615
  Batch 300 Loss 6.2520
  Batch 400 Loss 4.9972
  Batch 500 Loss 5.0524
  Batch 600 Loss 4.9026
  Batch 700 Loss 4.8603
Finished epoch 12 in 71.0 seconds
Perplexity training: 2.204

==== Starting epoch 13 ====
  Batch 0 Loss 5.2449
  Batch 100 Loss 5.6044
  Batch 200 Loss 6.8417
  Batch 300 Loss 5.3673
  Batch 400 Loss 4.4337
  Batch 500 Loss 4.2460
  Batch 600 Loss 3.9651
  Batch 700 Loss 4.2559
Finished epoch 13 in 72.0 seconds
Perplexity training: 1.980
Measuring development set...
Recognition iteration 0 Loss 56.554
Recognition finished, iteration 100 Loss 1.890
Recognition iteration 0 Loss 55.069
Recognition finished, iteration 100 Loss 1.930
Recognition iteration 0 Loss 57.744
Recognition finished, iteration 100 Loss 1.468
Recognition iteration 0 Loss 56.347
Recognition finished, iteration 100 Loss 1.789
Perplexity dev: 5.820

==== Starting epoch 14 ====
  Batch 0 Loss 4.4245
  Batch 100 Loss 4.8523
  Batch 200 Loss 6.0272
  Batch 300 Loss 4.7838
  Batch 400 Loss 4.0232
  Batch 500 Loss 3.6157
  Batch 600 Loss 3.8398
  Batch 700 Loss 3.8645
Finished epoch 14 in 72.0 seconds
Perplexity training: 1.809

==== Starting epoch 15 ====
  Batch 0 Loss 3.6643
  Batch 100 Loss 4.1365
  Batch 200 Loss 5.3157
  Batch 300 Loss 4.0107
  Batch 400 Loss 3.4105
  Batch 500 Loss 2.9557
  Batch 600 Loss 3.2869
  Batch 700 Loss 3.4852
Finished epoch 15 in 72.0 seconds
Perplexity training: 1.674
Measuring development set...
Recognition iteration 0 Loss 60.577
Recognition finished, iteration 100 Loss 1.489
Recognition iteration 0 Loss 58.903
Recognition finished, iteration 100 Loss 1.659
Recognition iteration 0 Loss 61.519
Recognition finished, iteration 100 Loss 1.199
Recognition iteration 0 Loss 59.653
Recognition finished, iteration 100 Loss 1.436
Perplexity dev: 8.850

==== Starting epoch 16 ====
  Batch 0 Loss 3.2458
  Batch 100 Loss 3.5735
  Batch 200 Loss 4.7533
  Batch 300 Loss 3.6318
  Batch 400 Loss 3.0826
  Batch 500 Loss 2.6229
  Batch 600 Loss 2.7331
  Batch 700 Loss 2.8818
Finished epoch 16 in 72.0 seconds
Perplexity training: 1.559

==== Starting epoch 17 ====
  Batch 0 Loss 2.8764
  Batch 100 Loss 3.1172
  Batch 200 Loss 4.2285
  Batch 300 Loss 3.1831
  Batch 400 Loss 2.5880
  Batch 500 Loss 2.4460
  Batch 600 Loss 2.5088
  Batch 700 Loss 2.4119
Finished epoch 17 in 73.0 seconds
Perplexity training: 1.469
Measuring development set...
Recognition iteration 0 Loss 65.776
Recognition finished, iteration 100 Loss 1.135
Recognition iteration 0 Loss 63.789
Recognition finished, iteration 100 Loss 1.261
Recognition iteration 0 Loss 65.829
Recognition finished, iteration 100 Loss 0.751
Recognition iteration 0 Loss 63.607
Recognition finished, iteration 100 Loss 0.941
Perplexity dev: 10.092

==== Starting epoch 18 ====
  Batch 0 Loss 2.6002
  Batch 100 Loss 2.8270
  Batch 200 Loss 3.7927
  Batch 300 Loss 2.9774
  Batch 400 Loss 2.2175
  Batch 500 Loss 1.9357
  Batch 600 Loss 2.0670
  Batch 700 Loss 2.1093
Finished epoch 18 in 73.0 seconds
Perplexity training: 1.396

==== Starting epoch 19 ====
  Batch 0 Loss 2.2236
  Batch 100 Loss 2.6912
  Batch 200 Loss 3.0651
  Batch 300 Loss 2.5992
  Batch 400 Loss 1.8031
  Batch 500 Loss 1.7900
  Batch 600 Loss 1.7642
  Batch 700 Loss 1.7872
Finished epoch 19 in 74.0 seconds
Perplexity training: 1.336
Measuring development set...
Recognition iteration 0 Loss 68.991
Recognition finished, iteration 100 Loss 0.983
Recognition iteration 0 Loss 65.897
Recognition finished, iteration 100 Loss 1.187
Recognition iteration 0 Loss 69.095
Recognition finished, iteration 100 Loss 0.693
Recognition iteration 0 Loss 65.993
Recognition finished, iteration 100 Loss 0.828
Perplexity dev: 15.520

==== Starting epoch 20 ====
  Batch 0 Loss 1.8132
  Batch 100 Loss 2.2340
  Batch 200 Loss 2.7934
  Batch 300 Loss 2.3818
  Batch 400 Loss 1.5827
  Batch 500 Loss 1.6048
  Batch 600 Loss 1.4978
  Batch 700 Loss 1.5439
Finished epoch 20 in 74.0 seconds
Perplexity training: 1.288

==== Starting epoch 21 ====
  Batch 0 Loss 1.6416
  Batch 100 Loss 1.7402
  Batch 200 Loss 2.9339
  Batch 300 Loss 1.9636
  Batch 400 Loss 1.4956
  Batch 500 Loss 1.3874
  Batch 600 Loss 1.3212
  Batch 700 Loss 1.3623
Finished epoch 21 in 74.0 seconds
Perplexity training: 1.249
Measuring development set...
Recognition iteration 0 Loss 73.111
Recognition finished, iteration 100 Loss 0.777
Recognition iteration 0 Loss 70.629
Recognition finished, iteration 100 Loss 0.918
Recognition iteration 0 Loss 72.911
Recognition finished, iteration 100 Loss 0.519
Recognition iteration 0 Loss 70.943
Recognition finished, iteration 100 Loss 0.640
Perplexity dev: 21.003

==== Starting epoch 22 ====
  Batch 0 Loss 1.4768
  Batch 100 Loss 1.4182
  Batch 200 Loss 2.3407
  Batch 300 Loss 1.5738
  Batch 400 Loss 1.2526
  Batch 500 Loss 1.1866
  Batch 600 Loss 1.3524
  Batch 700 Loss 1.2313
Finished epoch 22 in 75.0 seconds
Perplexity training: 1.215

==== Starting epoch 23 ====
  Batch 0 Loss 1.1501
  Batch 100 Loss 1.3936
  Batch 200 Loss 2.1933
  Batch 300 Loss 1.6427
  Batch 400 Loss 1.0177
  Batch 500 Loss 1.0399
  Batch 600 Loss 1.0534
  Batch 700 Loss 1.0081
Finished epoch 23 in 76.0 seconds
Perplexity training: 1.188
Measuring development set...
Recognition iteration 0 Loss 75.347
Recognition finished, iteration 100 Loss 0.653
Recognition iteration 0 Loss 72.478
Recognition finished, iteration 100 Loss 0.853
Recognition iteration 0 Loss 75.749
Recognition finished, iteration 100 Loss 0.437
Recognition iteration 0 Loss 73.456
Recognition finished, iteration 100 Loss 0.699
Perplexity dev: 18.564

==== Starting epoch 24 ====
  Batch 0 Loss 1.1865
  Batch 100 Loss 1.4243
  Batch 200 Loss 1.9239
  Batch 300 Loss 1.3795
  Batch 400 Loss 0.9751
  Batch 500 Loss 0.9614
  Batch 600 Loss 0.9057
  Batch 700 Loss 0.8078
Finished epoch 24 in 76.0 seconds
Perplexity training: 1.164

==== Starting epoch 25 ====
  Batch 0 Loss 0.9944
  Batch 100 Loss 1.1942
  Batch 200 Loss 1.7326
  Batch 300 Loss 1.3409
  Batch 400 Loss 0.8869
  Batch 500 Loss 0.8468
  Batch 600 Loss 0.8676
  Batch 700 Loss 0.9004
Finished epoch 25 in 79.0 seconds
Perplexity training: 1.146
Measuring development set...
Recognition iteration 0 Loss 77.709
Recognition finished, iteration 100 Loss 0.677
Recognition iteration 0 Loss 75.011
Recognition finished, iteration 100 Loss 0.847
Recognition iteration 0 Loss 78.683
Recognition finished, iteration 100 Loss 0.447
Recognition iteration 0 Loss 76.166
Recognition finished, iteration 100 Loss 0.714
Perplexity dev: 22.616

==== Starting epoch 26 ====
  Batch 0 Loss 0.8113
  Batch 100 Loss 1.0949
  Batch 200 Loss 1.5124
  Batch 300 Loss 1.2947
  Batch 400 Loss 0.5662
  Batch 500 Loss 0.7184
  Batch 600 Loss 0.8906
  Batch 700 Loss 0.6861
Finished epoch 26 in 77.0 seconds
Perplexity training: 1.132

==== Starting epoch 27 ====
  Batch 0 Loss 0.7715
  Batch 100 Loss 0.9390
  Batch 200 Loss 1.4699
  Batch 300 Loss 1.1481
  Batch 400 Loss 0.7565
  Batch 500 Loss 0.6925
  Batch 600 Loss 0.8036
  Batch 700 Loss 0.6186
Finished epoch 27 in 79.0 seconds
Perplexity training: 1.118
Measuring development set...
Recognition iteration 0 Loss 81.007
Recognition finished, iteration 100 Loss 0.665
Recognition iteration 0 Loss 77.413
Recognition finished, iteration 100 Loss 0.811
Recognition iteration 0 Loss 81.611
Recognition finished, iteration 100 Loss 0.329
Recognition iteration 0 Loss 78.381
Recognition finished, iteration 100 Loss 0.551
Perplexity dev: 34.473

==== Starting epoch 28 ====
  Batch 0 Loss 0.7577
  Batch 100 Loss 0.7687
  Batch 200 Loss 1.2788
  Batch 300 Loss 0.9796
  Batch 400 Loss 0.5798
  Batch 500 Loss 0.7584
  Batch 600 Loss 0.7146
  Batch 700 Loss 0.6930
Finished epoch 28 in 79.0 seconds
Perplexity training: 1.107

==== Starting epoch 29 ====
  Batch 0 Loss 0.5641
  Batch 100 Loss 0.7578
  Batch 200 Loss 1.2164
  Batch 300 Loss 0.9311
  Batch 400 Loss 0.4481
  Batch 500 Loss 0.5954
  Batch 600 Loss 0.6715
  Batch 700 Loss 0.5569
Finished epoch 29 in 80.0 seconds
Perplexity training: 1.098
Measuring development set...
Recognition iteration 0 Loss 81.531
Recognition finished, iteration 100 Loss 0.621
Recognition iteration 0 Loss 76.675
Recognition finished, iteration 100 Loss 0.643
Recognition iteration 0 Loss 82.191
Recognition finished, iteration 100 Loss 0.269
Recognition iteration 0 Loss 78.316
Recognition finished, iteration 100 Loss 0.515
Perplexity dev: 59.598

==== Starting epoch 30 ====
  Batch 0 Loss 0.5304
  Batch 100 Loss 0.7251
  Batch 200 Loss 0.9910
  Batch 300 Loss 0.8689
  Batch 400 Loss 0.5802
  Batch 500 Loss 0.4816
  Batch 600 Loss 0.5309
  Batch 700 Loss 0.5014
Finished epoch 30 in 80.0 seconds
Perplexity training: 1.090

==== Starting epoch 31 ====
  Batch 0 Loss 0.4575
  Batch 100 Loss 0.6400
  Batch 200 Loss 1.0829
  Batch 300 Loss 0.9083
  Batch 400 Loss 0.4675
  Batch 500 Loss 0.4496
  Batch 600 Loss 0.4827
  Batch 700 Loss 0.4522
Finished epoch 31 in 80.0 seconds
Perplexity training: 1.083
Measuring development set...
Recognition iteration 0 Loss 84.454
Recognition finished, iteration 100 Loss 0.739
Recognition iteration 0 Loss 80.687
Recognition finished, iteration 100 Loss 0.718
Recognition iteration 0 Loss 85.662
Recognition finished, iteration 100 Loss 0.421
Recognition iteration 0 Loss 81.983
Recognition finished, iteration 100 Loss 0.359
Perplexity dev: 50.457
Finished training in 2448.96 seconds
Finished training after development set stopped improving.
