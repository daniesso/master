2019-06-26 17:01:55.021421: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-26 17:01:55.030166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-26 17:01:55.030969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-26 17:01:55.031107: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 17:01:55.032280: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-26 17:01:55.033572: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-26 17:01:55.033849: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-26 17:01:55.035193: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-26 17:01:55.036366: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-26 17:01:55.039364: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 17:01:55.042139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
Starting training procedure.
Loading training set...
2019-06-26 17:01:55.902767: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-06-26 17:01:56.251038: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23a4790 executing computations on platform CUDA. Devices:
2019-06-26 17:01:56.251089: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-26 17:01:56.273005: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2019-06-26 17:01:56.276216: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x239ac40 executing computations on platform Host. Devices:
2019-06-26 17:01:56.276261: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-06-26 17:01:56.277580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-26 17:01:56.277632: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 17:01:56.277659: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-26 17:01:56.277669: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-26 17:01:56.277681: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-26 17:01:56.277690: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-26 17:01:56.277698: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-26 17:01:56.277708: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 17:01:56.279716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 1
2019-06-26 17:01:56.279768: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-26 17:01:56.282036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-26 17:01:56.282060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      1 
2019-06-26 17:01:56.282066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N 
2019-06-26 17:01:56.284697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30071 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
Saving vocab defined by training set...
Loading development set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


=== Model ===
Name: rnnpbnmt
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0.4
Num PBs: 1024
Bind hard: True
Binding strength: 1.0
Autoencode: False
PB learning rate: 0.01
Sigma: 0
p_reset: 0
Max recog epochs: 100


=== Training ===
Max epochs: 0
Early stopping steps: 20
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-06-26 17:02:01.417592: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-26 17:02:02.743763: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0626 17:02:03.093771 139876068427584 deprecation.py:323] From /lhome/daniesso/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 56.8500
  Batch 100 Loss 40.0501
  Batch 200 Loss 32.7978
  Batch 300 Loss 32.0337
  Batch 400 Loss 28.8617
  Batch 500 Loss 29.0695
  Batch 600 Loss 27.9218
  Batch 700 Loss 28.0820
Finished epoch 1 in 68.0 seconds
Perplexity training: 82.572
Measuring development set...
Recognition iteration 0 Loss 29.642
Recognition finished, iteration 100 Loss 26.757
Recognition iteration 0 Loss 29.034
Recognition finished, iteration 100 Loss 25.711
Recognition iteration 0 Loss 27.724
Recognition finished, iteration 100 Loss 24.838
Recognition iteration 0 Loss 28.953
Recognition finished, iteration 100 Loss 25.600
Perplexity dev: 37.225

==== Starting epoch 2 ====
  Batch 0 Loss 25.7802
  Batch 100 Loss 29.4738
  Batch 200 Loss 25.7016
  Batch 300 Loss 26.4450
  Batch 400 Loss 24.2146
  Batch 500 Loss 25.2827
  Batch 600 Loss 24.4346
  Batch 700 Loss 25.2243
Finished epoch 2 in 65.0 seconds
Perplexity training: 28.603

==== Starting epoch 3 ====
  Batch 0 Loss 22.8458
  Batch 100 Loss 26.3067
  Batch 200 Loss 22.9251
  Batch 300 Loss 23.7392
  Batch 400 Loss 21.4480
  Batch 500 Loss 22.4072
  Batch 600 Loss 21.8453
  Batch 700 Loss 22.3444
Finished epoch 3 in 65.0 seconds
Perplexity training: 20.037
Measuring development set...
Recognition iteration 0 Loss 28.184
Recognition finished, iteration 100 Loss 16.914
Recognition iteration 0 Loss 27.549
Recognition finished, iteration 100 Loss 16.407
Recognition iteration 0 Loss 26.965
Recognition finished, iteration 100 Loss 15.616
Recognition iteration 0 Loss 27.434
Recognition finished, iteration 100 Loss 15.715
Perplexity dev: 15.148

==== Starting epoch 4 ====
  Batch 0 Loss 20.6766
  Batch 100 Loss 23.9020
  Batch 200 Loss 20.0647
  Batch 300 Loss 20.8346
  Batch 400 Loss 18.7867
  Batch 500 Loss 20.1979
  Batch 600 Loss 18.5834
  Batch 700 Loss 19.5783
Finished epoch 4 in 66.0 seconds
Perplexity training: 14.377

==== Starting epoch 5 ====
  Batch 0 Loss 17.4294
  Batch 100 Loss 20.8787
  Batch 200 Loss 17.9691
  Batch 300 Loss 18.7216
  Batch 400 Loss 16.5116
  Batch 500 Loss 17.4281
  Batch 600 Loss 16.3254
  Batch 700 Loss 17.6997
Finished epoch 5 in 66.0 seconds
Perplexity training: 10.787
Measuring development set...
Recognition iteration 0 Loss 32.293
Recognition finished, iteration 100 Loss 11.364
Recognition iteration 0 Loss 31.904
Recognition finished, iteration 100 Loss 11.032
Recognition iteration 0 Loss 30.666
Recognition finished, iteration 100 Loss 10.348
Recognition iteration 0 Loss 31.949
Recognition finished, iteration 100 Loss 10.383
Perplexity dev: 7.550

==== Starting epoch 6 ====
  Batch 0 Loss 15.6221
  Batch 100 Loss 18.8921
  Batch 200 Loss 16.2475
  Batch 300 Loss 16.9469
  Batch 400 Loss 15.0186
  Batch 500 Loss 15.9369
  Batch 600 Loss 14.7769
  Batch 700 Loss 16.0603
Finished epoch 6 in 65.0 seconds
Perplexity training: 8.501

==== Starting epoch 7 ====
  Batch 0 Loss 13.7575
  Batch 100 Loss 17.0749
  Batch 200 Loss 14.3367
  Batch 300 Loss 15.2195
  Batch 400 Loss 13.4180
  Batch 500 Loss 14.4018
  Batch 600 Loss 13.2519
  Batch 700 Loss 14.4838
Finished epoch 7 in 66.0 seconds
Perplexity training: 7.024
Measuring development set...
Recognition iteration 0 Loss 35.771
Recognition finished, iteration 100 Loss 8.182
Recognition iteration 0 Loss 36.453
Recognition finished, iteration 100 Loss 8.043
Recognition iteration 0 Loss 35.035
Recognition finished, iteration 100 Loss 7.180
Recognition iteration 0 Loss 35.724
Recognition finished, iteration 100 Loss 7.286
Perplexity dev: 5.489

==== Starting epoch 8 ====
  Batch 0 Loss 12.2755
  Batch 100 Loss 15.7904
  Batch 200 Loss 12.5706
  Batch 300 Loss 13.7482
  Batch 400 Loss 12.0501
  Batch 500 Loss 12.9288
  Batch 600 Loss 11.9648
  Batch 700 Loss 12.9495
Finished epoch 8 in 66.0 seconds
Perplexity training: 5.917

==== Starting epoch 9 ====
  Batch 0 Loss 11.2722
  Batch 100 Loss 14.2172
  Batch 200 Loss 11.7182
  Batch 300 Loss 12.4220
  Batch 400 Loss 11.6278
  Batch 500 Loss 12.0842
  Batch 600 Loss 10.8790
  Batch 700 Loss 12.0132
Finished epoch 9 in 66.0 seconds
Perplexity training: 5.101
Measuring development set...
Recognition iteration 0 Loss 40.651
Recognition finished, iteration 100 Loss 6.196
Recognition iteration 0 Loss 40.966
Recognition finished, iteration 100 Loss 5.801
Recognition iteration 0 Loss 39.381
Recognition finished, iteration 100 Loss 5.278
Recognition iteration 0 Loss 41.373
Recognition finished, iteration 100 Loss 5.344
Perplexity dev: 4.514

==== Starting epoch 10 ====
  Batch 0 Loss 10.0999
  Batch 100 Loss 12.8908
  Batch 200 Loss 10.7843
  Batch 300 Loss 11.7035
  Batch 400 Loss 10.8954
  Batch 500 Loss 11.0538
  Batch 600 Loss 9.8928
  Batch 700 Loss 11.2836
Finished epoch 10 in 66.0 seconds
Perplexity training: 4.480

==== Starting epoch 11 ====
  Batch 0 Loss 8.9558
  Batch 100 Loss 12.0887
  Batch 200 Loss 9.9760
  Batch 300 Loss 10.7403
  Batch 400 Loss 9.7955
  Batch 500 Loss 10.4134
  Batch 600 Loss 9.2402
  Batch 700 Loss 10.2476
Finished epoch 11 in 67.0 seconds
Perplexity training: 3.992
Measuring development set...
Recognition iteration 0 Loss 44.355
Recognition finished, iteration 100 Loss 4.989
Recognition iteration 0 Loss 45.101
Recognition finished, iteration 100 Loss 4.578
Recognition iteration 0 Loss 42.866
Recognition finished, iteration 100 Loss 4.146
Recognition iteration 0 Loss 45.543
Recognition finished, iteration 100 Loss 4.280
Perplexity dev: 4.278

==== Starting epoch 12 ====
  Batch 0 Loss 8.0964
  Batch 100 Loss 11.4314
  Batch 200 Loss 9.2129
  Batch 300 Loss 10.0923
  Batch 400 Loss 8.7512
  Batch 500 Loss 9.5643
  Batch 600 Loss 8.8618
  Batch 700 Loss 9.6168
Finished epoch 12 in 67.0 seconds
Perplexity training: 3.614

==== Starting epoch 13 ====
  Batch 0 Loss 7.6604
  Batch 100 Loss 10.3517
  Batch 200 Loss 8.7035
  Batch 300 Loss 9.2371
  Batch 400 Loss 8.4046
  Batch 500 Loss 8.7320
  Batch 600 Loss 8.0849
  Batch 700 Loss 9.0278
Finished epoch 13 in 68.0 seconds
Perplexity training: 3.299
Measuring development set...
Recognition iteration 0 Loss 46.946
Recognition finished, iteration 100 Loss 4.058
Recognition iteration 0 Loss 47.989
Recognition finished, iteration 100 Loss 3.587
Recognition iteration 0 Loss 45.782
Recognition finished, iteration 100 Loss 3.258
Recognition iteration 0 Loss 47.527
Recognition finished, iteration 100 Loss 3.431
Perplexity dev: 4.291

==== Starting epoch 14 ====
  Batch 0 Loss 7.3082
  Batch 100 Loss 9.6095
  Batch 200 Loss 8.2257
  Batch 300 Loss 8.2782
  Batch 400 Loss 7.7855
  Batch 500 Loss 8.1788
  Batch 600 Loss 7.5487
  Batch 700 Loss 8.0180
Finished epoch 14 in 69.0 seconds
Perplexity training: 3.038

==== Starting epoch 15 ====
  Batch 0 Loss 6.8231
  Batch 100 Loss 9.0098
  Batch 200 Loss 7.1254
  Batch 300 Loss 7.8543
  Batch 400 Loss 7.2239
  Batch 500 Loss 7.4747
  Batch 600 Loss 6.9311
  Batch 700 Loss 7.9283
Finished epoch 15 in 69.0 seconds
Perplexity training: 2.827
Measuring development set...
Recognition iteration 0 Loss 49.413
Recognition finished, iteration 100 Loss 3.406
Recognition iteration 0 Loss 50.803
Recognition finished, iteration 100 Loss 2.963
Recognition iteration 0 Loss 48.293
Recognition finished, iteration 100 Loss 2.564
Recognition iteration 0 Loss 50.597
Recognition finished, iteration 100 Loss 2.807
Perplexity dev: 4.461

==== Starting epoch 16 ====
  Batch 0 Loss 6.0689
  Batch 100 Loss 8.3539
  Batch 200 Loss 6.8543
  Batch 300 Loss 7.0848
  Batch 400 Loss 7.0118
  Batch 500 Loss 7.1689
  Batch 600 Loss 6.2429
  Batch 700 Loss 7.1874
Finished epoch 16 in 69.0 seconds
Perplexity training: 2.634

==== Starting epoch 17 ====
  Batch 0 Loss 5.5727
  Batch 100 Loss 8.0777
  Batch 200 Loss 6.5866
  Batch 300 Loss 7.0757
  Batch 400 Loss 6.5259
  Batch 500 Loss 6.6613
  Batch 600 Loss 5.9314
  Batch 700 Loss 6.8232
Finished epoch 17 in 70.0 seconds
Perplexity training: 2.473
Measuring development set...
Recognition iteration 0 Loss 52.645
Recognition finished, iteration 100 Loss 2.723
Recognition iteration 0 Loss 54.157
Recognition finished, iteration 100 Loss 2.463
Recognition iteration 0 Loss 51.236
Recognition finished, iteration 100 Loss 2.244
Recognition iteration 0 Loss 53.898
Recognition finished, iteration 100 Loss 2.220
Perplexity dev: 5.386

==== Starting epoch 18 ====
  Batch 0 Loss 5.2025
  Batch 100 Loss 7.2474
  Batch 200 Loss 6.2260
  Batch 300 Loss 6.3966
  Batch 400 Loss 6.1463
  Batch 500 Loss 6.3468
  Batch 600 Loss 5.9202
  Batch 700 Loss 6.2404
Finished epoch 18 in 70.0 seconds
Perplexity training: 2.344

==== Starting epoch 19 ====
  Batch 0 Loss 4.6433
  Batch 100 Loss 6.7936
  Batch 200 Loss 6.0127
  Batch 300 Loss 6.2840
  Batch 400 Loss 5.4749
  Batch 500 Loss 5.7854
  Batch 600 Loss 5.3318
  Batch 700 Loss 6.0474
Finished epoch 19 in 70.0 seconds
Perplexity training: 2.233
Measuring development set...
Recognition iteration 0 Loss 55.367
Recognition finished, iteration 100 Loss 2.432
Recognition iteration 0 Loss 56.355
Recognition finished, iteration 100 Loss 2.267
Recognition iteration 0 Loss 53.696
Recognition finished, iteration 100 Loss 1.942
Recognition iteration 0 Loss 56.397
Recognition finished, iteration 100 Loss 1.808
Perplexity dev: 4.990

==== Starting epoch 20 ====
  Batch 0 Loss 4.6197
  Batch 100 Loss 6.5504
  Batch 200 Loss 5.3756
  Batch 300 Loss 5.7342
  Batch 400 Loss 5.5385
  Batch 500 Loss 5.6093
  Batch 600 Loss 5.0349
  Batch 700 Loss 5.7458
Finished epoch 20 in 70.0 seconds
Perplexity training: 2.125

==== Starting epoch 21 ====
  Batch 0 Loss 4.1995
  Batch 100 Loss 6.4243
  Batch 200 Loss 5.1920
  Batch 300 Loss 5.4740
  Batch 400 Loss 5.0217
  Batch 500 Loss 5.4119
  Batch 600 Loss 4.8253
  Batch 700 Loss 5.1672
Finished epoch 21 in 70.0 seconds
Perplexity training: 2.036
Measuring development set...
Recognition iteration 0 Loss 57.092
Recognition finished, iteration 100 Loss 2.240
Recognition iteration 0 Loss 58.748
Recognition finished, iteration 100 Loss 1.907
Recognition iteration 0 Loss 55.602
Recognition finished, iteration 100 Loss 1.901
Recognition iteration 0 Loss 59.074
Recognition finished, iteration 100 Loss 1.667
Perplexity dev: 5.966

==== Starting epoch 22 ====
  Batch 0 Loss 4.0221
  Batch 100 Loss 6.1453
  Batch 200 Loss 5.0083
  Batch 300 Loss 5.5828
  Batch 400 Loss 4.8248
  Batch 500 Loss 4.8372
  Batch 600 Loss 4.5135
  Batch 700 Loss 5.1886
Finished epoch 22 in 70.0 seconds
Perplexity training: 1.958

==== Starting epoch 23 ====
  Batch 0 Loss 3.4075
  Batch 100 Loss 5.7227
  Batch 200 Loss 4.5529
  Batch 300 Loss 4.9446
  Batch 400 Loss 4.4516
  Batch 500 Loss 5.2423
  Batch 600 Loss 4.3730
  Batch 700 Loss 4.9611
Finished epoch 23 in 71.0 seconds
Perplexity training: 1.891
Measuring development set...
Recognition iteration 0 Loss 58.753
Recognition finished, iteration 100 Loss 2.050
Recognition iteration 0 Loss 60.466
Recognition finished, iteration 100 Loss 1.617
Recognition iteration 0 Loss 56.917
Recognition finished, iteration 100 Loss 1.426
Recognition iteration 0 Loss 60.779
Recognition finished, iteration 100 Loss 1.558
Perplexity dev: 6.290

==== Starting epoch 24 ====
  Batch 0 Loss 3.1985
  Batch 100 Loss 5.4659
  Batch 200 Loss 4.4199
  Batch 300 Loss 4.8253
  Batch 400 Loss 4.6894
  Batch 500 Loss 4.7378
  Batch 600 Loss 4.1645
  Batch 700 Loss 4.5675
Finished epoch 24 in 74.0 seconds
Perplexity training: 1.837

==== Starting epoch 25 ====
  Batch 0 Loss 3.4646
  Batch 100 Loss 5.0319
  Batch 200 Loss 4.1971
  Batch 300 Loss 4.5534
  Batch 400 Loss 4.2748
  Batch 500 Loss 4.2241
  Batch 600 Loss 4.1758
  Batch 700 Loss 4.1547
Finished epoch 25 in 79.0 seconds
Perplexity training: 1.778
Measuring development set...
Recognition iteration 0 Loss 60.508
Recognition finished, iteration 100 Loss 1.569
Recognition iteration 0 Loss 61.726
Recognition finished, iteration 100 Loss 1.556
Recognition iteration 0 Loss 59.123
Recognition finished, iteration 100 Loss 1.249
Recognition iteration 0 Loss 61.609
Recognition finished, iteration 100 Loss 1.231
Perplexity dev: 7.581

==== Starting epoch 26 ====
  Batch 0 Loss 3.1560
  Batch 100 Loss 4.9728
  Batch 200 Loss 3.6781
  Batch 300 Loss 4.2794
  Batch 400 Loss 4.0296
  Batch 500 Loss 4.2758
  Batch 600 Loss 4.0614
  Batch 700 Loss 4.0955
Finished epoch 26 in 79.0 seconds
Perplexity training: 1.728

==== Starting epoch 27 ====
  Batch 0 Loss 3.0259
  Batch 100 Loss 4.6371
  Batch 200 Loss 3.9479
  Batch 300 Loss 3.6856
  Batch 400 Loss 3.8080
  Batch 500 Loss 3.9624
  Batch 600 Loss 3.7845
  Batch 700 Loss 3.9747
Finished epoch 27 in 80.0 seconds
Perplexity training: 1.678
Measuring development set...
Recognition iteration 0 Loss 61.455
Recognition finished, iteration 100 Loss 1.517
Recognition iteration 0 Loss 62.220
Recognition finished, iteration 100 Loss 1.350
Recognition iteration 0 Loss 59.886
Recognition finished, iteration 100 Loss 1.286
Recognition iteration 0 Loss 62.448
Recognition finished, iteration 100 Loss 1.007
Perplexity dev: 7.148

==== Starting epoch 28 ====
  Batch 0 Loss 2.9480
  Batch 100 Loss 4.5311
  Batch 200 Loss 3.3985
  Batch 300 Loss 3.5292
  Batch 400 Loss 3.8504
  Batch 500 Loss 3.9422
  Batch 600 Loss 3.7968
  Batch 700 Loss 3.8679
Finished epoch 28 in 80.0 seconds
Perplexity training: 1.645

==== Starting epoch 29 ====
  Batch 0 Loss 2.5626
  Batch 100 Loss 4.5322
  Batch 200 Loss 3.8159
  Batch 300 Loss 3.4347
  Batch 400 Loss 3.4492
  Batch 500 Loss 3.9255
  Batch 600 Loss 2.8456
  Batch 700 Loss 3.7879
Finished epoch 29 in 80.0 seconds
Perplexity training: 1.611
Measuring development set...
Recognition iteration 0 Loss 64.510
Recognition finished, iteration 100 Loss 1.358
Recognition iteration 0 Loss 65.058
Recognition finished, iteration 100 Loss 1.428
Recognition iteration 0 Loss 62.892
Recognition finished, iteration 100 Loss 1.005
Recognition iteration 0 Loss 65.408
Recognition finished, iteration 100 Loss 0.964
Perplexity dev: 12.840

==== Starting epoch 30 ====
  Batch 0 Loss 2.7272
  Batch 100 Loss 4.1626
  Batch 200 Loss 3.6275
  Batch 300 Loss 3.5139
  Batch 400 Loss 3.4686
  Batch 500 Loss 3.8922
  Batch 600 Loss 3.3960
  Batch 700 Loss 3.6034
Finished epoch 30 in 80.0 seconds
Perplexity training: 1.579

==== Starting epoch 31 ====
  Batch 0 Loss 2.3934
  Batch 100 Loss 4.0052
  Batch 200 Loss 3.1418
  Batch 300 Loss 3.4647
  Batch 400 Loss 2.9106
  Batch 500 Loss 3.1195
  Batch 600 Loss 3.3023
  Batch 700 Loss 3.9886
Finished epoch 31 in 81.0 seconds
Perplexity training: 1.548
Measuring development set...
Recognition iteration 0 Loss 66.295
Recognition finished, iteration 100 Loss 1.305
Recognition iteration 0 Loss 67.438
Recognition finished, iteration 100 Loss 1.365
Recognition iteration 0 Loss 64.788
Recognition finished, iteration 100 Loss 0.981
Recognition iteration 0 Loss 67.355
Recognition finished, iteration 100 Loss 0.967
Perplexity dev: 32.933
Finished training in 2385.73 seconds
Finished training after development set stopped improving.
