Starting training procedure.
Loading training set...
2019-06-30 19:02:07.014740: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-30 19:02:07.025394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-30 19:02:07.026008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-30 19:02:07.026371: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-30 19:02:07.027740: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-30 19:02:07.029193: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-30 19:02:07.029537: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-30 19:02:07.031131: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-30 19:02:07.032817: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-30 19:02:07.036922: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-30 19:02:07.040325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-06-30 19:02:07.040725: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-06-30 19:02:07.766879: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2652a20 executing computations on platform CUDA. Devices:
2019-06-30 19:02:07.766947: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-30 19:02:07.766961: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-30 19:02:07.788996: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2019-06-30 19:02:07.792919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x26678e0 executing computations on platform Host. Devices:
2019-06-30 19:02:07.793104: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-06-30 19:02:07.798736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-30 19:02:07.799346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-30 19:02:07.799395: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-30 19:02:07.799406: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-30 19:02:07.799415: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-30 19:02:07.799423: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-30 19:02:07.799431: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-30 19:02:07.799439: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-30 19:02:07.799457: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-30 19:02:07.802121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-06-30 19:02:07.802183: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-30 19:02:07.804018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-30 19:02:07.804039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 
2019-06-30 19:02:07.804045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y 
2019-06-30 19:02:07.804048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N 
2019-06-30 19:02:07.806830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30458 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
2019-06-30 19:02:07.807824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 927 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
Saving vocab defined by training set...
Loading development set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


=== Model ===
Name: rnnpbnmt
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0.1
Num PBs: 1024
Bind hard: True
Binding strength: 1.0
Autoencode: False
PB learning rate: 0.01
Sigma: 0
p_reset: 0.2
Max recog epochs: 100


=== Training ===
Max epochs: 0
Early stopping steps: 20
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-06-30 19:02:13.953796: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-30 19:02:15.522107: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0630 19:02:15.917621 140198691284800 deprecation.py:323] From /lhome/daniesso/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 59.6760
  Batch 100 Loss 37.2587
  Batch 200 Loss 36.3371
  Batch 300 Loss 32.2081
  Batch 400 Loss 31.7410
  Batch 500 Loss 28.4404
  Batch 600 Loss 28.0752
  Batch 700 Loss 29.8817
Resetting 10051 PBs
Finished epoch 1 in 86.0 seconds
Perplexity training: 81.994
Measuring development set...
Recognition iteration 0 Loss 28.191
Recognition finished, iteration 100 Loss 24.909
Recognition iteration 0 Loss 29.727
Recognition finished, iteration 100 Loss 26.248
Recognition iteration 0 Loss 26.807
Recognition finished, iteration 100 Loss 23.888
Recognition iteration 0 Loss 28.984
Recognition finished, iteration 100 Loss 25.801
Perplexity dev: 36.531

==== Starting epoch 2 ====
  Batch 0 Loss 27.0456
  Batch 100 Loss 26.9001
  Batch 200 Loss 28.7857
  Batch 300 Loss 25.8945
  Batch 400 Loss 26.0887
  Batch 500 Loss 23.4933
  Batch 600 Loss 23.3403
  Batch 700 Loss 25.7521
Resetting 10014 PBs
Finished epoch 2 in 87.0 seconds
Perplexity training: 26.525

==== Starting epoch 3 ====
  Batch 0 Loss 24.4141
  Batch 100 Loss 23.8894
  Batch 200 Loss 25.7618
  Batch 300 Loss 23.0772
  Batch 400 Loss 22.8814
  Batch 500 Loss 20.8418
  Batch 600 Loss 20.1824
  Batch 700 Loss 22.6475
Resetting 10009 PBs
Finished epoch 3 in 85.0 seconds
Perplexity training: 18.835
Measuring development set...
Recognition iteration 0 Loss 25.623
Recognition finished, iteration 100 Loss 14.871
Recognition iteration 0 Loss 27.032
Recognition finished, iteration 100 Loss 15.392
Recognition iteration 0 Loss 24.794
Recognition finished, iteration 100 Loss 14.096
Recognition iteration 0 Loss 26.162
Recognition finished, iteration 100 Loss 14.890
Perplexity dev: 13.524

==== Starting epoch 4 ====
  Batch 0 Loss 21.6546
  Batch 100 Loss 20.5936
  Batch 200 Loss 22.8743
  Batch 300 Loss 20.8925
  Batch 400 Loss 20.5143
  Batch 500 Loss 18.0466
  Batch 600 Loss 17.6687
  Batch 700 Loss 20.7877
Resetting 10015 PBs
Finished epoch 4 in 83.0 seconds
Perplexity training: 14.337

==== Starting epoch 5 ====
  Batch 0 Loss 18.8949
  Batch 100 Loss 18.5536
  Batch 200 Loss 19.9403
  Batch 300 Loss 17.7276
  Batch 400 Loss 17.8534
  Batch 500 Loss 16.8207
  Batch 600 Loss 16.3743
  Batch 700 Loss 17.8141
Resetting 10121 PBs
Finished epoch 5 in 86.0 seconds
Perplexity training: 11.616
Measuring development set...
Recognition iteration 0 Loss 24.543
Recognition finished, iteration 100 Loss 9.328
Recognition iteration 0 Loss 26.376
Recognition finished, iteration 100 Loss 10.028
Recognition iteration 0 Loss 23.979
Recognition finished, iteration 100 Loss 8.767
Recognition iteration 0 Loss 24.974
Recognition finished, iteration 100 Loss 9.247
Perplexity dev: 7.973

==== Starting epoch 6 ====
  Batch 0 Loss 16.7426
  Batch 100 Loss 16.6285
  Batch 200 Loss 18.4942
  Batch 300 Loss 16.8192
  Batch 400 Loss 16.3325
  Batch 500 Loss 15.1450
  Batch 600 Loss 15.0765
  Batch 700 Loss 16.4302
Resetting 9852 PBs
Finished epoch 6 in 87.0 seconds
Perplexity training: 9.881

==== Starting epoch 7 ====
  Batch 0 Loss 15.6467
  Batch 100 Loss 15.6828
  Batch 200 Loss 17.3160
  Batch 300 Loss 15.4108
  Batch 400 Loss 13.5572
  Batch 500 Loss 14.3497
  Batch 600 Loss 14.4752
  Batch 700 Loss 15.6723
Resetting 10055 PBs
Finished epoch 7 in 88.0 seconds
Perplexity training: 8.656
Measuring development set...
Recognition iteration 0 Loss 24.266
Recognition finished, iteration 100 Loss 5.700
Recognition iteration 0 Loss 25.885
Recognition finished, iteration 100 Loss 6.443
Recognition iteration 0 Loss 23.393
Recognition finished, iteration 100 Loss 5.398
Recognition iteration 0 Loss 24.656
Recognition finished, iteration 100 Loss 5.606
Perplexity dev: 5.939

==== Starting epoch 8 ====
  Batch 0 Loss 13.5648
  Batch 100 Loss 14.0301
  Batch 200 Loss 15.7146
  Batch 300 Loss 14.1010
  Batch 400 Loss 13.6758
  Batch 500 Loss 12.4969
  Batch 600 Loss 11.2478
  Batch 700 Loss 14.1107
Resetting 9955 PBs
Finished epoch 8 in 89.0 seconds
Perplexity training: 7.880

==== Starting epoch 9 ====
  Batch 0 Loss 12.9340
  Batch 100 Loss 13.1309
  Batch 200 Loss 14.2951
  Batch 300 Loss 14.1866
  Batch 400 Loss 12.5278
  Batch 500 Loss 13.2032
  Batch 600 Loss 10.9843
  Batch 700 Loss 12.2444
Resetting 10060 PBs
Finished epoch 9 in 87.0 seconds
Perplexity training: 7.173
Measuring development set...
Recognition iteration 0 Loss 23.869
Recognition finished, iteration 100 Loss 3.335
Recognition iteration 0 Loss 25.453
Recognition finished, iteration 100 Loss 3.886
Recognition iteration 0 Loss 23.390
Recognition finished, iteration 100 Loss 3.083
Recognition iteration 0 Loss 24.070
Recognition finished, iteration 100 Loss 3.196
Perplexity dev: 5.036

==== Starting epoch 10 ====
  Batch 0 Loss 12.7064
  Batch 100 Loss 12.4817
  Batch 200 Loss 12.8071
  Batch 300 Loss 11.5332
  Batch 400 Loss 11.2885
  Batch 500 Loss 10.7756
  Batch 600 Loss 9.4504
  Batch 700 Loss 10.8752
Resetting 10058 PBs
Finished epoch 10 in 88.0 seconds
Perplexity training: 6.813

==== Starting epoch 11 ====
  Batch 0 Loss 11.8774
  Batch 100 Loss 12.2746
  Batch 200 Loss 12.2095
  Batch 300 Loss 11.7890
  Batch 400 Loss 10.5335
  Batch 500 Loss 11.2182
  Batch 600 Loss 9.2416
  Batch 700 Loss 11.3917
Resetting 10043 PBs
Finished epoch 11 in 89.0 seconds
Perplexity training: 6.419
Measuring development set...
Recognition iteration 0 Loss 23.946
Recognition finished, iteration 100 Loss 1.859
Recognition iteration 0 Loss 24.993
Recognition finished, iteration 100 Loss 2.344
Recognition iteration 0 Loss 23.073
Recognition finished, iteration 100 Loss 1.681
Recognition iteration 0 Loss 23.610
Recognition finished, iteration 100 Loss 1.672
Perplexity dev: 4.334

==== Starting epoch 12 ====
  Batch 0 Loss 11.0486
  Batch 100 Loss 9.9115
  Batch 200 Loss 11.7297
  Batch 300 Loss 10.9308
  Batch 400 Loss 9.5628
  Batch 500 Loss 9.8565
  Batch 600 Loss 8.3592
  Batch 700 Loss 11.5042
Resetting 10068 PBs
Finished epoch 12 in 89.0 seconds
Perplexity training: 6.160

==== Starting epoch 13 ====
  Batch 0 Loss 11.6984
  Batch 100 Loss 10.2200
  Batch 200 Loss 11.8271
  Batch 300 Loss 9.1066
  Batch 400 Loss 9.9512
  Batch 500 Loss 7.6539
  Batch 600 Loss 6.3460
  Batch 700 Loss 9.8535
Resetting 9970 PBs
Finished epoch 13 in 92.0 seconds
Perplexity training: 5.805
Measuring development set...
Recognition iteration 0 Loss 23.715
Recognition finished, iteration 100 Loss 0.960
Recognition iteration 0 Loss 25.124
Recognition finished, iteration 100 Loss 1.255
Recognition iteration 0 Loss 23.020
Recognition finished, iteration 100 Loss 0.947
Recognition iteration 0 Loss 23.535
Recognition finished, iteration 100 Loss 0.902
Perplexity dev: 4.161

==== Starting epoch 14 ====
  Batch 0 Loss 10.4422
  Batch 100 Loss 9.8464
  Batch 200 Loss 11.0308
  Batch 300 Loss 8.4433
  Batch 400 Loss 8.7277
  Batch 500 Loss 8.6443
  Batch 600 Loss 6.5068
  Batch 700 Loss 8.2971
Resetting 9975 PBs
Finished epoch 14 in 92.0 seconds
Perplexity training: 5.539

==== Starting epoch 15 ====
  Batch 0 Loss 9.5512
  Batch 100 Loss 9.0142
  Batch 200 Loss 12.0088
  Batch 300 Loss 9.3026
  Batch 400 Loss 8.0723
  Batch 500 Loss 8.3175
  Batch 600 Loss 8.6053
  Batch 700 Loss 8.9826
Resetting 9980 PBs
Finished epoch 15 in 92.0 seconds
Perplexity training: 5.411
Measuring development set...
Recognition iteration 0 Loss 23.746
Recognition finished, iteration 100 Loss 0.551
Recognition iteration 0 Loss 24.944
Recognition finished, iteration 100 Loss 0.743
Recognition iteration 0 Loss 23.059
Recognition finished, iteration 100 Loss 0.522
Recognition iteration 0 Loss 23.850
Recognition finished, iteration 100 Loss 0.522
Perplexity dev: 4.222

==== Starting epoch 16 ====
  Batch 0 Loss 8.4838
  Batch 100 Loss 9.4019
  Batch 200 Loss 8.5705
  Batch 300 Loss 7.3267
  Batch 400 Loss 7.5329
  Batch 500 Loss 7.1956
  Batch 600 Loss 6.4563
  Batch 700 Loss 8.6483
Resetting 10027 PBs
Finished epoch 16 in 91.0 seconds
Perplexity training: 5.222

==== Starting epoch 17 ====
  Batch 0 Loss 10.3115
  Batch 100 Loss 8.8864
  Batch 200 Loss 7.0337
  Batch 300 Loss 7.7363
  Batch 400 Loss 6.8775
  Batch 500 Loss 6.8780
  Batch 600 Loss 7.7699
  Batch 700 Loss 9.2997
Resetting 10035 PBs
Finished epoch 17 in 90.0 seconds
Perplexity training: 5.152
Measuring development set...
Recognition iteration 0 Loss 23.632
Recognition finished, iteration 100 Loss 0.328
Recognition iteration 0 Loss 24.878
Recognition finished, iteration 100 Loss 0.444
Recognition iteration 0 Loss 22.800
Recognition finished, iteration 100 Loss 0.292
Recognition iteration 0 Loss 23.343
Recognition finished, iteration 100 Loss 0.294
Perplexity dev: 3.920

==== Starting epoch 18 ====
  Batch 0 Loss 8.2463
  Batch 100 Loss 7.3123
  Batch 200 Loss 8.6173
  Batch 300 Loss 6.9387
  Batch 400 Loss 8.0501
  Batch 500 Loss 7.3426
  Batch 600 Loss 8.1685
  Batch 700 Loss 8.5627
Resetting 9995 PBs
Finished epoch 18 in 90.0 seconds
Perplexity training: 4.973

==== Starting epoch 19 ====
  Batch 0 Loss 7.1449
  Batch 100 Loss 7.0173
  Batch 200 Loss 6.6790
  Batch 300 Loss 7.7094
  Batch 400 Loss 7.1785
  Batch 500 Loss 6.7373
  Batch 600 Loss 8.8650
  Batch 700 Loss 8.1831
Resetting 9842 PBs
Finished epoch 19 in 91.0 seconds
Perplexity training: 4.924
Measuring development set...
Recognition iteration 0 Loss 23.215
Recognition finished, iteration 100 Loss 0.210
Recognition iteration 0 Loss 24.585
Recognition finished, iteration 100 Loss 0.279
Recognition iteration 0 Loss 22.673
Recognition finished, iteration 100 Loss 0.192
Recognition iteration 0 Loss 23.018
Recognition finished, iteration 100 Loss 0.168
Perplexity dev: 3.813

==== Starting epoch 20 ====
  Batch 0 Loss 7.1443
  Batch 100 Loss 6.7658
  Batch 200 Loss 7.4128
  Batch 300 Loss 9.1904
  Batch 400 Loss 6.0959
  Batch 500 Loss 6.5335
  Batch 600 Loss 6.7471
  Batch 700 Loss 6.1777
Resetting 10121 PBs
Finished epoch 20 in 92.0 seconds
Perplexity training: 4.775

==== Starting epoch 21 ====
  Batch 0 Loss 6.4880
  Batch 100 Loss 6.9230
  Batch 200 Loss 5.6308
  Batch 300 Loss 6.2490
  Batch 400 Loss 7.5006
  Batch 500 Loss 4.9976
  Batch 600 Loss 8.6185
  Batch 700 Loss 7.4566
Resetting 10052 PBs
Finished epoch 21 in 92.0 seconds
Perplexity training: 4.728
Measuring development set...
Recognition iteration 0 Loss 22.738
Recognition finished, iteration 100 Loss 0.131
Recognition iteration 0 Loss 24.442
Recognition finished, iteration 100 Loss 0.155
Recognition iteration 0 Loss 22.628
Recognition finished, iteration 100 Loss 0.113
Recognition iteration 0 Loss 23.188
Recognition finished, iteration 100 Loss 0.117
Perplexity dev: 4.022

==== Starting epoch 22 ====
  Batch 0 Loss 7.2070
  Batch 100 Loss 5.4358
  Batch 200 Loss 8.5304
  Batch 300 Loss 6.1102
  Batch 400 Loss 5.9066
  Batch 500 Loss 5.4704
  Batch 600 Loss 7.0745
  Batch 700 Loss 6.9953
Resetting 9987 PBs
Finished epoch 22 in 93.0 seconds
Perplexity training: 4.590

==== Starting epoch 23 ====
  Batch 0 Loss 6.7198
  Batch 100 Loss 5.3423
  Batch 200 Loss 6.3837
  Batch 300 Loss 7.5270
  Batch 400 Loss 6.7665
  Batch 500 Loss 6.1062
  Batch 600 Loss 5.5151
  Batch 700 Loss 7.2383
Resetting 10015 PBs
Finished epoch 23 in 93.0 seconds
Perplexity training: 4.512
Measuring development set...
Recognition iteration 0 Loss 23.030
Recognition finished, iteration 100 Loss 0.096
Recognition iteration 0 Loss 24.428
Recognition finished, iteration 100 Loss 0.108
Recognition iteration 0 Loss 22.742
Recognition finished, iteration 100 Loss 0.066
Recognition iteration 0 Loss 22.934
Recognition finished, iteration 100 Loss 0.079
Perplexity dev: 3.275

==== Starting epoch 24 ====
  Batch 0 Loss 7.8342
  Batch 100 Loss 6.6820
  Batch 200 Loss 6.9626
  Batch 300 Loss 6.9440
  Batch 400 Loss 4.8280
  Batch 500 Loss 5.6701
  Batch 600 Loss 5.0886
  Batch 700 Loss 5.9861
Resetting 10032 PBs
Finished epoch 24 in 95.0 seconds
Perplexity training: 4.463

==== Starting epoch 25 ====
  Batch 0 Loss 5.1642
  Batch 100 Loss 5.2853
  Batch 200 Loss 4.4902
  Batch 300 Loss 6.4094
  Batch 400 Loss 7.0428
  Batch 500 Loss 5.0667
  Batch 600 Loss 6.1486
  Batch 700 Loss 6.8700
Resetting 9943 PBs
Finished epoch 25 in 96.0 seconds
Perplexity training: 4.407
Measuring development set...
Recognition iteration 0 Loss 22.564
Recognition finished, iteration 100 Loss 0.067
Recognition iteration 0 Loss 24.948
Recognition finished, iteration 100 Loss 0.076
Recognition iteration 0 Loss 22.532
Recognition finished, iteration 100 Loss 0.051
Recognition iteration 0 Loss 22.761
Recognition finished, iteration 100 Loss 0.057
Perplexity dev: 3.139

==== Starting epoch 26 ====
  Batch 0 Loss 4.6866
  Batch 100 Loss 6.9402
  Batch 200 Loss 5.6203
  Batch 300 Loss 7.0099
  Batch 400 Loss 6.8023
  Batch 500 Loss 4.8242
  Batch 600 Loss 6.0100
  Batch 700 Loss 4.7951
Resetting 9993 PBs
Finished epoch 26 in 96.0 seconds
Perplexity training: 4.352

==== Starting epoch 27 ====
  Batch 0 Loss 7.2606
  Batch 100 Loss 5.5225
  Batch 200 Loss 6.3652
  Batch 300 Loss 6.0867
  Batch 400 Loss 4.7060
  Batch 500 Loss 5.6126
  Batch 600 Loss 5.7286
  Batch 700 Loss 6.3980
Resetting 10221 PBs
Finished epoch 27 in 98.0 seconds
Perplexity training: 4.188
Measuring development set...
Recognition iteration 0 Loss 22.609
Recognition finished, iteration 100 Loss 0.052
Recognition iteration 0 Loss 24.220
Recognition finished, iteration 100 Loss 0.063
Recognition iteration 0 Loss 22.732
Recognition finished, iteration 100 Loss 0.035
Recognition iteration 0 Loss 22.978
Recognition finished, iteration 100 Loss 0.040
Perplexity dev: 3.483

==== Starting epoch 28 ====
  Batch 0 Loss 7.4968
  Batch 100 Loss 6.1938
  Batch 200 Loss 6.8066
  Batch 300 Loss 4.9273
  Batch 400 Loss 5.8839
  Batch 500 Loss 5.7232
  Batch 600 Loss 6.3895
  Batch 700 Loss 6.3593
Resetting 9926 PBs
Finished epoch 28 in 97.0 seconds
Perplexity training: 4.246

==== Starting epoch 29 ====
  Batch 0 Loss 5.9000
  Batch 100 Loss 4.7090
  Batch 200 Loss 6.1382
  Batch 300 Loss 6.5969
  Batch 400 Loss 4.7960
  Batch 500 Loss 3.6857
  Batch 600 Loss 4.5678
  Batch 700 Loss 6.0761
Resetting 10213 PBs
Finished epoch 29 in 98.0 seconds
Perplexity training: 4.076
Measuring development set...
Recognition iteration 0 Loss 22.626
Recognition finished, iteration 100 Loss 0.042
Recognition iteration 0 Loss 24.276
Recognition finished, iteration 100 Loss 0.037
Recognition iteration 0 Loss 22.562
Recognition finished, iteration 100 Loss 0.026
Recognition iteration 0 Loss 22.746
Recognition finished, iteration 100 Loss 0.034
Perplexity dev: 3.003

==== Starting epoch 30 ====
  Batch 0 Loss 6.1600
  Batch 100 Loss 6.0665
  Batch 200 Loss 6.8554
  Batch 300 Loss 7.4808
  Batch 400 Loss 6.9592
  Batch 500 Loss 3.4705
  Batch 600 Loss 7.3741
  Batch 700 Loss 6.8983
Resetting 9938 PBs
Finished epoch 30 in 98.0 seconds
Perplexity training: 4.115

==== Starting epoch 31 ====
  Batch 0 Loss 6.1449
  Batch 100 Loss 6.2970
  Batch 200 Loss 7.2828
  Batch 300 Loss 6.6762
  Batch 400 Loss 4.2859
  Batch 500 Loss 5.0811
  Batch 600 Loss 4.2733
  Batch 700 Loss 6.2863
Resetting 9899 PBs
Finished epoch 31 in 100.0 seconds
Perplexity training: 4.057
Measuring development set...
Recognition iteration 0 Loss 22.378
Recognition finished, iteration 100 Loss 0.029
Recognition iteration 0 Loss 24.500
Recognition finished, iteration 100 Loss 0.032
Recognition iteration 0 Loss 22.385
Recognition finished, iteration 100 Loss 0.020
Recognition iteration 0 Loss 22.676
Recognition finished, iteration 100 Loss 0.026
Perplexity dev: 3.243

==== Starting epoch 32 ====
  Batch 0 Loss 5.3301
  Batch 100 Loss 4.3983
  Batch 200 Loss 6.3329
  Batch 300 Loss 6.2881
  Batch 400 Loss 4.7033
  Batch 500 Loss 3.8854
  Batch 600 Loss 4.1493
  Batch 700 Loss 3.8336
Resetting 10080 PBs
Finished epoch 32 in 99.0 seconds
Perplexity training: 4.016

==== Starting epoch 33 ====
  Batch 0 Loss 5.3270
  Batch 100 Loss 6.5403
  Batch 200 Loss 5.9069
  Batch 300 Loss 6.2608
  Batch 400 Loss 3.3573
  Batch 500 Loss 5.3492
  Batch 600 Loss 3.9244
  Batch 700 Loss 4.8527
Resetting 10014 PBs
Finished epoch 33 in 101.0 seconds
Perplexity training: 4.037
Measuring development set...
Recognition iteration 0 Loss 22.371
Recognition finished, iteration 100 Loss 0.031
Recognition iteration 0 Loss 24.061
Recognition finished, iteration 100 Loss 0.027
Recognition iteration 0 Loss 22.326
Recognition finished, iteration 100 Loss 0.016
Recognition iteration 0 Loss 22.610
Recognition finished, iteration 100 Loss 0.022
Perplexity dev: 3.213

==== Starting epoch 34 ====
  Batch 0 Loss 7.6139
  Batch 100 Loss 7.1134
  Batch 200 Loss 5.2952
  Batch 300 Loss 4.4827
  Batch 400 Loss 5.4693
  Batch 500 Loss 3.8171
  Batch 600 Loss 2.8428
  Batch 700 Loss 6.7406
Resetting 9955 PBs
Finished epoch 34 in 100.0 seconds
Perplexity training: 3.925

==== Starting epoch 35 ====
  Batch 0 Loss 4.4407
  Batch 100 Loss 4.5778
  Batch 200 Loss 7.8774
  Batch 300 Loss 3.1867
  Batch 400 Loss 7.6335
  Batch 500 Loss 4.7445
  Batch 600 Loss 4.8144
  Batch 700 Loss 6.0559
Resetting 10000 PBs
Finished epoch 35 in 102.0 seconds
Perplexity training: 3.876
Measuring development set...
Recognition iteration 0 Loss 22.405
Recognition finished, iteration 100 Loss 0.018
Recognition iteration 0 Loss 24.136
Recognition finished, iteration 100 Loss 0.022
Recognition iteration 0 Loss 22.286
Recognition finished, iteration 100 Loss 0.014
Recognition iteration 0 Loss 22.637
Recognition finished, iteration 100 Loss 0.018
Perplexity dev: 2.904

==== Starting epoch 36 ====
  Batch 0 Loss 4.6280
  Batch 100 Loss 4.0172
  Batch 200 Loss 4.5434
  Batch 300 Loss 4.8804
  Batch 400 Loss 3.9965
  Batch 500 Loss 4.4802
  Batch 600 Loss 6.1220
  Batch 700 Loss 4.7281
Resetting 10014 PBs
Finished epoch 36 in 101.0 seconds
Perplexity training: 3.860

==== Starting epoch 37 ====
  Batch 0 Loss 4.0942
  Batch 100 Loss 3.8023
  Batch 200 Loss 5.9121
  Batch 300 Loss 4.9152
  Batch 400 Loss 4.3161
  Batch 500 Loss 4.5166
  Batch 600 Loss 5.6863
  Batch 700 Loss 7.7440
Resetting 10012 PBs
Finished epoch 37 in 102.0 seconds
Perplexity training: 3.803
Measuring development set...
Recognition iteration 0 Loss 22.601
Recognition finished, iteration 100 Loss 0.017
Recognition iteration 0 Loss 24.507
Recognition finished, iteration 100 Loss 0.020
Recognition iteration 0 Loss 22.390
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 22.804
Recognition finished, iteration 100 Loss 0.015
Perplexity dev: 3.068

==== Starting epoch 38 ====
  Batch 0 Loss 6.8892
  Batch 100 Loss 4.7385
  Batch 200 Loss 5.9734
  Batch 300 Loss 6.0946
  Batch 400 Loss 5.8166
  Batch 500 Loss 3.5879
  Batch 600 Loss 4.3116
  Batch 700 Loss 6.9659
Resetting 10184 PBs
Finished epoch 38 in 101.0 seconds
Perplexity training: 3.808

==== Starting epoch 39 ====
  Batch 0 Loss 4.5689
  Batch 100 Loss 4.4560
  Batch 200 Loss 5.2280
  Batch 300 Loss 5.7992
  Batch 400 Loss 5.3453
  Batch 500 Loss 3.7413
  Batch 600 Loss 4.7435
  Batch 700 Loss 6.5888
Resetting 9902 PBs
Finished epoch 39 in 102.0 seconds
Perplexity training: 3.882
Measuring development set...
Recognition iteration 0 Loss 22.045
Recognition finished, iteration 100 Loss 0.014
Recognition iteration 0 Loss 24.100
Recognition finished, iteration 100 Loss 0.015
Recognition iteration 0 Loss 22.447
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 22.494
Recognition finished, iteration 100 Loss 0.012
Perplexity dev: 2.888

==== Starting epoch 40 ====
  Batch 0 Loss 8.0054
  Batch 100 Loss 5.1406
  Batch 200 Loss 5.6754
  Batch 300 Loss 5.3787
  Batch 400 Loss 5.6424
  Batch 500 Loss 5.0578
  Batch 600 Loss 4.9412
  Batch 700 Loss 7.3389
Resetting 9783 PBs
Finished epoch 40 in 103.0 seconds
Perplexity training: 3.733

==== Starting epoch 41 ====
  Batch 0 Loss 4.0027
  Batch 100 Loss 4.5512
  Batch 200 Loss 4.9583
  Batch 300 Loss 5.4911
  Batch 400 Loss 4.2792
  Batch 500 Loss 2.8806
  Batch 600 Loss 3.4376
  Batch 700 Loss 5.7145
Resetting 9869 PBs
Finished epoch 41 in 104.0 seconds
Perplexity training: 3.635
Measuring development set...
Recognition iteration 0 Loss 22.131
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 24.084
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 22.332
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 22.708
Recognition finished, iteration 100 Loss 0.011
Perplexity dev: 2.702

==== Starting epoch 42 ====
  Batch 0 Loss 5.1570
  Batch 100 Loss 3.6416
  Batch 200 Loss 3.6026
  Batch 300 Loss 7.6188
  Batch 400 Loss 4.8817
  Batch 500 Loss 3.1818
  Batch 600 Loss 4.3907
  Batch 700 Loss 4.6660
Resetting 9933 PBs
Finished epoch 42 in 103.0 seconds
Perplexity training: 3.657

==== Starting epoch 43 ====
  Batch 0 Loss 4.0439
  Batch 100 Loss 6.1894
  Batch 200 Loss 6.3759
  Batch 300 Loss 5.0366
  Batch 400 Loss 5.2829
  Batch 500 Loss 6.4422
  Batch 600 Loss 4.8244
  Batch 700 Loss 7.4062
Resetting 10168 PBs
Finished epoch 43 in 105.0 seconds
Perplexity training: 3.642
Measuring development set...
Recognition iteration 0 Loss 22.121
Recognition finished, iteration 100 Loss 0.011
Recognition iteration 0 Loss 23.624
Recognition finished, iteration 100 Loss 0.013
Recognition iteration 0 Loss 22.469
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 22.635
Recognition finished, iteration 100 Loss 0.010
Perplexity dev: 3.561

==== Starting epoch 44 ====
  Batch 0 Loss 4.8904
  Batch 100 Loss 4.9542
  Batch 200 Loss 6.2205
  Batch 300 Loss 3.6968
  Batch 400 Loss 5.1741
  Batch 500 Loss 4.0982
  Batch 600 Loss 4.4252
  Batch 700 Loss 5.3032
Resetting 9863 PBs
Finished epoch 44 in 104.0 seconds
Perplexity training: 3.645

==== Starting epoch 45 ====
  Batch 0 Loss 3.3591
  Batch 100 Loss 3.5007
  Batch 200 Loss 4.7307
  Batch 300 Loss 6.4340
  Batch 400 Loss 6.2186
  Batch 500 Loss 5.5396
  Batch 600 Loss 3.8558
  Batch 700 Loss 5.2846
Resetting 9844 PBs
Finished epoch 45 in 105.0 seconds
Perplexity training: 3.565
Measuring development set...
Recognition iteration 0 Loss 22.234
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 23.972
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 22.768
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 23.076
Recognition finished, iteration 100 Loss 0.009
Perplexity dev: 4.145

==== Starting epoch 46 ====
  Batch 0 Loss 4.3935
  Batch 100 Loss 2.5237
  Batch 200 Loss 3.3205
  Batch 300 Loss 6.3447
  Batch 400 Loss 5.3474
  Batch 500 Loss 3.6974
  Batch 600 Loss 3.7462
  Batch 700 Loss 6.0149
Resetting 10124 PBs
Finished epoch 46 in 105.0 seconds
Perplexity training: 3.514

==== Starting epoch 47 ====
  Batch 0 Loss 4.9625
  Batch 100 Loss 4.1149
  Batch 200 Loss 4.5292
  Batch 300 Loss 4.6485
  Batch 400 Loss 3.4909
  Batch 500 Loss 4.9480
  Batch 600 Loss 5.9929
  Batch 700 Loss 5.0991
Resetting 9947 PBs
Finished epoch 47 in 106.0 seconds
Perplexity training: 3.630
Measuring development set...
Recognition iteration 0 Loss 21.765
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 23.804
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 22.340
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.136
Recognition finished, iteration 100 Loss 0.007
Perplexity dev: 4.596

==== Starting epoch 48 ====
  Batch 0 Loss 4.0323
  Batch 100 Loss 4.4945
  Batch 200 Loss 7.4486
  Batch 300 Loss 6.4455
  Batch 400 Loss 6.6618
  Batch 500 Loss 5.1585
  Batch 600 Loss 4.9002
  Batch 700 Loss 4.0688
Resetting 10001 PBs
Finished epoch 48 in 106.0 seconds
Perplexity training: 3.498

==== Starting epoch 49 ====
  Batch 0 Loss 6.1036
  Batch 100 Loss 4.7743
  Batch 200 Loss 4.6144
  Batch 300 Loss 5.0147
  Batch 400 Loss 4.8282
  Batch 500 Loss 4.2119
  Batch 600 Loss 4.0963
  Batch 700 Loss 4.2781
Resetting 10011 PBs
Finished epoch 49 in 107.0 seconds
Perplexity training: 3.505
Measuring development set...
Recognition iteration 0 Loss 22.018
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 23.930
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 22.513
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.300
Recognition finished, iteration 100 Loss 0.007
Perplexity dev: 3.659

==== Starting epoch 50 ====
  Batch 0 Loss 6.5034
  Batch 100 Loss 3.9670
  Batch 200 Loss 3.4072
  Batch 300 Loss 3.2269
  Batch 400 Loss 6.1013
  Batch 500 Loss 6.0846
  Batch 600 Loss 4.5217
  Batch 700 Loss 6.2334
Resetting 9947 PBs
Finished epoch 50 in 120.0 seconds
Perplexity training: 3.476

==== Starting epoch 51 ====
  Batch 0 Loss 5.7109
  Batch 100 Loss 6.1900
  Batch 200 Loss 6.8049
  Batch 300 Loss 4.9257
  Batch 400 Loss 3.7267
  Batch 500 Loss 3.7370
  Batch 600 Loss 3.9285
  Batch 700 Loss 3.4119
Resetting 9879 PBs
Finished epoch 51 in 115.0 seconds
Perplexity training: 3.431
Measuring development set...
Recognition iteration 0 Loss 21.964
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 24.160
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 22.494
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 22.270
Recognition finished, iteration 100 Loss 0.007
Perplexity dev: 3.730

==== Starting epoch 52 ====
  Batch 0 Loss 4.7410
  Batch 100 Loss 4.0287
  Batch 200 Loss 5.6165
  Batch 300 Loss 3.6754
  Batch 400 Loss 5.3338
  Batch 500 Loss 4.0001
  Batch 600 Loss 4.7377
  Batch 700 Loss 3.7332
Resetting 10011 PBs
Finished epoch 52 in 115.0 seconds
Perplexity training: 3.388

==== Starting epoch 53 ====
  Batch 0 Loss 4.0465
  Batch 100 Loss 5.1395
  Batch 200 Loss 4.1117
  Batch 300 Loss 4.1888
  Batch 400 Loss 5.4492
  Batch 500 Loss 5.8438
  Batch 600 Loss 5.2711
  Batch 700 Loss 5.8218
Resetting 9958 PBs
Finished epoch 53 in 116.0 seconds
Perplexity training: 3.438
Measuring development set...
Recognition iteration 0 Loss 21.783
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 24.072
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.300
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 22.288
Recognition finished, iteration 100 Loss 0.006
Perplexity dev: 3.454

==== Starting epoch 54 ====
  Batch 0 Loss 4.6999
  Batch 100 Loss 4.1733
  Batch 200 Loss 3.5247
  Batch 300 Loss 4.0388
  Batch 400 Loss 4.7563
  Batch 500 Loss 4.5303
  Batch 600 Loss 4.5580
  Batch 700 Loss 4.8571
Resetting 9989 PBs
Finished epoch 54 in 117.0 seconds
Perplexity training: 3.415

==== Starting epoch 55 ====
  Batch 0 Loss 3.6330
  Batch 100 Loss 5.3253
  Batch 200 Loss 5.5540
  Batch 300 Loss 5.0541
  Batch 400 Loss 4.4629
  Batch 500 Loss 5.0242
  Batch 600 Loss 4.6589
  Batch 700 Loss 4.7020
Resetting 10000 PBs
Finished epoch 55 in 117.0 seconds
Perplexity training: 3.393
Measuring development set...
Recognition iteration 0 Loss 21.931
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.943
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.158
Recognition finished, iteration 100 Loss 0.004
Recognition iteration 0 Loss 22.039
Recognition finished, iteration 100 Loss 0.006
Perplexity dev: 3.323

==== Starting epoch 56 ====
  Batch 0 Loss 5.5000
  Batch 100 Loss 3.6479
  Batch 200 Loss 5.6358
  Batch 300 Loss 4.4170
  Batch 400 Loss 3.7116
  Batch 500 Loss 4.7183
  Batch 600 Loss 4.3819
  Batch 700 Loss 5.9705
Resetting 9885 PBs
Finished epoch 56 in 117.0 seconds
Perplexity training: 3.432

==== Starting epoch 57 ====
  Batch 0 Loss 4.8773
  Batch 100 Loss 3.6470
  Batch 200 Loss 4.8496
  Batch 300 Loss 4.0979
  Batch 400 Loss 5.1811
  Batch 500 Loss 5.5048
  Batch 600 Loss 3.9956
  Batch 700 Loss 4.8042
Resetting 9979 PBs
Finished epoch 57 in 117.0 seconds
Perplexity training: 3.341
Measuring development set...
Recognition iteration 0 Loss 21.666
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 23.718
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.452
Recognition finished, iteration 100 Loss 0.004
Recognition iteration 0 Loss 22.109
Recognition finished, iteration 100 Loss 0.005
Perplexity dev: 3.271

==== Starting epoch 58 ====
  Batch 0 Loss 5.2432
  Batch 100 Loss 2.6021
  Batch 200 Loss 6.6540
  Batch 300 Loss 4.7854
  Batch 400 Loss 5.3781
  Batch 500 Loss 4.8250
  Batch 600 Loss 5.6601
  Batch 700 Loss 3.4439
Resetting 9966 PBs
Finished epoch 58 in 120.0 seconds
Perplexity training: 3.335

==== Starting epoch 59 ====
  Batch 0 Loss 5.0469
  Batch 100 Loss 3.7087
  Batch 200 Loss 7.4825
  Batch 300 Loss 3.9892
  Batch 400 Loss 5.8486
  Batch 500 Loss 4.6708
  Batch 600 Loss 5.6776
  Batch 700 Loss 5.7244
Resetting 9949 PBs
Finished epoch 59 in 119.0 seconds
Perplexity training: 3.324
Measuring development set...
Recognition iteration 0 Loss 21.711
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.604
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.619
Recognition finished, iteration 99 Loss 0.004
Recognition iteration 0 Loss 22.522
Recognition finished, iteration 100 Loss 0.004
Perplexity dev: 3.051

==== Starting epoch 60 ====
  Batch 0 Loss 6.3155
  Batch 100 Loss 2.8536
  Batch 200 Loss 5.3441
  Batch 300 Loss 5.8312
  Batch 400 Loss 3.5719
  Batch 500 Loss 4.8716
  Batch 600 Loss 4.8109
  Batch 700 Loss 4.5836
Resetting 9893 PBs
Finished epoch 60 in 119.0 seconds
Perplexity training: 3.282

==== Starting epoch 61 ====
  Batch 0 Loss 5.1023
  Batch 100 Loss 4.0308
  Batch 200 Loss 5.9610
  Batch 300 Loss 5.0822
  Batch 400 Loss 3.5559
  Batch 500 Loss 4.6957
  Batch 600 Loss 2.9688
  Batch 700 Loss 3.6215
Resetting 9964 PBs
Finished epoch 61 in 119.0 seconds
Perplexity training: 3.260
Measuring development set...
Recognition iteration 0 Loss 21.815
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 23.616
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 22.192
Recognition finished, iteration 95 Loss 0.004
Recognition iteration 0 Loss 21.808
Recognition finished, iteration 100 Loss 0.004
Perplexity dev: 3.787
Finished training in 6647.37 seconds
Finished training after development set stopped improving.
