Starting training procedure.
Loading training set...
2019-06-30 13:56:06.768766: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-30 13:56:08.261560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-30 13:56:08.262361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-30 13:56:08.262594: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-30 13:56:08.264102: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-30 13:56:08.265328: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-30 13:56:08.265587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-30 13:56:08.266825: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-30 13:56:08.268038: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-30 13:56:08.270879: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-30 13:56:08.274032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-06-30 13:56:08.275090: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-06-30 13:56:08.861186: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d72a20 executing computations on platform CUDA. Devices:
2019-06-30 13:56:08.861227: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-30 13:56:08.861233: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-30 13:56:08.885078: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2019-06-30 13:56:08.887971: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d878e0 executing computations on platform Host. Devices:
2019-06-30 13:56:08.887990: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-06-30 13:56:08.891673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-30 13:56:08.892438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-30 13:56:08.892472: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-30 13:56:08.892481: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-30 13:56:08.892488: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-30 13:56:08.892495: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-30 13:56:08.892502: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-30 13:56:08.892509: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-30 13:56:08.892526: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-30 13:56:08.895609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-06-30 13:56:08.895641: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-30 13:56:08.897747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-30 13:56:08.897760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 
2019-06-30 13:56:08.897765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y 
2019-06-30 13:56:08.897769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N 
2019-06-30 13:56:08.900905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30458 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
2019-06-30 13:56:08.901946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30458 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
Saving vocab defined by training set...
Loading development set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


=== Model ===
Name: rnnpbnmt
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0.1
Num PBs: 1024
Bind hard: True
Binding strength: 1.0
Autoencode: False
PB learning rate: 0.01
Sigma: 0
p_reset: 0.1
Max recog epochs: 100


=== Training ===
Max epochs: 0
Early stopping steps: 20
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-06-30 13:56:13.783270: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-30 13:56:14.935678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0630 13:56:15.225841 140099671775040 deprecation.py:323] From /lhome/daniesso/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 58.8847
  Batch 100 Loss 36.1682
  Batch 200 Loss 30.7797
  Batch 300 Loss 31.5305
  Batch 400 Loss 29.1241
  Batch 500 Loss 29.0856
  Batch 600 Loss 27.9380
  Batch 700 Loss 25.3672
Resetting 5011 PBs
Finished epoch 1 in 59.0 seconds
Perplexity training: 80.508
Measuring development set...
Recognition iteration 0 Loss 28.472
Recognition finished, iteration 100 Loss 25.036
Recognition iteration 0 Loss 28.595
Recognition finished, iteration 100 Loss 25.107
Recognition iteration 0 Loss 27.984
Recognition finished, iteration 100 Loss 24.681
Recognition iteration 0 Loss 28.719
Recognition finished, iteration 100 Loss 25.055
Perplexity dev: 36.449

==== Starting epoch 2 ====
  Batch 0 Loss 26.6958
  Batch 100 Loss 26.2270
  Batch 200 Loss 23.9543
  Batch 300 Loss 25.5656
  Batch 400 Loss 24.4324
  Batch 500 Loss 24.3706
  Batch 600 Loss 23.6679
  Batch 700 Loss 21.2197
Resetting 5013 PBs
Finished epoch 2 in 57.0 seconds
Perplexity training: 25.938

==== Starting epoch 3 ====
  Batch 0 Loss 23.3958
  Batch 100 Loss 22.6514
  Batch 200 Loss 20.5369
  Batch 300 Loss 22.0049
  Batch 400 Loss 21.5468
  Batch 500 Loss 20.9053
  Batch 600 Loss 20.1335
  Batch 700 Loss 18.8193
Resetting 5027 PBs
Finished epoch 3 in 57.0 seconds
Perplexity training: 17.584
Measuring development set...
Recognition iteration 0 Loss 26.403
Recognition finished, iteration 100 Loss 14.309
Recognition iteration 0 Loss 26.580
Recognition finished, iteration 100 Loss 14.158
Recognition iteration 0 Loss 25.872
Recognition finished, iteration 100 Loss 13.691
Recognition iteration 0 Loss 26.771
Recognition finished, iteration 100 Loss 14.295
Perplexity dev: 10.876

==== Starting epoch 4 ====
  Batch 0 Loss 19.8920
  Batch 100 Loss 19.4437
  Batch 200 Loss 17.7463
  Batch 300 Loss 19.4432
  Batch 400 Loss 18.2583
  Batch 500 Loss 18.0964
  Batch 600 Loss 18.0106
  Batch 700 Loss 16.0839
Resetting 4963 PBs
Finished epoch 4 in 58.0 seconds
Perplexity training: 12.949

==== Starting epoch 5 ====
  Batch 0 Loss 16.9969
  Batch 100 Loss 16.4235
  Batch 200 Loss 15.4038
  Batch 300 Loss 17.4209
  Batch 400 Loss 16.0323
  Batch 500 Loss 16.2529
  Batch 600 Loss 15.7976
  Batch 700 Loss 14.5277
Resetting 4968 PBs
Finished epoch 5 in 58.0 seconds
Perplexity training: 10.137
Measuring development set...
Recognition iteration 0 Loss 26.287
Recognition finished, iteration 100 Loss 8.726
Recognition iteration 0 Loss 26.080
Recognition finished, iteration 100 Loss 8.651
Recognition iteration 0 Loss 25.728
Recognition finished, iteration 100 Loss 8.108
Recognition iteration 0 Loss 25.845
Recognition finished, iteration 100 Loss 8.490
Perplexity dev: 6.774

==== Starting epoch 6 ====
  Batch 0 Loss 15.0787
  Batch 100 Loss 14.4056
  Batch 200 Loss 13.2674
  Batch 300 Loss 15.4183
  Batch 400 Loss 13.9514
  Batch 500 Loss 14.1288
  Batch 600 Loss 12.6777
  Batch 700 Loss 13.4118
Resetting 4924 PBs
Finished epoch 6 in 58.0 seconds
Perplexity training: 8.173

==== Starting epoch 7 ====
  Batch 0 Loss 12.2128
  Batch 100 Loss 12.7613
  Batch 200 Loss 11.6485
  Batch 300 Loss 12.8969
  Batch 400 Loss 11.8232
  Batch 500 Loss 12.4413
  Batch 600 Loss 12.4617
  Batch 700 Loss 10.6523
Resetting 4954 PBs
Finished epoch 7 in 59.0 seconds
Perplexity training: 6.901
Measuring development set...
Recognition iteration 0 Loss 26.097
Recognition finished, iteration 100 Loss 5.127
Recognition iteration 0 Loss 25.465
Recognition finished, iteration 100 Loss 4.941
Recognition iteration 0 Loss 24.864
Recognition finished, iteration 100 Loss 4.491
Recognition iteration 0 Loss 25.427
Recognition finished, iteration 100 Loss 4.920
Perplexity dev: 5.146

==== Starting epoch 8 ====
  Batch 0 Loss 11.7909
  Batch 100 Loss 11.1845
  Batch 200 Loss 10.4429
  Batch 300 Loss 11.3754
  Batch 400 Loss 10.3568
  Batch 500 Loss 11.1557
  Batch 600 Loss 10.7956
  Batch 700 Loss 9.7806
Resetting 4935 PBs
Finished epoch 8 in 59.0 seconds
Perplexity training: 6.118

==== Starting epoch 9 ====
  Batch 0 Loss 9.1683
  Batch 100 Loss 9.8488
  Batch 200 Loss 8.6145
  Batch 300 Loss 9.7987
  Batch 400 Loss 8.9108
  Batch 500 Loss 10.1734
  Batch 600 Loss 9.8920
  Batch 700 Loss 7.7709
Resetting 5107 PBs
Finished epoch 9 in 60.0 seconds
Perplexity training: 5.449
Measuring development set...
Recognition iteration 0 Loss 25.981
Recognition finished, iteration 100 Loss 3.041
Recognition iteration 0 Loss 25.661
Recognition finished, iteration 100 Loss 2.753
Recognition iteration 0 Loss 24.243
Recognition finished, iteration 100 Loss 2.507
Recognition iteration 0 Loss 25.456
Recognition finished, iteration 100 Loss 2.887
Perplexity dev: 4.728

==== Starting epoch 10 ====
  Batch 0 Loss 9.6563
  Batch 100 Loss 9.0081
  Batch 200 Loss 8.3808
  Batch 300 Loss 8.4627
  Batch 400 Loss 9.3340
  Batch 500 Loss 9.5553
  Batch 600 Loss 9.5333
  Batch 700 Loss 7.8446
Resetting 4983 PBs
Finished epoch 10 in 61.0 seconds
Perplexity training: 5.193

==== Starting epoch 11 ====
  Batch 0 Loss 8.1884
  Batch 100 Loss 7.1683
  Batch 200 Loss 7.4914
  Batch 300 Loss 7.8496
  Batch 400 Loss 9.1652
  Batch 500 Loss 7.9400
  Batch 600 Loss 8.4186
  Batch 700 Loss 7.3849
Resetting 4891 PBs
Finished epoch 11 in 60.0 seconds
Perplexity training: 4.795
Measuring development set...
Recognition iteration 0 Loss 25.390
Recognition finished, iteration 100 Loss 1.770
Recognition iteration 0 Loss 25.718
Recognition finished, iteration 100 Loss 1.519
Recognition iteration 0 Loss 24.407
Recognition finished, iteration 100 Loss 1.434
Recognition iteration 0 Loss 25.243
Recognition finished, iteration 100 Loss 1.687
Perplexity dev: 4.072

==== Starting epoch 12 ====
  Batch 0 Loss 7.2481
  Batch 100 Loss 7.8016
  Batch 200 Loss 6.1006
  Batch 300 Loss 6.5722
  Batch 400 Loss 7.8346
  Batch 500 Loss 7.6153
  Batch 600 Loss 6.5063
  Batch 700 Loss 6.9538
Resetting 5095 PBs
Finished epoch 12 in 61.0 seconds
Perplexity training: 4.376

==== Starting epoch 13 ====
  Batch 0 Loss 7.1386
  Batch 100 Loss 7.6889
  Batch 200 Loss 6.3899
  Batch 300 Loss 5.7647
  Batch 400 Loss 7.8149
  Batch 500 Loss 6.7017
  Batch 600 Loss 6.5215
  Batch 700 Loss 6.7639
Resetting 4972 PBs
Finished epoch 13 in 62.0 seconds
Perplexity training: 4.267
Measuring development set...
Recognition iteration 0 Loss 25.208
Recognition finished, iteration 100 Loss 1.033
Recognition iteration 0 Loss 25.674
Recognition finished, iteration 100 Loss 0.833
Recognition iteration 0 Loss 24.289
Recognition finished, iteration 100 Loss 0.834
Recognition iteration 0 Loss 24.774
Recognition finished, iteration 100 Loss 0.974
Perplexity dev: 4.061

==== Starting epoch 14 ====
  Batch 0 Loss 6.6695
  Batch 100 Loss 6.0575
  Batch 200 Loss 5.6741
  Batch 300 Loss 5.3357
  Batch 400 Loss 8.8203
  Batch 500 Loss 8.6394
  Batch 600 Loss 5.6598
  Batch 700 Loss 5.7903
Resetting 4957 PBs
Finished epoch 14 in 61.0 seconds
Perplexity training: 4.056

==== Starting epoch 15 ====
  Batch 0 Loss 6.2822
  Batch 100 Loss 6.8981
  Batch 200 Loss 5.0385
  Batch 300 Loss 6.0828
  Batch 400 Loss 8.4241
  Batch 500 Loss 9.0464
  Batch 600 Loss 5.6077
  Batch 700 Loss 6.2387
Resetting 5066 PBs
Finished epoch 15 in 61.0 seconds
Perplexity training: 3.901
Measuring development set...
Recognition iteration 0 Loss 25.282
Recognition finished, iteration 100 Loss 0.587
Recognition iteration 0 Loss 24.942
Recognition finished, iteration 100 Loss 0.540
Recognition iteration 0 Loss 23.608
Recognition finished, iteration 100 Loss 0.496
Recognition iteration 0 Loss 24.760
Recognition finished, iteration 100 Loss 0.598
Perplexity dev: 4.108

==== Starting epoch 16 ====
  Batch 0 Loss 5.0636
  Batch 100 Loss 7.5110
  Batch 200 Loss 5.6334
  Batch 300 Loss 6.2898
  Batch 400 Loss 5.5995
  Batch 500 Loss 6.6167
  Batch 600 Loss 6.1818
  Batch 700 Loss 4.3706
Resetting 5019 PBs
Finished epoch 16 in 61.0 seconds
Perplexity training: 3.841

==== Starting epoch 17 ====
  Batch 0 Loss 4.1760
  Batch 100 Loss 5.8458
  Batch 200 Loss 4.4450
  Batch 300 Loss 5.4548
  Batch 400 Loss 6.3270
  Batch 500 Loss 4.5553
  Batch 600 Loss 5.6534
  Batch 700 Loss 3.9639
Resetting 4915 PBs
Finished epoch 17 in 62.0 seconds
Perplexity training: 3.708
Measuring development set...
Recognition iteration 0 Loss 24.732
Recognition finished, iteration 100 Loss 0.351
Recognition iteration 0 Loss 24.735
Recognition finished, iteration 100 Loss 0.293
Recognition iteration 0 Loss 23.666
Recognition finished, iteration 100 Loss 0.260
Recognition iteration 0 Loss 24.396
Recognition finished, iteration 100 Loss 0.359
Perplexity dev: 3.491

==== Starting epoch 18 ====
  Batch 0 Loss 4.3084
  Batch 100 Loss 4.4310
  Batch 200 Loss 4.9538
  Batch 300 Loss 5.2511
  Batch 400 Loss 5.3531
  Batch 500 Loss 4.6500
  Batch 600 Loss 5.8641
  Batch 700 Loss 3.1749
Resetting 5029 PBs
Finished epoch 18 in 63.0 seconds
Perplexity training: 3.527

==== Starting epoch 19 ====
  Batch 0 Loss 2.8207
  Batch 100 Loss 4.8993
  Batch 200 Loss 4.3358
  Batch 300 Loss 6.6838
  Batch 400 Loss 5.0280
  Batch 500 Loss 6.2558
  Batch 600 Loss 5.2723
  Batch 700 Loss 4.1261
Resetting 4949 PBs
Finished epoch 19 in 62.0 seconds
Perplexity training: 3.522
Measuring development set...
Recognition iteration 0 Loss 24.665
Recognition finished, iteration 100 Loss 0.192
Recognition iteration 0 Loss 24.735
Recognition finished, iteration 100 Loss 0.202
Recognition iteration 0 Loss 23.935
Recognition finished, iteration 100 Loss 0.164
Recognition iteration 0 Loss 24.612
Recognition finished, iteration 100 Loss 0.209
Perplexity dev: 3.621

==== Starting epoch 20 ====
  Batch 0 Loss 4.0334
  Batch 100 Loss 5.6988
  Batch 200 Loss 3.8911
  Batch 300 Loss 5.4747
  Batch 400 Loss 4.4948
  Batch 500 Loss 5.7477
  Batch 600 Loss 4.1978
  Batch 700 Loss 5.1659
Resetting 5095 PBs
Finished epoch 20 in 63.0 seconds
Perplexity training: 3.408

==== Starting epoch 21 ====
  Batch 0 Loss 4.7991
  Batch 100 Loss 3.9034
  Batch 200 Loss 3.3406
  Batch 300 Loss 5.3069
  Batch 400 Loss 4.6877
  Batch 500 Loss 5.5163
  Batch 600 Loss 4.5325
  Batch 700 Loss 4.4033
Resetting 4853 PBs
Finished epoch 21 in 64.0 seconds
Perplexity training: 3.450
Measuring development set...
Recognition iteration 0 Loss 24.712
Recognition finished, iteration 100 Loss 0.152
Recognition iteration 0 Loss 24.450
Recognition finished, iteration 100 Loss 0.148
Recognition iteration 0 Loss 23.487
Recognition finished, iteration 100 Loss 0.121
Recognition iteration 0 Loss 24.629
Recognition finished, iteration 100 Loss 0.212
Perplexity dev: 3.725

==== Starting epoch 22 ====
  Batch 0 Loss 2.3797
  Batch 100 Loss 4.7324
  Batch 200 Loss 2.7299
  Batch 300 Loss 4.3236
  Batch 400 Loss 6.4945
  Batch 500 Loss 4.6604
  Batch 600 Loss 4.1832
  Batch 700 Loss 3.6115
Resetting 5016 PBs
Finished epoch 22 in 64.0 seconds
Perplexity training: 3.349

==== Starting epoch 23 ====
  Batch 0 Loss 4.1603
  Batch 100 Loss 4.0395
  Batch 200 Loss 4.1372
  Batch 300 Loss 3.8263
  Batch 400 Loss 4.7322
  Batch 500 Loss 4.4445
  Batch 600 Loss 3.0157
  Batch 700 Loss 4.4760
Resetting 5024 PBs
Finished epoch 23 in 65.0 seconds
Perplexity training: 3.270
Measuring development set...
Recognition iteration 0 Loss 24.657
Recognition finished, iteration 100 Loss 0.090
Recognition iteration 0 Loss 24.353
Recognition finished, iteration 100 Loss 0.100
Recognition iteration 0 Loss 23.606
Recognition finished, iteration 100 Loss 0.088
Recognition iteration 0 Loss 24.469
Recognition finished, iteration 100 Loss 0.107
Perplexity dev: 3.876

==== Starting epoch 24 ====
  Batch 0 Loss 3.0541
  Batch 100 Loss 3.1337
  Batch 200 Loss 3.5049
  Batch 300 Loss 4.3667
  Batch 400 Loss 4.2783
  Batch 500 Loss 3.8335
  Batch 600 Loss 3.4175
  Batch 700 Loss 4.2061
Resetting 4911 PBs
Finished epoch 24 in 64.0 seconds
Perplexity training: 3.229

==== Starting epoch 25 ====
  Batch 0 Loss 3.6327
  Batch 100 Loss 3.3178
  Batch 200 Loss 3.6427
  Batch 300 Loss 3.2022
  Batch 400 Loss 3.5598
  Batch 500 Loss 3.1192
  Batch 600 Loss 5.4517
  Batch 700 Loss 2.0063
Resetting 5031 PBs
Finished epoch 25 in 67.0 seconds
Perplexity training: 3.178
Measuring development set...
Recognition iteration 0 Loss 24.722
Recognition finished, iteration 100 Loss 0.080
Recognition iteration 0 Loss 24.560
Recognition finished, iteration 100 Loss 0.070
Recognition iteration 0 Loss 23.294
Recognition finished, iteration 100 Loss 0.058
Recognition iteration 0 Loss 24.374
Recognition finished, iteration 100 Loss 0.089
Perplexity dev: 3.263

==== Starting epoch 26 ====
  Batch 0 Loss 4.8076
  Batch 100 Loss 3.8822
  Batch 200 Loss 4.0261
  Batch 300 Loss 2.9131
  Batch 400 Loss 4.2535
  Batch 500 Loss 2.8299
  Batch 600 Loss 4.4775
  Batch 700 Loss 2.3478
Resetting 4869 PBs
Finished epoch 26 in 66.0 seconds
Perplexity training: 3.157

==== Starting epoch 27 ====
  Batch 0 Loss 3.0523
  Batch 100 Loss 2.8568
  Batch 200 Loss 3.0689
  Batch 300 Loss 4.2363
  Batch 400 Loss 4.6943
  Batch 500 Loss 3.5651
  Batch 600 Loss 4.1941
  Batch 700 Loss 3.2218
Resetting 5006 PBs
Finished epoch 27 in 66.0 seconds
Perplexity training: 3.103
Measuring development set...
Recognition iteration 0 Loss 24.289
Recognition finished, iteration 100 Loss 0.060
Recognition iteration 0 Loss 23.795
Recognition finished, iteration 100 Loss 0.053
Recognition iteration 0 Loss 23.036
Recognition finished, iteration 100 Loss 0.045
Recognition iteration 0 Loss 24.083
Recognition finished, iteration 100 Loss 0.065
Perplexity dev: 2.755

==== Starting epoch 28 ====
  Batch 0 Loss 2.6862
  Batch 100 Loss 2.9623
  Batch 200 Loss 2.9263
  Batch 300 Loss 2.8434
  Batch 400 Loss 3.1122
  Batch 500 Loss 3.4874
  Batch 600 Loss 3.8942
  Batch 700 Loss 3.3302
Resetting 5038 PBs
Finished epoch 28 in 67.0 seconds
Perplexity training: 3.037

==== Starting epoch 29 ====
  Batch 0 Loss 4.2795
  Batch 100 Loss 4.9958
  Batch 200 Loss 2.2221
  Batch 300 Loss 3.4381
  Batch 400 Loss 5.3705
  Batch 500 Loss 3.3983
  Batch 600 Loss 3.6788
  Batch 700 Loss 2.7714
Resetting 4989 PBs
Finished epoch 29 in 67.0 seconds
Perplexity training: 3.035
Measuring development set...
Recognition iteration 0 Loss 24.009
Recognition finished, iteration 100 Loss 0.043
Recognition iteration 0 Loss 23.874
Recognition finished, iteration 100 Loss 0.044
Recognition iteration 0 Loss 23.057
Recognition finished, iteration 100 Loss 0.038
Recognition iteration 0 Loss 24.135
Recognition finished, iteration 100 Loss 0.043
Perplexity dev: 3.300

==== Starting epoch 30 ====
  Batch 0 Loss 2.9077
  Batch 100 Loss 4.4527
  Batch 200 Loss 3.0840
  Batch 300 Loss 2.8124
  Batch 400 Loss 2.2437
  Batch 500 Loss 3.0916
  Batch 600 Loss 4.7126
  Batch 700 Loss 2.9018
Resetting 5031 PBs
Finished epoch 30 in 67.0 seconds
Perplexity training: 3.023

==== Starting epoch 31 ====
  Batch 0 Loss 4.4402
  Batch 100 Loss 4.3973
  Batch 200 Loss 3.0582
  Batch 300 Loss 4.4038
  Batch 400 Loss 2.0559
  Batch 500 Loss 2.2893
  Batch 600 Loss 4.1902
  Batch 700 Loss 2.3683
Resetting 5022 PBs
Finished epoch 31 in 67.0 seconds
Perplexity training: 3.028
Measuring development set...
Recognition iteration 0 Loss 24.358
Recognition finished, iteration 100 Loss 0.037
Recognition iteration 0 Loss 24.453
Recognition finished, iteration 100 Loss 0.040
Recognition iteration 0 Loss 22.608
Recognition finished, iteration 100 Loss 0.032
Recognition iteration 0 Loss 24.176
Recognition finished, iteration 100 Loss 0.056
Perplexity dev: 2.906

==== Starting epoch 32 ====
  Batch 0 Loss 4.1881
  Batch 100 Loss 2.9359
  Batch 200 Loss 2.7404
  Batch 300 Loss 4.9350
  Batch 400 Loss 0.9822
  Batch 500 Loss 2.7377
  Batch 600 Loss 2.9545
  Batch 700 Loss 2.0406
Resetting 5041 PBs
Finished epoch 32 in 68.0 seconds
Perplexity training: 2.996

==== Starting epoch 33 ====
  Batch 0 Loss 3.4051
  Batch 100 Loss 2.2367
  Batch 200 Loss 3.0862
  Batch 300 Loss 3.6596
  Batch 400 Loss 1.5455
  Batch 500 Loss 2.1436
  Batch 600 Loss 3.6830
  Batch 700 Loss 3.3671
Resetting 5063 PBs
Finished epoch 33 in 69.0 seconds
Perplexity training: 2.914
Measuring development set...
Recognition iteration 0 Loss 23.928
Recognition finished, iteration 100 Loss 0.029
Recognition iteration 0 Loss 23.955
Recognition finished, iteration 100 Loss 0.030
Recognition iteration 0 Loss 22.786
Recognition finished, iteration 100 Loss 0.024
Recognition iteration 0 Loss 24.116
Recognition finished, iteration 100 Loss 0.032
Perplexity dev: 2.748

==== Starting epoch 34 ====
  Batch 0 Loss 2.3970
  Batch 100 Loss 2.8715
  Batch 200 Loss 2.9992
  Batch 300 Loss 3.3563
  Batch 400 Loss 2.8937
  Batch 500 Loss 2.2180
  Batch 600 Loss 4.0667
  Batch 700 Loss 2.9859
Resetting 4931 PBs
Finished epoch 34 in 69.0 seconds
Perplexity training: 2.976

==== Starting epoch 35 ====
  Batch 0 Loss 2.9786
  Batch 100 Loss 1.8009
  Batch 200 Loss 2.6618
  Batch 300 Loss 2.3727
  Batch 400 Loss 2.5295
  Batch 500 Loss 3.5400
  Batch 600 Loss 3.3400
  Batch 700 Loss 3.2076
Resetting 5090 PBs
Finished epoch 35 in 69.0 seconds
Perplexity training: 2.901
Measuring development set...
Recognition iteration 0 Loss 24.003
Recognition finished, iteration 100 Loss 0.023
Recognition iteration 0 Loss 23.873
Recognition finished, iteration 100 Loss 0.025
Recognition iteration 0 Loss 22.698
Recognition finished, iteration 100 Loss 0.019
Recognition iteration 0 Loss 24.035
Recognition finished, iteration 100 Loss 0.027
Perplexity dev: 2.823

==== Starting epoch 36 ====
  Batch 0 Loss 3.8532
  Batch 100 Loss 1.5487
  Batch 200 Loss 2.4036
  Batch 300 Loss 2.7388
  Batch 400 Loss 3.7506
  Batch 500 Loss 2.7907
  Batch 600 Loss 4.6021
  Batch 700 Loss 3.0201
Resetting 5009 PBs
Finished epoch 36 in 69.0 seconds
Perplexity training: 2.881

==== Starting epoch 37 ====
  Batch 0 Loss 1.7435
  Batch 100 Loss 1.4006
  Batch 200 Loss 2.5422
  Batch 300 Loss 4.9845
  Batch 400 Loss 2.7348
  Batch 500 Loss 3.0610
  Batch 600 Loss 5.3043
  Batch 700 Loss 3.1203
Resetting 4999 PBs
Finished epoch 37 in 70.0 seconds
Perplexity training: 2.840
Measuring development set...
Recognition iteration 0 Loss 23.732
Recognition finished, iteration 100 Loss 0.019
Recognition iteration 0 Loss 23.612
Recognition finished, iteration 100 Loss 0.021
Recognition iteration 0 Loss 22.356
Recognition finished, iteration 100 Loss 0.018
Recognition iteration 0 Loss 23.489
Recognition finished, iteration 100 Loss 0.021
Perplexity dev: 3.494

==== Starting epoch 38 ====
  Batch 0 Loss 3.4914
  Batch 100 Loss 3.8757
  Batch 200 Loss 3.1022
  Batch 300 Loss 3.6581
  Batch 400 Loss 2.7753
  Batch 500 Loss 3.3845
  Batch 600 Loss 3.0808
  Batch 700 Loss 2.6612
Resetting 5020 PBs
Finished epoch 38 in 71.0 seconds
Perplexity training: 2.804

==== Starting epoch 39 ====
  Batch 0 Loss 3.1477
  Batch 100 Loss 3.2302
  Batch 200 Loss 1.5336
  Batch 300 Loss 3.3593
  Batch 400 Loss 2.9738
  Batch 500 Loss 2.9900
  Batch 600 Loss 3.7463
  Batch 700 Loss 3.0510
Resetting 5034 PBs
Finished epoch 39 in 71.0 seconds
Perplexity training: 2.803
Measuring development set...
Recognition iteration 0 Loss 24.184
Recognition finished, iteration 100 Loss 0.023
Recognition iteration 0 Loss 23.838
Recognition finished, iteration 100 Loss 0.018
Recognition iteration 0 Loss 22.746
Recognition finished, iteration 100 Loss 0.016
Recognition iteration 0 Loss 23.339
Recognition finished, iteration 100 Loss 0.016
Perplexity dev: 2.649

==== Starting epoch 40 ====
  Batch 0 Loss 4.4581
  Batch 100 Loss 3.4848
  Batch 200 Loss 4.5421
  Batch 300 Loss 3.1593
  Batch 400 Loss 2.6616
  Batch 500 Loss 2.1188
  Batch 600 Loss 2.8702
  Batch 700 Loss 1.9261
Resetting 4992 PBs
Finished epoch 40 in 72.0 seconds
Perplexity training: 2.791

==== Starting epoch 41 ====
  Batch 0 Loss 3.0549
  Batch 100 Loss 2.4655
  Batch 200 Loss 2.4607
  Batch 300 Loss 2.2198
  Batch 400 Loss 1.6912
  Batch 500 Loss 3.3423
  Batch 600 Loss 3.0095
  Batch 700 Loss 3.3930
Resetting 5032 PBs
Finished epoch 41 in 71.0 seconds
Perplexity training: 2.756
Measuring development set...
Recognition iteration 0 Loss 23.555
Recognition finished, iteration 100 Loss 0.020
Recognition iteration 0 Loss 24.026
Recognition finished, iteration 100 Loss 0.015
Recognition iteration 0 Loss 22.386
Recognition finished, iteration 100 Loss 0.013
Recognition iteration 0 Loss 23.971
Recognition finished, iteration 100 Loss 0.015
Perplexity dev: 2.721

==== Starting epoch 42 ====
  Batch 0 Loss 2.9408
  Batch 100 Loss 1.8987
  Batch 200 Loss 2.1342
  Batch 300 Loss 3.1228
  Batch 400 Loss 3.5604
  Batch 500 Loss 2.7790
  Batch 600 Loss 3.0274
  Batch 700 Loss 3.1925
Resetting 5043 PBs
Finished epoch 42 in 72.0 seconds
Perplexity training: 2.711

==== Starting epoch 43 ====
  Batch 0 Loss 2.8192
  Batch 100 Loss 2.5469
  Batch 200 Loss 2.1291
  Batch 300 Loss 2.6420
  Batch 400 Loss 4.3285
  Batch 500 Loss 3.0670
  Batch 600 Loss 3.2487
  Batch 700 Loss 3.0111
Resetting 4981 PBs
Finished epoch 43 in 71.0 seconds
Perplexity training: 2.761
Measuring development set...
Recognition iteration 0 Loss 23.684
Recognition finished, iteration 100 Loss 0.014
Recognition iteration 0 Loss 23.772
Recognition finished, iteration 100 Loss 0.015
Recognition iteration 0 Loss 22.240
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 23.265
Recognition finished, iteration 100 Loss 0.013
Perplexity dev: 2.421

==== Starting epoch 44 ====
  Batch 0 Loss 4.1099
  Batch 100 Loss 1.5983
  Batch 200 Loss 2.1448
  Batch 300 Loss 1.4844
  Batch 400 Loss 4.9160
  Batch 500 Loss 3.4709
  Batch 600 Loss 2.5325
  Batch 700 Loss 2.7849
Resetting 5048 PBs
Finished epoch 44 in 71.0 seconds
Perplexity training: 2.694

==== Starting epoch 45 ====
  Batch 0 Loss 1.7189
  Batch 100 Loss 2.1010
  Batch 200 Loss 1.7734
  Batch 300 Loss 2.9000
  Batch 400 Loss 3.9925
  Batch 500 Loss 3.2827
  Batch 600 Loss 2.0809
  Batch 700 Loss 4.3705
Resetting 4913 PBs
Finished epoch 45 in 72.0 seconds
Perplexity training: 2.703
Measuring development set...
Recognition iteration 0 Loss 23.562
Recognition finished, iteration 100 Loss 0.013
Recognition iteration 0 Loss 23.378
Recognition finished, iteration 100 Loss 0.014
Recognition iteration 0 Loss 22.451
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 23.334
Recognition finished, iteration 100 Loss 0.012
Perplexity dev: 3.009

==== Starting epoch 46 ====
  Batch 0 Loss 2.4628
  Batch 100 Loss 3.3115
  Batch 200 Loss 1.6933
  Batch 300 Loss 3.6177
  Batch 400 Loss 3.8420
  Batch 500 Loss 3.1320
  Batch 600 Loss 2.8259
  Batch 700 Loss 2.4252
Resetting 5054 PBs
Finished epoch 46 in 73.0 seconds
Perplexity training: 2.648

==== Starting epoch 47 ====
  Batch 0 Loss 2.2993
  Batch 100 Loss 2.3237
  Batch 200 Loss 2.1475
  Batch 300 Loss 3.0047
  Batch 400 Loss 3.1074
  Batch 500 Loss 3.8489
  Batch 600 Loss 2.2904
  Batch 700 Loss 3.8210
Resetting 4892 PBs
Finished epoch 47 in 73.0 seconds
Perplexity training: 2.652
Measuring development set...
Recognition iteration 0 Loss 23.653
Recognition finished, iteration 100 Loss 0.011
Recognition iteration 0 Loss 23.845
Recognition finished, iteration 100 Loss 0.013
Recognition iteration 0 Loss 22.509
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 23.082
Recognition finished, iteration 100 Loss 0.012
Perplexity dev: 2.508

==== Starting epoch 48 ====
  Batch 0 Loss 1.6218
  Batch 100 Loss 1.8460
  Batch 200 Loss 0.8711
  Batch 300 Loss 3.0143
  Batch 400 Loss 2.5826
  Batch 500 Loss 4.2471
  Batch 600 Loss 3.4320
  Batch 700 Loss 2.9832
Resetting 5130 PBs
Finished epoch 48 in 72.0 seconds
Perplexity training: 2.651

==== Starting epoch 49 ====
  Batch 0 Loss 3.9094
  Batch 100 Loss 2.4553
  Batch 200 Loss 1.2906
  Batch 300 Loss 3.7917
  Batch 400 Loss 1.9324
  Batch 500 Loss 3.1766
  Batch 600 Loss 2.8470
  Batch 700 Loss 2.0615
Resetting 5068 PBs
Finished epoch 49 in 74.0 seconds
Perplexity training: 2.672
Measuring development set...
Recognition iteration 0 Loss 23.429
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 23.485
Recognition finished, iteration 100 Loss 0.011
Recognition iteration 0 Loss 22.454
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 23.109
Recognition finished, iteration 100 Loss 0.011
Perplexity dev: 3.608

==== Starting epoch 50 ====
  Batch 0 Loss 2.0176
  Batch 100 Loss 2.5282
  Batch 200 Loss 2.6803
  Batch 300 Loss 3.3385
  Batch 400 Loss 4.4217
  Batch 500 Loss 4.0473
  Batch 600 Loss 3.2204
  Batch 700 Loss 2.8489
Resetting 5104 PBs
Finished epoch 50 in 80.0 seconds
Perplexity training: 2.648

==== Starting epoch 51 ====
  Batch 0 Loss 2.9834
  Batch 100 Loss 2.3245
  Batch 200 Loss 2.8451
  Batch 300 Loss 2.8441
  Batch 400 Loss 2.5978
  Batch 500 Loss 2.7755
  Batch 600 Loss 2.5711
  Batch 700 Loss 2.7352
Resetting 5135 PBs
Finished epoch 51 in 72.0 seconds
Perplexity training: 2.575
Measuring development set...
Recognition iteration 0 Loss 23.157
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 23.430
Recognition finished, iteration 100 Loss 0.011
Recognition iteration 0 Loss 22.348
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.061
Recognition finished, iteration 100 Loss 0.010
Perplexity dev: 5.447

==== Starting epoch 52 ====
  Batch 0 Loss 2.1426
  Batch 100 Loss 3.0790
  Batch 200 Loss 2.1480
  Batch 300 Loss 3.4001
  Batch 400 Loss 2.7696
  Batch 500 Loss 1.6434
  Batch 600 Loss 2.9541
  Batch 700 Loss 3.2403
Resetting 4991 PBs
Finished epoch 52 in 73.0 seconds
Perplexity training: 2.618

==== Starting epoch 53 ====
  Batch 0 Loss 2.8195
  Batch 100 Loss 2.4119
  Batch 200 Loss 3.0446
  Batch 300 Loss 3.1587
  Batch 400 Loss 3.6441
  Batch 500 Loss 2.0081
  Batch 600 Loss 2.1851
  Batch 700 Loss 2.1818
Resetting 5011 PBs
Finished epoch 53 in 73.0 seconds
Perplexity training: 2.570
Measuring development set...
Recognition iteration 0 Loss 23.470
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 23.628
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 22.278
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.226
Recognition finished, iteration 100 Loss 0.008
Perplexity dev: 2.834

==== Starting epoch 54 ====
  Batch 0 Loss 3.0284
  Batch 100 Loss 2.1928
  Batch 200 Loss 1.7741
  Batch 300 Loss 2.3268
  Batch 400 Loss 2.8722
  Batch 500 Loss 3.0005
  Batch 600 Loss 3.1269
  Batch 700 Loss 3.4330
Resetting 5038 PBs
Finished epoch 54 in 73.0 seconds
Perplexity training: 2.561

==== Starting epoch 55 ====
  Batch 0 Loss 3.1272
  Batch 100 Loss 3.0593
  Batch 200 Loss 1.4319
  Batch 300 Loss 2.3387
  Batch 400 Loss 2.8913
  Batch 500 Loss 3.2716
  Batch 600 Loss 2.6664
  Batch 700 Loss 2.2899
Resetting 4972 PBs
Finished epoch 55 in 73.0 seconds
Perplexity training: 2.525
Measuring development set...
Recognition iteration 0 Loss 23.184
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 23.508
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 22.142
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.036
Recognition finished, iteration 100 Loss 0.008
Perplexity dev: 3.046

==== Starting epoch 56 ====
  Batch 0 Loss 2.1790
  Batch 100 Loss 1.9688
  Batch 200 Loss 2.7148
  Batch 300 Loss 1.9216
  Batch 400 Loss 4.1074
  Batch 500 Loss 3.3659
  Batch 600 Loss 3.0498
  Batch 700 Loss 1.4309
Resetting 4865 PBs
Finished epoch 56 in 73.0 seconds
Perplexity training: 2.549

==== Starting epoch 57 ====
  Batch 0 Loss 2.0130
  Batch 100 Loss 1.8020
  Batch 200 Loss 3.7157
  Batch 300 Loss 2.8701
  Batch 400 Loss 2.9224
  Batch 500 Loss 3.1690
  Batch 600 Loss 2.6526
  Batch 700 Loss 1.0588
Resetting 5002 PBs
Finished epoch 57 in 74.0 seconds
Perplexity training: 2.477
Measuring development set...
Recognition iteration 0 Loss 23.376
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 23.643
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 22.237
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.918
Recognition finished, iteration 100 Loss 0.007
Perplexity dev: 3.707

==== Starting epoch 58 ====
  Batch 0 Loss 1.7694
  Batch 100 Loss 1.3041
  Batch 200 Loss 2.0110
  Batch 300 Loss 2.8852
  Batch 400 Loss 2.3698
  Batch 500 Loss 3.1351
  Batch 600 Loss 3.5135
  Batch 700 Loss 1.7814
Resetting 5007 PBs
Finished epoch 58 in 75.0 seconds
Perplexity training: 2.524

==== Starting epoch 59 ====
  Batch 0 Loss 1.9427
  Batch 100 Loss 2.7433
  Batch 200 Loss 2.2509
  Batch 300 Loss 3.7846
  Batch 400 Loss 2.4370
  Batch 500 Loss 2.3498
  Batch 600 Loss 2.9169
  Batch 700 Loss 3.0321
Resetting 5005 PBs
Finished epoch 59 in 75.0 seconds
Perplexity training: 2.526
Measuring development set...
Recognition iteration 0 Loss 23.351
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 23.559
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 22.178
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.733
Recognition finished, iteration 100 Loss 0.006
Perplexity dev: 2.536

==== Starting epoch 60 ====
  Batch 0 Loss 0.7177
  Batch 100 Loss 2.6709
  Batch 200 Loss 3.2718
  Batch 300 Loss 3.0197
  Batch 400 Loss 3.4378
  Batch 500 Loss 3.6880
  Batch 600 Loss 1.3872
  Batch 700 Loss 1.6616
Resetting 5042 PBs
Finished epoch 60 in 76.0 seconds
Perplexity training: 2.509

==== Starting epoch 61 ====
  Batch 0 Loss 1.1729
  Batch 100 Loss 2.3832
  Batch 200 Loss 2.3194
  Batch 300 Loss 1.8975
  Batch 400 Loss 3.9633
  Batch 500 Loss 3.2234
  Batch 600 Loss 1.4034
  Batch 700 Loss 2.5105
Resetting 5080 PBs
Finished epoch 61 in 76.0 seconds
Perplexity training: 2.483
Measuring development set...
Recognition iteration 0 Loss 23.236
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 23.329
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.369
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 23.071
Recognition finished, iteration 100 Loss 0.005
Perplexity dev: 3.110

==== Starting epoch 62 ====
  Batch 0 Loss 2.7668
  Batch 100 Loss 2.4216
  Batch 200 Loss 2.4841
  Batch 300 Loss 2.8734
  Batch 400 Loss 4.4214
  Batch 500 Loss 3.4122
  Batch 600 Loss 3.0166
  Batch 700 Loss 3.4139
Resetting 5032 PBs
Finished epoch 62 in 76.0 seconds
Perplexity training: 2.533

==== Starting epoch 63 ====
  Batch 0 Loss 2.7697
  Batch 100 Loss 1.9227
  Batch 200 Loss 2.0859
  Batch 300 Loss 3.1205
  Batch 400 Loss 2.4564
  Batch 500 Loss 4.1984
  Batch 600 Loss 5.1482
  Batch 700 Loss 1.1938
Resetting 5063 PBs
Finished epoch 63 in 77.0 seconds
Perplexity training: 2.475
Measuring development set...
Recognition iteration 0 Loss 22.782
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.419
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.234
Recognition finished, iteration 100 Loss 0.004
Recognition iteration 0 Loss 22.761
Recognition finished, iteration 100 Loss 0.006
Perplexity dev: 2.556
Finished training in 4611.71 seconds
Finished training after development set stopped improving.
