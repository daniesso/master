2019-06-30 19:08:18.346322: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-30 19:08:18.356295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-06-30 19:08:18.357183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-30 19:08:18.357427: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-30 19:08:18.358815: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-30 19:08:18.360333: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-30 19:08:18.360805: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-30 19:08:18.362708: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-30 19:08:18.364409: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-30 19:08:18.368906: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-30 19:08:18.372574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
Starting training procedure.
Loading training set...
2019-06-30 19:08:19.530146: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-06-30 19:08:19.907172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x38e1a80 executing computations on platform CUDA. Devices:
2019-06-30 19:08:19.907249: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-06-30 19:08:19.932955: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2019-06-30 19:08:19.937709: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x38f6940 executing computations on platform Host. Devices:
2019-06-30 19:08:19.937767: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-06-30 19:08:19.938963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-06-30 19:08:19.939038: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-30 19:08:19.939052: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-30 19:08:19.939076: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-30 19:08:19.939115: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-30 19:08:19.939135: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-30 19:08:19.939147: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-30 19:08:19.939158: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-30 19:08:19.941268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 1
2019-06-30 19:08:19.941354: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-30 19:08:19.943909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-30 19:08:19.943957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      1 
2019-06-30 19:08:19.943964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N 
2019-06-30 19:08:19.947081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30069 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
Saving vocab defined by training set...
Loading development set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


=== Model ===
Name: rnnpbnmt
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0.3
Num PBs: 1024
Bind hard: True
Binding strength: 1.0
Autoencode: False
PB learning rate: 0.01
Sigma: 0
p_reset: 0.3
Max recog epochs: 100


=== Training ===
Max epochs: 0
Early stopping steps: 20
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-06-30 19:08:26.333867: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-30 19:08:28.042564: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0630 19:08:28.479795 140322133100352 deprecation.py:323] From /lhome/daniesso/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 63.0669
  Batch 100 Loss 37.8441
  Batch 200 Loss 35.1898
  Batch 300 Loss 34.7859
  Batch 400 Loss 29.4832
  Batch 500 Loss 30.4142
  Batch 600 Loss 30.6708
  Batch 700 Loss 27.8423
Resetting 15047 PBs
Finished epoch 1 in 95.0 seconds
Perplexity training: 82.367
Measuring development set...
Recognition iteration 0 Loss 29.298
Recognition finished, iteration 100 Loss 26.320
Recognition iteration 0 Loss 28.481
Recognition finished, iteration 100 Loss 25.176
Recognition iteration 0 Loss 26.734
Recognition finished, iteration 100 Loss 24.044
Recognition iteration 0 Loss 30.256
Recognition finished, iteration 100 Loss 26.869
Perplexity dev: 36.147

==== Starting epoch 2 ====
  Batch 0 Loss 30.0725
  Batch 100 Loss 27.8611
  Batch 200 Loss 28.0999
  Batch 300 Loss 29.1147
  Batch 400 Loss 24.9958
  Batch 500 Loss 26.1851
  Batch 600 Loss 27.2240
  Batch 700 Loss 24.6051
Resetting 14845 PBs
Finished epoch 2 in 90.0 seconds
Perplexity training: 28.668

==== Starting epoch 3 ====
  Batch 0 Loss 26.7541
  Batch 100 Loss 25.2815
  Batch 200 Loss 25.6101
  Batch 300 Loss 26.2680
  Batch 400 Loss 22.6297
  Batch 500 Loss 23.7279
  Batch 600 Loss 24.0331
  Batch 700 Loss 22.3433
Resetting 15115 PBs
Finished epoch 3 in 91.0 seconds
Perplexity training: 21.331
Measuring development set...
Recognition iteration 0 Loss 26.305
Recognition finished, iteration 100 Loss 16.808
Recognition iteration 0 Loss 25.643
Recognition finished, iteration 100 Loss 15.841
Recognition iteration 0 Loss 23.387
Recognition finished, iteration 100 Loss 14.947
Recognition iteration 0 Loss 27.367
Recognition finished, iteration 100 Loss 17.146
Perplexity dev: 13.656

==== Starting epoch 4 ====
  Batch 0 Loss 24.1983
  Batch 100 Loss 23.4268
  Batch 200 Loss 22.9689
  Batch 300 Loss 24.4292
  Batch 400 Loss 20.2621
  Batch 500 Loss 22.0326
  Batch 600 Loss 23.1148
  Batch 700 Loss 20.0729
Resetting 15037 PBs
Finished epoch 4 in 91.0 seconds
Perplexity training: 17.145

==== Starting epoch 5 ====
  Batch 0 Loss 22.0398
  Batch 100 Loss 21.1320
  Batch 200 Loss 21.5949
  Batch 300 Loss 22.6074
  Batch 400 Loss 19.5378
  Batch 500 Loss 20.2626
  Batch 600 Loss 20.8921
  Batch 700 Loss 18.4099
Resetting 15172 PBs
Finished epoch 5 in 91.0 seconds
Perplexity training: 14.596
Measuring development set...
Recognition iteration 0 Loss 25.534
Recognition finished, iteration 100 Loss 11.909
Recognition iteration 0 Loss 24.541
Recognition finished, iteration 100 Loss 10.842
Recognition iteration 0 Loss 22.792
Recognition finished, iteration 100 Loss 10.375
Recognition iteration 0 Loss 26.427
Recognition finished, iteration 100 Loss 11.986
Perplexity dev: 8.822

==== Starting epoch 6 ====
  Batch 0 Loss 21.3882
  Batch 100 Loss 20.2047
  Batch 200 Loss 21.0810
  Batch 300 Loss 20.4411
  Batch 400 Loss 17.6630
  Batch 500 Loss 19.5059
  Batch 600 Loss 20.0205
  Batch 700 Loss 17.6990
Resetting 14939 PBs
Finished epoch 6 in 90.0 seconds
Perplexity training: 12.741

==== Starting epoch 7 ====
  Batch 0 Loss 20.9444
  Batch 100 Loss 17.9619
  Batch 200 Loss 18.5732
  Batch 300 Loss 19.8838
  Batch 400 Loss 17.2040
  Batch 500 Loss 17.4719
  Batch 600 Loss 18.4032
  Batch 700 Loss 16.6980
Resetting 15095 PBs
Finished epoch 7 in 91.0 seconds
Perplexity training: 11.423
Measuring development set...
Recognition iteration 0 Loss 24.971
Recognition finished, iteration 100 Loss 8.351
Recognition iteration 0 Loss 23.764
Recognition finished, iteration 100 Loss 7.346
Recognition iteration 0 Loss 22.294
Recognition finished, iteration 100 Loss 7.095
Recognition iteration 0 Loss 25.996
Recognition finished, iteration 100 Loss 8.454
Perplexity dev: 6.583

==== Starting epoch 8 ====
  Batch 0 Loss 18.6299
  Batch 100 Loss 17.5116
  Batch 200 Loss 17.6703
  Batch 300 Loss 19.2357
  Batch 400 Loss 15.9450
  Batch 500 Loss 16.2422
  Batch 600 Loss 18.3180
  Batch 700 Loss 15.6932
Resetting 14920 PBs
Finished epoch 8 in 90.0 seconds
Perplexity training: 10.463

==== Starting epoch 9 ====
  Batch 0 Loss 16.9416
  Batch 100 Loss 17.2996
  Batch 200 Loss 16.5586
  Batch 300 Loss 18.8497
  Batch 400 Loss 14.3813
  Batch 500 Loss 15.6579
  Batch 600 Loss 16.2707
  Batch 700 Loss 15.6057
Resetting 15026 PBs
Finished epoch 9 in 90.0 seconds
Perplexity training: 9.663
Measuring development set...
Recognition iteration 0 Loss 24.791
Recognition finished, iteration 100 Loss 5.813
Recognition iteration 0 Loss 23.524
Recognition finished, iteration 100 Loss 4.711
Recognition iteration 0 Loss 22.081
Recognition finished, iteration 100 Loss 4.783
Recognition iteration 0 Loss 25.856
Recognition finished, iteration 100 Loss 5.779
Perplexity dev: 5.324

==== Starting epoch 10 ====
  Batch 0 Loss 15.5772
  Batch 100 Loss 16.4652
  Batch 200 Loss 17.4700
  Batch 300 Loss 18.3433
  Batch 400 Loss 14.1307
  Batch 500 Loss 14.6381
  Batch 600 Loss 15.4301
  Batch 700 Loss 13.5905
Resetting 15221 PBs
Finished epoch 10 in 91.0 seconds
Perplexity training: 9.172

==== Starting epoch 11 ====
  Batch 0 Loss 15.3295
  Batch 100 Loss 15.5260
  Batch 200 Loss 15.9967
  Batch 300 Loss 17.2496
  Batch 400 Loss 12.8204
  Batch 500 Loss 13.1608
  Batch 600 Loss 14.6835
  Batch 700 Loss 13.5978
Resetting 14987 PBs
Finished epoch 11 in 92.0 seconds
Perplexity training: 8.762
Measuring development set...
Recognition iteration 0 Loss 24.663
Recognition finished, iteration 100 Loss 3.928
Recognition iteration 0 Loss 23.090
Recognition finished, iteration 100 Loss 2.973
Recognition iteration 0 Loss 21.847
Recognition finished, iteration 100 Loss 3.195
Recognition iteration 0 Loss 25.329
Recognition finished, iteration 100 Loss 3.793
Perplexity dev: 4.696

==== Starting epoch 12 ====
  Batch 0 Loss 14.9632
  Batch 100 Loss 14.8174
  Batch 200 Loss 15.4601
  Batch 300 Loss 16.1907
  Batch 400 Loss 12.9085
  Batch 500 Loss 13.4155
  Batch 600 Loss 14.7062
  Batch 700 Loss 12.2454
Resetting 14867 PBs
Finished epoch 12 in 90.0 seconds
Perplexity training: 8.265

==== Starting epoch 13 ====
  Batch 0 Loss 14.1101
  Batch 100 Loss 13.6737
  Batch 200 Loss 14.9157
  Batch 300 Loss 16.9396
  Batch 400 Loss 11.6409
  Batch 500 Loss 12.6788
  Batch 600 Loss 13.3225
  Batch 700 Loss 12.4054
Resetting 14987 PBs
Finished epoch 13 in 92.0 seconds
Perplexity training: 7.916
Measuring development set...
Recognition iteration 0 Loss 24.488
Recognition finished, iteration 100 Loss 2.596
Recognition iteration 0 Loss 22.845
Recognition finished, iteration 100 Loss 1.863
Recognition iteration 0 Loss 21.759
Recognition finished, iteration 100 Loss 1.997
Recognition iteration 0 Loss 25.341
Recognition finished, iteration 100 Loss 2.409
Perplexity dev: 4.499

==== Starting epoch 14 ====
  Batch 0 Loss 14.0513
  Batch 100 Loss 12.8425
  Batch 200 Loss 13.6667
  Batch 300 Loss 14.2928
  Batch 400 Loss 9.6274
  Batch 500 Loss 11.1761
  Batch 600 Loss 13.6753
  Batch 700 Loss 11.2227
Resetting 15098 PBs
Finished epoch 14 in 92.0 seconds
Perplexity training: 7.691

==== Starting epoch 15 ====
  Batch 0 Loss 11.9596
  Batch 100 Loss 13.2059
  Batch 200 Loss 12.4564
  Batch 300 Loss 13.4886
  Batch 400 Loss 11.5045
  Batch 500 Loss 12.1608
  Batch 600 Loss 12.9162
  Batch 700 Loss 12.4328
Resetting 14933 PBs
Finished epoch 15 in 93.0 seconds
Perplexity training: 7.406
Measuring development set...
Recognition iteration 0 Loss 24.484
Recognition finished, iteration 100 Loss 1.736
Recognition iteration 0 Loss 22.708
Recognition finished, iteration 100 Loss 1.088
Recognition iteration 0 Loss 21.576
Recognition finished, iteration 100 Loss 1.214
Recognition iteration 0 Loss 24.999
Recognition finished, iteration 100 Loss 1.535
Perplexity dev: 3.917

==== Starting epoch 16 ====
  Batch 0 Loss 12.5271
  Batch 100 Loss 14.7653
  Batch 200 Loss 12.1632
  Batch 300 Loss 14.4892
  Batch 400 Loss 10.9326
  Batch 500 Loss 10.7402
  Batch 600 Loss 14.1367
  Batch 700 Loss 9.9661
Resetting 14968 PBs
Finished epoch 16 in 94.0 seconds
Perplexity training: 7.185

==== Starting epoch 17 ====
  Batch 0 Loss 11.9761
  Batch 100 Loss 11.6790
  Batch 200 Loss 13.0340
  Batch 300 Loss 12.2294
  Batch 400 Loss 10.2880
  Batch 500 Loss 9.1364
  Batch 600 Loss 13.1201
  Batch 700 Loss 9.2557
Resetting 15216 PBs
Finished epoch 17 in 94.0 seconds
Perplexity training: 6.969
Measuring development set...
Recognition iteration 0 Loss 24.179
Recognition finished, iteration 100 Loss 1.168
Recognition iteration 0 Loss 22.568
Recognition finished, iteration 100 Loss 0.685
Recognition iteration 0 Loss 21.676
Recognition finished, iteration 100 Loss 0.725
Recognition iteration 0 Loss 24.870
Recognition finished, iteration 100 Loss 0.937
Perplexity dev: 3.644

==== Starting epoch 18 ====
  Batch 0 Loss 12.4280
  Batch 100 Loss 10.6706
  Batch 200 Loss 11.6107
  Batch 300 Loss 13.2978
  Batch 400 Loss 10.1732
  Batch 500 Loss 9.8257
  Batch 600 Loss 10.9753
  Batch 700 Loss 10.5513
Resetting 14947 PBs
Finished epoch 18 in 94.0 seconds
Perplexity training: 6.898

==== Starting epoch 19 ====
  Batch 0 Loss 10.9636
  Batch 100 Loss 11.2556
  Batch 200 Loss 13.2678
  Batch 300 Loss 13.0877
  Batch 400 Loss 11.8586
  Batch 500 Loss 11.1489
  Batch 600 Loss 10.5378
  Batch 700 Loss 10.6218
Resetting 15329 PBs
Finished epoch 19 in 94.0 seconds
Perplexity training: 6.717
Measuring development set...
Recognition iteration 0 Loss 24.197
Recognition finished, iteration 100 Loss 0.789
Recognition iteration 0 Loss 22.541
Recognition finished, iteration 100 Loss 0.408
Recognition iteration 0 Loss 21.367
Recognition finished, iteration 100 Loss 0.413
Recognition iteration 0 Loss 24.843
Recognition finished, iteration 100 Loss 0.574
Perplexity dev: 3.444

==== Starting epoch 20 ====
  Batch 0 Loss 11.2477
  Batch 100 Loss 11.1613
  Batch 200 Loss 13.7831
  Batch 300 Loss 13.7157
  Batch 400 Loss 9.1162
  Batch 500 Loss 10.0964
  Batch 600 Loss 10.4255
  Batch 700 Loss 9.9051
Resetting 14851 PBs
Finished epoch 20 in 95.0 seconds
Perplexity training: 6.683

==== Starting epoch 21 ====
  Batch 0 Loss 12.1169
  Batch 100 Loss 10.3189
  Batch 200 Loss 12.4248
  Batch 300 Loss 10.9513
  Batch 400 Loss 8.6110
  Batch 500 Loss 11.3924
  Batch 600 Loss 11.0453
  Batch 700 Loss 9.4219
Resetting 14972 PBs
Finished epoch 21 in 95.0 seconds
Perplexity training: 6.367
Measuring development set...
Recognition iteration 0 Loss 24.206
Recognition finished, iteration 100 Loss 0.550
Recognition iteration 0 Loss 22.206
Recognition finished, iteration 100 Loss 0.272
Recognition iteration 0 Loss 21.405
Recognition finished, iteration 100 Loss 0.283
Recognition iteration 0 Loss 24.636
Recognition finished, iteration 100 Loss 0.362
Perplexity dev: 3.292

==== Starting epoch 22 ====
  Batch 0 Loss 10.7960
  Batch 100 Loss 10.3026
  Batch 200 Loss 10.7268
  Batch 300 Loss 13.2019
  Batch 400 Loss 8.7610
  Batch 500 Loss 10.7333
  Batch 600 Loss 10.6966
  Batch 700 Loss 9.6462
Resetting 15111 PBs
Finished epoch 22 in 95.0 seconds
Perplexity training: 6.318

==== Starting epoch 23 ====
  Batch 0 Loss 9.3550
  Batch 100 Loss 12.9330
  Batch 200 Loss 11.4225
  Batch 300 Loss 14.2910
  Batch 400 Loss 11.3153
  Batch 500 Loss 9.8618
  Batch 600 Loss 11.2624
  Batch 700 Loss 10.0401
Resetting 14894 PBs
Finished epoch 23 in 96.0 seconds
Perplexity training: 6.285
Measuring development set...
Recognition iteration 0 Loss 24.261
Recognition finished, iteration 100 Loss 0.382
Recognition iteration 0 Loss 22.216
Recognition finished, iteration 100 Loss 0.180
Recognition iteration 0 Loss 21.157
Recognition finished, iteration 100 Loss 0.193
Recognition iteration 0 Loss 24.647
Recognition finished, iteration 100 Loss 0.240
Perplexity dev: 3.005

==== Starting epoch 24 ====
  Batch 0 Loss 10.7337
  Batch 100 Loss 10.3045
  Batch 200 Loss 9.4924
  Batch 300 Loss 11.4417
  Batch 400 Loss 8.4629
  Batch 500 Loss 10.1931
  Batch 600 Loss 11.6854
  Batch 700 Loss 7.9300
Resetting 15049 PBs
Finished epoch 24 in 96.0 seconds
Perplexity training: 6.143

==== Starting epoch 25 ====
  Batch 0 Loss 13.2473
  Batch 100 Loss 9.8692
  Batch 200 Loss 10.5674
  Batch 300 Loss 11.2857
  Batch 400 Loss 9.1622
  Batch 500 Loss 11.2078
  Batch 600 Loss 10.1887
  Batch 700 Loss 7.6359
Resetting 14961 PBs
Finished epoch 25 in 99.0 seconds
Perplexity training: 6.003
Measuring development set...
Recognition iteration 0 Loss 24.043
Recognition finished, iteration 100 Loss 0.269
Recognition iteration 0 Loss 22.094
Recognition finished, iteration 100 Loss 0.131
Recognition iteration 0 Loss 20.874
Recognition finished, iteration 100 Loss 0.124
Recognition iteration 0 Loss 24.468
Recognition finished, iteration 100 Loss 0.176
Perplexity dev: 3.204

==== Starting epoch 26 ====
  Batch 0 Loss 9.6465
  Batch 100 Loss 12.4470
  Batch 200 Loss 9.9697
  Batch 300 Loss 9.2061
  Batch 400 Loss 9.4425
  Batch 500 Loss 11.7199
  Batch 600 Loss 8.9116
  Batch 700 Loss 8.0315
Resetting 15148 PBs
Finished epoch 26 in 101.0 seconds
Perplexity training: 5.972

==== Starting epoch 27 ====
  Batch 0 Loss 10.8275
  Batch 100 Loss 8.8117
  Batch 200 Loss 10.3874
  Batch 300 Loss 9.7134
  Batch 400 Loss 10.1356
  Batch 500 Loss 8.9077
  Batch 600 Loss 10.2027
  Batch 700 Loss 9.7437
Resetting 14906 PBs
Finished epoch 27 in 100.0 seconds
Perplexity training: 5.935
Measuring development set...
Recognition iteration 0 Loss 24.218
Recognition finished, iteration 100 Loss 0.195
Recognition iteration 0 Loss 22.001
Recognition finished, iteration 100 Loss 0.090
Recognition iteration 0 Loss 20.867
Recognition finished, iteration 100 Loss 0.097
Recognition iteration 0 Loss 24.475
Recognition finished, iteration 100 Loss 0.134
Perplexity dev: 2.752

==== Starting epoch 28 ====
  Batch 0 Loss 10.0850
  Batch 100 Loss 11.2740
  Batch 200 Loss 10.9100
  Batch 300 Loss 8.5385
  Batch 400 Loss 7.9357
  Batch 500 Loss 8.0490
  Batch 600 Loss 9.1244
  Batch 700 Loss 7.2132
Resetting 15185 PBs
Finished epoch 28 in 101.0 seconds
Perplexity training: 5.873

==== Starting epoch 29 ====
  Batch 0 Loss 9.6400
  Batch 100 Loss 6.7969
  Batch 200 Loss 9.1412
  Batch 300 Loss 9.7442
  Batch 400 Loss 10.3188
  Batch 500 Loss 6.8368
  Batch 600 Loss 10.6247
  Batch 700 Loss 8.3032
Resetting 15042 PBs
Finished epoch 29 in 101.0 seconds
Perplexity training: 5.831
Measuring development set...
Recognition iteration 0 Loss 24.352
Recognition finished, iteration 100 Loss 0.184
Recognition iteration 0 Loss 22.053
Recognition finished, iteration 100 Loss 0.066
Recognition iteration 0 Loss 20.960
Recognition finished, iteration 100 Loss 0.071
Recognition iteration 0 Loss 24.102
Recognition finished, iteration 100 Loss 0.098
Perplexity dev: 2.791

==== Starting epoch 30 ====
  Batch 0 Loss 8.9791
  Batch 100 Loss 7.7691
  Batch 200 Loss 10.8406
  Batch 300 Loss 10.7908
  Batch 400 Loss 7.0097
  Batch 500 Loss 7.5284
  Batch 600 Loss 6.1426
  Batch 700 Loss 9.0665
Resetting 14996 PBs
Finished epoch 30 in 101.0 seconds
Perplexity training: 5.749

==== Starting epoch 31 ====
  Batch 0 Loss 9.7913
  Batch 100 Loss 7.8728
  Batch 200 Loss 9.6163
  Batch 300 Loss 11.3106
  Batch 400 Loss 7.3396
  Batch 500 Loss 7.9754
  Batch 600 Loss 10.0773
  Batch 700 Loss 8.8447
Resetting 14938 PBs
Finished epoch 31 in 101.0 seconds
Perplexity training: 5.691
Measuring development set...
Recognition iteration 0 Loss 24.156
Recognition finished, iteration 100 Loss 0.125
Recognition iteration 0 Loss 22.007
Recognition finished, iteration 100 Loss 0.060
Recognition iteration 0 Loss 20.779
Recognition finished, iteration 100 Loss 0.054
Recognition iteration 0 Loss 23.955
Recognition finished, iteration 100 Loss 0.083
Perplexity dev: 2.752

==== Starting epoch 32 ====
  Batch 0 Loss 10.1728
  Batch 100 Loss 9.3714
  Batch 200 Loss 8.5820
  Batch 300 Loss 11.1915
  Batch 400 Loss 8.0611
  Batch 500 Loss 8.2516
  Batch 600 Loss 9.4302
  Batch 700 Loss 7.6660
Resetting 14822 PBs
Finished epoch 32 in 102.0 seconds
Perplexity training: 5.650

==== Starting epoch 33 ====
  Batch 0 Loss 9.0621
  Batch 100 Loss 8.8073
  Batch 200 Loss 8.1341
  Batch 300 Loss 8.5139
  Batch 400 Loss 6.6558
  Batch 500 Loss 9.4448
  Batch 600 Loss 9.2288
  Batch 700 Loss 9.1697
Resetting 15021 PBs
Finished epoch 33 in 102.0 seconds
Perplexity training: 5.484
Measuring development set...
Recognition iteration 0 Loss 24.299
Recognition finished, iteration 100 Loss 0.097
Recognition iteration 0 Loss 21.825
Recognition finished, iteration 100 Loss 0.046
Recognition iteration 0 Loss 20.894
Recognition finished, iteration 100 Loss 0.044
Recognition iteration 0 Loss 24.077
Recognition finished, iteration 100 Loss 0.063
Perplexity dev: 2.735

==== Starting epoch 34 ====
  Batch 0 Loss 10.0007
  Batch 100 Loss 7.4053
  Batch 200 Loss 7.1143
  Batch 300 Loss 8.1996
  Batch 400 Loss 6.6809
  Batch 500 Loss 7.9477
  Batch 600 Loss 8.7618
  Batch 700 Loss 7.4882
Resetting 15004 PBs
Finished epoch 34 in 103.0 seconds
Perplexity training: 5.528

==== Starting epoch 35 ====
  Batch 0 Loss 8.7935
  Batch 100 Loss 10.8207
  Batch 200 Loss 6.2394
  Batch 300 Loss 10.6975
  Batch 400 Loss 8.4842
  Batch 500 Loss 8.6810
  Batch 600 Loss 7.2114
  Batch 700 Loss 6.3937
Resetting 15011 PBs
Finished epoch 35 in 103.0 seconds
Perplexity training: 5.425
Measuring development set...
Recognition iteration 0 Loss 24.220
Recognition finished, iteration 100 Loss 0.092
Recognition iteration 0 Loss 21.945
Recognition finished, iteration 100 Loss 0.042
Recognition iteration 0 Loss 20.677
Recognition finished, iteration 100 Loss 0.038
Recognition iteration 0 Loss 24.133
Recognition finished, iteration 100 Loss 0.052
Perplexity dev: 2.977

==== Starting epoch 36 ====
  Batch 0 Loss 9.2397
  Batch 100 Loss 6.9617
  Batch 200 Loss 6.6878
  Batch 300 Loss 7.8892
  Batch 400 Loss 5.5041
  Batch 500 Loss 8.5141
  Batch 600 Loss 8.9792
  Batch 700 Loss 7.2484
Resetting 15051 PBs
Finished epoch 36 in 104.0 seconds
Perplexity training: 5.425

==== Starting epoch 37 ====
  Batch 0 Loss 10.0130
  Batch 100 Loss 9.5099
  Batch 200 Loss 8.5529
  Batch 300 Loss 8.9283
  Batch 400 Loss 9.0194
  Batch 500 Loss 8.5717
  Batch 600 Loss 9.2166
  Batch 700 Loss 7.6216
Resetting 15115 PBs
Finished epoch 37 in 105.0 seconds
Perplexity training: 5.387
Measuring development set...
Recognition iteration 0 Loss 23.859
Recognition finished, iteration 100 Loss 0.071
Recognition iteration 0 Loss 21.659
Recognition finished, iteration 100 Loss 0.033
Recognition iteration 0 Loss 20.612
Recognition finished, iteration 100 Loss 0.030
Recognition iteration 0 Loss 23.837
Recognition finished, iteration 100 Loss 0.043
Perplexity dev: 2.648

==== Starting epoch 38 ====
  Batch 0 Loss 10.0136
  Batch 100 Loss 8.3869
  Batch 200 Loss 8.7591
  Batch 300 Loss 7.1747
  Batch 400 Loss 8.2690
  Batch 500 Loss 10.1941
  Batch 600 Loss 8.9430
  Batch 700 Loss 9.4643
Resetting 14910 PBs
Finished epoch 38 in 104.0 seconds
Perplexity training: 5.353

==== Starting epoch 39 ====
  Batch 0 Loss 8.0363
  Batch 100 Loss 11.2539
  Batch 200 Loss 8.6658
  Batch 300 Loss 7.3768
  Batch 400 Loss 6.5535
  Batch 500 Loss 8.4085
  Batch 600 Loss 8.5476
  Batch 700 Loss 7.3066
Resetting 14915 PBs
Finished epoch 39 in 106.0 seconds
Perplexity training: 5.309
Measuring development set...
Recognition iteration 0 Loss 23.984
Recognition finished, iteration 100 Loss 0.062
Recognition iteration 0 Loss 21.713
Recognition finished, iteration 100 Loss 0.030
Recognition iteration 0 Loss 20.393
Recognition finished, iteration 100 Loss 0.026
Recognition iteration 0 Loss 23.741
Recognition finished, iteration 100 Loss 0.037
Perplexity dev: 2.472

==== Starting epoch 40 ====
  Batch 0 Loss 7.0028
  Batch 100 Loss 9.7493
  Batch 200 Loss 9.0401
  Batch 300 Loss 9.6434
  Batch 400 Loss 6.8639
  Batch 500 Loss 8.7185
  Batch 600 Loss 8.3771
  Batch 700 Loss 7.5429
Resetting 15060 PBs
Finished epoch 40 in 105.0 seconds
Perplexity training: 5.206

==== Starting epoch 41 ====
  Batch 0 Loss 11.0737
  Batch 100 Loss 8.0872
  Batch 200 Loss 8.2693
  Batch 300 Loss 9.3807
  Batch 400 Loss 7.5162
  Batch 500 Loss 6.5190
  Batch 600 Loss 9.4171
  Batch 700 Loss 7.6120
Resetting 15058 PBs
Finished epoch 41 in 106.0 seconds
Perplexity training: 5.232
Measuring development set...
Recognition iteration 0 Loss 23.997
Recognition finished, iteration 100 Loss 0.044
Recognition iteration 0 Loss 21.854
Recognition finished, iteration 100 Loss 0.026
Recognition iteration 0 Loss 20.488
Recognition finished, iteration 100 Loss 0.022
Recognition iteration 0 Loss 23.508
Recognition finished, iteration 100 Loss 0.031
Perplexity dev: 2.345

==== Starting epoch 42 ====
  Batch 0 Loss 9.8807
  Batch 100 Loss 7.7741
  Batch 200 Loss 10.0926
  Batch 300 Loss 9.4798
  Batch 400 Loss 7.4195
  Batch 500 Loss 7.9583
  Batch 600 Loss 7.0216
  Batch 700 Loss 5.6039
Resetting 15029 PBs
Finished epoch 42 in 106.0 seconds
Perplexity training: 5.219

==== Starting epoch 43 ====
  Batch 0 Loss 8.7395
  Batch 100 Loss 7.4401
  Batch 200 Loss 8.3353
  Batch 300 Loss 7.6877
  Batch 400 Loss 6.0777
  Batch 500 Loss 7.5195
  Batch 600 Loss 8.8997
  Batch 700 Loss 5.8513
Resetting 14773 PBs
Finished epoch 43 in 107.0 seconds
Perplexity training: 5.211
Measuring development set...
Recognition iteration 0 Loss 23.868
Recognition finished, iteration 100 Loss 0.047
Recognition iteration 0 Loss 21.584
Recognition finished, iteration 100 Loss 0.021
Recognition iteration 0 Loss 20.506
Recognition finished, iteration 100 Loss 0.019
Recognition iteration 0 Loss 23.502
Recognition finished, iteration 100 Loss 0.027
Perplexity dev: 2.322

==== Starting epoch 44 ====
  Batch 0 Loss 8.7156
  Batch 100 Loss 4.8244
  Batch 200 Loss 8.3114
  Batch 300 Loss 10.2012
  Batch 400 Loss 6.9721
  Batch 500 Loss 7.2520
  Batch 600 Loss 7.8260
  Batch 700 Loss 7.6146
Resetting 14952 PBs
Finished epoch 44 in 107.0 seconds
Perplexity training: 5.012

==== Starting epoch 45 ====
  Batch 0 Loss 8.0415
  Batch 100 Loss 8.0386
  Batch 200 Loss 6.8971
  Batch 300 Loss 7.6578
  Batch 400 Loss 5.4869
  Batch 500 Loss 8.4429
  Batch 600 Loss 8.2472
  Batch 700 Loss 9.0144
Resetting 15201 PBs
Finished epoch 45 in 107.0 seconds
Perplexity training: 5.065
Measuring development set...
Recognition iteration 0 Loss 23.822
Recognition finished, iteration 100 Loss 0.039
Recognition iteration 0 Loss 21.647
Recognition finished, iteration 100 Loss 0.020
Recognition iteration 0 Loss 20.466
Recognition finished, iteration 100 Loss 0.017
Recognition iteration 0 Loss 23.557
Recognition finished, iteration 100 Loss 0.025
Perplexity dev: 2.247

==== Starting epoch 46 ====
  Batch 0 Loss 8.0316
  Batch 100 Loss 8.0733
  Batch 200 Loss 7.0403
  Batch 300 Loss 12.0689
  Batch 400 Loss 8.2351
  Batch 500 Loss 6.9869
  Batch 600 Loss 8.4153
  Batch 700 Loss 7.0224
Resetting 14993 PBs
Finished epoch 46 in 108.0 seconds
Perplexity training: 5.072

==== Starting epoch 47 ====
  Batch 0 Loss 7.4735
  Batch 100 Loss 6.9709
  Batch 200 Loss 7.3920
  Batch 300 Loss 8.3394
  Batch 400 Loss 6.9359
  Batch 500 Loss 6.7315
  Batch 600 Loss 7.7626
  Batch 700 Loss 8.1404
Resetting 15060 PBs
Finished epoch 47 in 107.0 seconds
Perplexity training: 4.985
Measuring development set...
Recognition iteration 0 Loss 23.797
Recognition finished, iteration 100 Loss 0.034
Recognition iteration 0 Loss 21.522
Recognition finished, iteration 100 Loss 0.018
Recognition iteration 0 Loss 20.265
Recognition finished, iteration 100 Loss 0.015
Recognition iteration 0 Loss 23.616
Recognition finished, iteration 100 Loss 0.021
Perplexity dev: 2.697

==== Starting epoch 48 ====
  Batch 0 Loss 9.4952
  Batch 100 Loss 6.7349
  Batch 200 Loss 8.4843
  Batch 300 Loss 9.2091
  Batch 400 Loss 5.7442
  Batch 500 Loss 7.6847
  Batch 600 Loss 7.7927
  Batch 700 Loss 8.0361
Resetting 14920 PBs
Finished epoch 48 in 109.0 seconds
Perplexity training: 4.965

==== Starting epoch 49 ====
  Batch 0 Loss 7.4168
  Batch 100 Loss 5.9178
  Batch 200 Loss 8.1380
  Batch 300 Loss 9.8175
  Batch 400 Loss 7.1245
  Batch 500 Loss 7.8213
  Batch 600 Loss 7.8936
  Batch 700 Loss 7.2439
Resetting 14965 PBs
Finished epoch 49 in 108.0 seconds
Perplexity training: 4.963
Measuring development set...
Recognition iteration 0 Loss 23.850
Recognition finished, iteration 100 Loss 0.033
Recognition iteration 0 Loss 21.486
Recognition finished, iteration 100 Loss 0.016
Recognition iteration 0 Loss 20.282
Recognition finished, iteration 100 Loss 0.014
Recognition iteration 0 Loss 23.521
Recognition finished, iteration 100 Loss 0.019
Perplexity dev: 2.346

==== Starting epoch 50 ====
  Batch 0 Loss 7.6979
  Batch 100 Loss 7.1373
  Batch 200 Loss 7.3715
  Batch 300 Loss 7.2723
  Batch 400 Loss 6.0334
  Batch 500 Loss 7.9767
  Batch 600 Loss 7.9924
  Batch 700 Loss 6.9960
Resetting 15018 PBs
Finished epoch 50 in 123.0 seconds
Perplexity training: 4.913

==== Starting epoch 51 ====
  Batch 0 Loss 7.9734
  Batch 100 Loss 8.0351
  Batch 200 Loss 6.2387
  Batch 300 Loss 8.8455
  Batch 400 Loss 8.6436
  Batch 500 Loss 7.1447
  Batch 600 Loss 9.6080
  Batch 700 Loss 5.9406
Resetting 15128 PBs
Finished epoch 51 in 116.0 seconds
Perplexity training: 4.908
Measuring development set...
Recognition iteration 0 Loss 23.721
Recognition finished, iteration 100 Loss 0.030
Recognition iteration 0 Loss 21.562
Recognition finished, iteration 100 Loss 0.015
Recognition iteration 0 Loss 20.276
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 23.517
Recognition finished, iteration 100 Loss 0.019
Perplexity dev: 2.103

==== Starting epoch 52 ====
  Batch 0 Loss 9.2901
  Batch 100 Loss 6.4291
  Batch 200 Loss 9.0496
  Batch 300 Loss 9.9838
  Batch 400 Loss 6.8527
  Batch 500 Loss 7.0288
  Batch 600 Loss 8.9478
  Batch 700 Loss 6.9621
Resetting 15031 PBs
Finished epoch 52 in 119.0 seconds
Perplexity training: 4.920

==== Starting epoch 53 ====
  Batch 0 Loss 8.8612
  Batch 100 Loss 7.6933
  Batch 200 Loss 10.2801
  Batch 300 Loss 10.2768
  Batch 400 Loss 5.5449
  Batch 500 Loss 8.1854
  Batch 600 Loss 8.0616
  Batch 700 Loss 6.3028
Resetting 14769 PBs
Finished epoch 53 in 118.0 seconds
Perplexity training: 4.878
Measuring development set...
Recognition iteration 0 Loss 23.600
Recognition finished, iteration 100 Loss 0.027
Recognition iteration 0 Loss 21.077
Recognition finished, iteration 100 Loss 0.013
Recognition iteration 0 Loss 20.090
Recognition finished, iteration 100 Loss 0.011
Recognition iteration 0 Loss 23.346
Recognition finished, iteration 100 Loss 0.016
Perplexity dev: 2.272

==== Starting epoch 54 ====
  Batch 0 Loss 6.9655
  Batch 100 Loss 7.2712
  Batch 200 Loss 8.4785
  Batch 300 Loss 7.8224
  Batch 400 Loss 8.2195
  Batch 500 Loss 6.6461
  Batch 600 Loss 9.6874
  Batch 700 Loss 6.7886
Resetting 14981 PBs
Finished epoch 54 in 118.0 seconds
Perplexity training: 4.759

==== Starting epoch 55 ====
  Batch 0 Loss 8.0354
  Batch 100 Loss 7.0610
  Batch 200 Loss 8.5046
  Batch 300 Loss 7.9418
  Batch 400 Loss 7.9897
  Batch 500 Loss 6.7638
  Batch 600 Loss 5.8874
  Batch 700 Loss 7.2423
Resetting 15042 PBs
Finished epoch 55 in 119.0 seconds
Perplexity training: 4.814
Measuring development set...
Recognition iteration 0 Loss 23.513
Recognition finished, iteration 100 Loss 0.024
Recognition iteration 0 Loss 21.356
Recognition finished, iteration 100 Loss 0.011
Recognition iteration 0 Loss 20.295
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 23.545
Recognition finished, iteration 100 Loss 0.015
Perplexity dev: 2.137

==== Starting epoch 56 ====
  Batch 0 Loss 8.3245
  Batch 100 Loss 8.2590
  Batch 200 Loss 7.4119
  Batch 300 Loss 8.4554
  Batch 400 Loss 9.7372
  Batch 500 Loss 6.5091
  Batch 600 Loss 7.9731
  Batch 700 Loss 7.3680
Resetting 14954 PBs
Finished epoch 56 in 121.0 seconds
Perplexity training: 4.813

==== Starting epoch 57 ====
  Batch 0 Loss 7.0783
  Batch 100 Loss 7.8443
  Batch 200 Loss 6.5477
  Batch 300 Loss 7.0627
  Batch 400 Loss 6.0168
  Batch 500 Loss 5.9806
  Batch 600 Loss 9.6146
  Batch 700 Loss 6.3567
Resetting 14832 PBs
Finished epoch 57 in 120.0 seconds
Perplexity training: 4.790
Measuring development set...
Recognition iteration 0 Loss 23.737
Recognition finished, iteration 100 Loss 0.023
Recognition iteration 0 Loss 21.238
Recognition finished, iteration 100 Loss 0.011
Recognition iteration 0 Loss 20.288
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 23.213
Recognition finished, iteration 100 Loss 0.013
Perplexity dev: 2.243

==== Starting epoch 58 ====
  Batch 0 Loss 6.5765
  Batch 100 Loss 6.9931
  Batch 200 Loss 10.4052
  Batch 300 Loss 6.8458
  Batch 400 Loss 7.1052
  Batch 500 Loss 5.3534
  Batch 600 Loss 5.6284
  Batch 700 Loss 8.3349
Resetting 15018 PBs
Finished epoch 58 in 124.0 seconds
Perplexity training: 4.686

==== Starting epoch 59 ====
  Batch 0 Loss 6.6922
  Batch 100 Loss 8.6300
  Batch 200 Loss 7.2936
  Batch 300 Loss 7.5621
  Batch 400 Loss 8.6712
  Batch 500 Loss 6.2394
  Batch 600 Loss 7.5316
  Batch 700 Loss 6.2971
Resetting 14933 PBs
Finished epoch 59 in 124.0 seconds
Perplexity training: 4.705
Measuring development set...
Recognition iteration 0 Loss 23.880
Recognition finished, iteration 100 Loss 0.024
Recognition iteration 0 Loss 21.368
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 20.250
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 23.134
Recognition finished, iteration 100 Loss 0.013
Perplexity dev: 2.384

==== Starting epoch 60 ====
  Batch 0 Loss 9.8548
  Batch 100 Loss 6.9680
  Batch 200 Loss 5.5727
  Batch 300 Loss 9.9624
  Batch 400 Loss 6.4179
  Batch 500 Loss 4.3279
  Batch 600 Loss 6.5704
  Batch 700 Loss 4.2470
Resetting 14759 PBs
Finished epoch 60 in 123.0 seconds
Perplexity training: 4.711

==== Starting epoch 61 ====
  Batch 0 Loss 8.6841
  Batch 100 Loss 5.7326
  Batch 200 Loss 4.8635
  Batch 300 Loss 7.4183
  Batch 400 Loss 7.2028
  Batch 500 Loss 6.1265
  Batch 600 Loss 9.3247
  Batch 700 Loss 7.5367
Resetting 14968 PBs
Finished epoch 61 in 125.0 seconds
Perplexity training: 4.612
Measuring development set...
Recognition iteration 0 Loss 23.887
Recognition finished, iteration 100 Loss 0.019
Recognition iteration 0 Loss 21.252
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 20.221
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 23.237
Recognition finished, iteration 100 Loss 0.012
Perplexity dev: 2.707

==== Starting epoch 62 ====
  Batch 0 Loss 7.1736
  Batch 100 Loss 6.0840
  Batch 200 Loss 6.2111
  Batch 300 Loss 8.4437
  Batch 400 Loss 6.6626
  Batch 500 Loss 5.4458
  Batch 600 Loss 6.3733
  Batch 700 Loss 5.8828
Resetting 15052 PBs
Finished epoch 62 in 126.0 seconds
Perplexity training: 4.649

==== Starting epoch 63 ====
  Batch 0 Loss 6.6752
  Batch 100 Loss 8.5239
  Batch 200 Loss 12.0472
  Batch 300 Loss 7.2319
  Batch 400 Loss 7.2621
  Batch 500 Loss 7.0530
  Batch 600 Loss 8.6194
  Batch 700 Loss 7.8633
Resetting 14804 PBs
Finished epoch 63 in 127.0 seconds
Perplexity training: 4.692
Measuring development set...
Recognition iteration 0 Loss 23.963
Recognition finished, iteration 100 Loss 0.018
Recognition iteration 0 Loss 21.166
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 20.216
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 23.087
Recognition finished, iteration 100 Loss 0.011
Perplexity dev: 2.258

==== Starting epoch 64 ====
  Batch 0 Loss 5.9161
  Batch 100 Loss 7.4734
  Batch 200 Loss 7.7772
  Batch 300 Loss 9.0558
  Batch 400 Loss 6.7461
  Batch 500 Loss 6.6733
  Batch 600 Loss 6.3148
  Batch 700 Loss 9.1987
Resetting 15024 PBs
Finished epoch 64 in 123.0 seconds
Perplexity training: 4.579

==== Starting epoch 65 ====
  Batch 0 Loss 9.1857
  Batch 100 Loss 6.2292
  Batch 200 Loss 8.6843
  Batch 300 Loss 9.3212
  Batch 400 Loss 6.8517
  Batch 500 Loss 5.8425
  Batch 600 Loss 8.1005
  Batch 700 Loss 6.2219
Resetting 14998 PBs
Finished epoch 65 in 122.0 seconds
Perplexity training: 4.613
Measuring development set...
Recognition iteration 0 Loss 23.908
Recognition finished, iteration 100 Loss 0.015
Recognition iteration 0 Loss 21.120
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 20.137
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 23.146
Recognition finished, iteration 100 Loss 0.011
Perplexity dev: 2.309

==== Starting epoch 66 ====
  Batch 0 Loss 6.1609
  Batch 100 Loss 8.0335
  Batch 200 Loss 9.1230
  Batch 300 Loss 8.3804
  Batch 400 Loss 6.1635
  Batch 500 Loss 6.7362
  Batch 600 Loss 6.3028
  Batch 700 Loss 5.6472
Resetting 15154 PBs
Finished epoch 66 in 124.0 seconds
Perplexity training: 4.561

==== Starting epoch 67 ====
  Batch 0 Loss 7.3558
  Batch 100 Loss 6.0274
  Batch 200 Loss 9.8381
  Batch 300 Loss 7.3964
  Batch 400 Loss 9.9394
  Batch 500 Loss 7.2984
  Batch 600 Loss 7.6510
  Batch 700 Loss 5.5438
Resetting 14997 PBs
Finished epoch 67 in 126.0 seconds
Perplexity training: 4.612
Measuring development set...
Recognition iteration 0 Loss 23.774
Recognition finished, iteration 100 Loss 0.016
Recognition iteration 0 Loss 20.939
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 20.207
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 23.255
Recognition finished, iteration 100 Loss 0.009
Perplexity dev: 2.752

==== Starting epoch 68 ====
  Batch 0 Loss 7.1444
  Batch 100 Loss 5.0720
  Batch 200 Loss 7.0874
  Batch 300 Loss 9.1548
  Batch 400 Loss 7.5341
  Batch 500 Loss 7.3042
  Batch 600 Loss 5.9232
  Batch 700 Loss 6.8061
Resetting 15069 PBs
Finished epoch 68 in 125.0 seconds
Perplexity training: 4.493

==== Starting epoch 69 ====
  Batch 0 Loss 7.1014
  Batch 100 Loss 8.4098
  Batch 200 Loss 6.4278
  Batch 300 Loss 8.8361
  Batch 400 Loss 6.9305
  Batch 500 Loss 6.3347
  Batch 600 Loss 7.4080
  Batch 700 Loss 4.1670
Resetting 14907 PBs
Finished epoch 69 in 125.0 seconds
Perplexity training: 4.509
Measuring development set...
Recognition iteration 0 Loss 23.656
Recognition finished, iteration 100 Loss 0.015
Recognition iteration 0 Loss 21.106
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 20.356
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 23.315
Recognition finished, iteration 100 Loss 0.009
Perplexity dev: 2.636

==== Starting epoch 70 ====
  Batch 0 Loss 7.4829
  Batch 100 Loss 7.4701
  Batch 200 Loss 8.2742
  Batch 300 Loss 6.3475
  Batch 400 Loss 8.3160
  Batch 500 Loss 7.7982
  Batch 600 Loss 7.1001
  Batch 700 Loss 7.7309
Resetting 14950 PBs
Finished epoch 70 in 127.0 seconds
Perplexity training: 4.486

==== Starting epoch 71 ====
  Batch 0 Loss 8.1648
  Batch 100 Loss 7.6886
  Batch 200 Loss 7.5275
  Batch 300 Loss 8.2746
  Batch 400 Loss 5.9059
  Batch 500 Loss 6.3701
  Batch 600 Loss 7.0758
  Batch 700 Loss 5.9255
Resetting 15193 PBs
Finished epoch 71 in 127.0 seconds
Perplexity training: 4.495
Measuring development set...
Recognition iteration 0 Loss 23.880
Recognition finished, iteration 100 Loss 0.016
Recognition iteration 0 Loss 21.368
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 20.339
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 23.311
Recognition finished, iteration 100 Loss 0.009
Perplexity dev: 2.666
Finished training in 8202.12 seconds
Finished training after development set stopped improving.
