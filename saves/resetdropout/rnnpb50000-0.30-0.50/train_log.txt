Starting training procedure.
Loading training set...
2019-07-03 07:48:47.219250: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-03 07:48:47.243247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-03 07:48:47.244095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:00:05.0
2019-07-03 07:48:47.244475: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-03 07:48:47.246430: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-03 07:48:47.247686: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-03 07:48:47.247952: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-03 07:48:47.249507: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-03 07:48:47.250777: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-03 07:48:47.254585: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-03 07:48:47.254709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-03 07:48:47.255538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-03 07:48:47.256155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-03 07:48:47.256574: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-03 07:48:47.366306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-03 07:48:47.367041: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2bb9b40 executing computations on platform CUDA. Devices:
2019-07-03 07:48:47.367096: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Quadro P4000, Compute Capability 6.1
2019-07-03 07:48:47.369727: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600010000 Hz
2019-07-03 07:48:47.370256: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b55680 executing computations on platform Host. Devices:
2019-07-03 07:48:47.370275: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-03 07:48:47.370493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-03 07:48:47.371127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:00:05.0
2019-07-03 07:48:47.371168: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-03 07:48:47.371180: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-03 07:48:47.371190: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-03 07:48:47.371215: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-03 07:48:47.371232: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-03 07:48:47.371258: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-03 07:48:47.371269: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-03 07:48:47.371312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-03 07:48:47.371872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-03 07:48:47.372388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-03 07:48:47.372420: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-03 07:48:47.373216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-03 07:48:47.373229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-03 07:48:47.373236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-03 07:48:47.373313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-03 07:48:47.373832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-03 07:48:47.374322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7629 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 0000:00:05.0, compute capability: 6.1)
Saving vocab defined by training set...
Loading development set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 468799
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30
  Reversed: False

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2438
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1385 (original 1381)
  Longest: 30


=== Model ===
Name: rnnpbnmt
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0.5
Num PBs: 1024
Bind hard: True
Binding strength: 1.0
Autoencode: False
PB learning rate: 0.01
Sigma: 0
p_reset: 0.3
Max recog epochs: 100


=== Training ===
Max epochs: 0
Early stopping steps: 20
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-07-03 07:48:54.055952: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-03 07:48:55.394518: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0703 07:48:55.786452 140038917416768 deprecation.py:323] From /home/paperspace/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 62.6144
  Batch 100 Loss 39.0935
  Batch 200 Loss 32.2148
  Batch 300 Loss 30.2390
  Batch 400 Loss 33.0000
  Batch 500 Loss 28.4984
  Batch 600 Loss 30.3010
  Batch 700 Loss 28.9797
Resetting 14873 PBs
Finished epoch 1 in 108.0 seconds
Perplexity training: 84.116
Measuring development set...
Recognition iteration 0 Loss 29.460
Recognition finished, iteration 100 Loss 26.644
Recognition iteration 0 Loss 28.457
Recognition finished, iteration 100 Loss 25.256
Recognition iteration 0 Loss 28.753
Recognition finished, iteration 100 Loss 26.101
Recognition iteration 0 Loss 29.387
Recognition finished, iteration 100 Loss 26.078
Perplexity dev: 38.342

==== Starting epoch 2 ====
  Batch 0 Loss 30.7919
  Batch 100 Loss 28.2452
  Batch 200 Loss 26.2102
  Batch 300 Loss 24.8964
  Batch 400 Loss 28.1363
  Batch 500 Loss 24.7652
  Batch 600 Loss 26.5159
  Batch 700 Loss 25.9248
Resetting 15044 PBs
Finished epoch 2 in 104.0 seconds
Perplexity training: 30.588

==== Starting epoch 3 ====
  Batch 0 Loss 28.6849
  Batch 100 Loss 25.4992
  Batch 200 Loss 23.9782
  Batch 300 Loss 22.7702
  Batch 400 Loss 25.8198
  Batch 500 Loss 22.9037
  Batch 600 Loss 24.0147
  Batch 700 Loss 23.5935
Resetting 14825 PBs
Finished epoch 3 in 105.0 seconds
Perplexity training: 23.450
Measuring development set...
Recognition iteration 0 Loss 26.512
Recognition finished, iteration 100 Loss 18.012
Recognition iteration 0 Loss 25.330
Recognition finished, iteration 100 Loss 16.762
Recognition iteration 0 Loss 25.666
Recognition finished, iteration 100 Loss 17.864
Recognition iteration 0 Loss 26.529
Recognition finished, iteration 100 Loss 17.678
Perplexity dev: 16.190

==== Starting epoch 4 ====
  Batch 0 Loss 26.6015
  Batch 100 Loss 23.6235
  Batch 200 Loss 21.9747
  Batch 300 Loss 21.1617
  Batch 400 Loss 23.4512
  Batch 500 Loss 20.8744
  Batch 600 Loss 22.9287
  Batch 700 Loss 22.3033
Resetting 15165 PBs
Finished epoch 4 in 104.0 seconds
Perplexity training: 19.006

==== Starting epoch 5 ====
  Batch 0 Loss 24.6309
  Batch 100 Loss 21.5934
  Batch 200 Loss 19.7737
  Batch 300 Loss 20.5480
  Batch 400 Loss 22.5579
  Batch 500 Loss 19.8072
  Batch 600 Loss 21.0464
  Batch 700 Loss 20.6662
Resetting 15050 PBs
Finished epoch 5 in 105.0 seconds
Perplexity training: 16.462
Measuring development set...
Recognition iteration 0 Loss 25.316
Recognition finished, iteration 100 Loss 13.383
Recognition iteration 0 Loss 24.227
Recognition finished, iteration 100 Loss 12.252
Recognition iteration 0 Loss 24.829
Recognition finished, iteration 100 Loss 13.211
Recognition iteration 0 Loss 25.296
Recognition finished, iteration 100 Loss 13.081
Perplexity dev: 10.406

==== Starting epoch 6 ====
  Batch 0 Loss 22.7212
  Batch 100 Loss 20.7312
  Batch 200 Loss 18.6674
  Batch 300 Loss 18.9773
  Batch 400 Loss 21.7763
  Batch 500 Loss 19.5490
  Batch 600 Loss 20.1204
  Batch 700 Loss 19.5345
Resetting 14996 PBs
Finished epoch 6 in 104.0 seconds
Perplexity training: 14.595

==== Starting epoch 7 ====
  Batch 0 Loss 22.0232
  Batch 100 Loss 20.5059
  Batch 200 Loss 17.5872
  Batch 300 Loss 17.6321
  Batch 400 Loss 20.7326
  Batch 500 Loss 18.0186
  Batch 600 Loss 18.8931
  Batch 700 Loss 18.7326
Resetting 14928 PBs
Finished epoch 7 in 104.0 seconds
Perplexity training: 13.379
Measuring development set...
Recognition iteration 0 Loss 24.639
Recognition finished, iteration 100 Loss 10.290
Recognition iteration 0 Loss 23.646
Recognition finished, iteration 100 Loss 9.298
Recognition iteration 0 Loss 24.539
Recognition finished, iteration 100 Loss 10.188
Recognition iteration 0 Loss 24.715
Recognition finished, iteration 100 Loss 9.955
Perplexity dev: 8.347

==== Starting epoch 8 ====
  Batch 0 Loss 20.5882
  Batch 100 Loss 17.8462
  Batch 200 Loss 17.2517
  Batch 300 Loss 17.5766
  Batch 400 Loss 19.8084
  Batch 500 Loss 16.9437
  Batch 600 Loss 18.1714
  Batch 700 Loss 17.4450
Resetting 14847 PBs
Finished epoch 8 in 104.0 seconds
Perplexity training: 12.265

==== Starting epoch 9 ====
  Batch 0 Loss 20.3634
  Batch 100 Loss 18.0806
  Batch 200 Loss 15.9014
  Batch 300 Loss 17.1270
  Batch 400 Loss 18.6615
  Batch 500 Loss 16.4057
  Batch 600 Loss 18.1070
  Batch 700 Loss 17.4428
Resetting 15084 PBs
Finished epoch 9 in 105.0 seconds
Perplexity training: 11.423
Measuring development set...
Recognition iteration 0 Loss 23.984
Recognition finished, iteration 100 Loss 7.933
Recognition iteration 0 Loss 23.252
Recognition finished, iteration 100 Loss 6.933
Recognition iteration 0 Loss 24.183
Recognition finished, iteration 100 Loss 7.686
Recognition iteration 0 Loss 24.422
Recognition finished, iteration 100 Loss 7.527
Perplexity dev: 7.117

==== Starting epoch 10 ====
  Batch 0 Loss 20.1041
  Batch 100 Loss 17.7724
  Batch 200 Loss 15.0179
  Batch 300 Loss 15.9290
  Batch 400 Loss 18.1869
  Batch 500 Loss 15.7659
  Batch 600 Loss 16.2586
  Batch 700 Loss 16.4778
Resetting 14880 PBs
Finished epoch 10 in 105.0 seconds
Perplexity training: 10.807

==== Starting epoch 11 ====
  Batch 0 Loss 19.1076
  Batch 100 Loss 16.6228
  Batch 200 Loss 15.5304
  Batch 300 Loss 15.9727
  Batch 400 Loss 18.3807
  Batch 500 Loss 15.8497
  Batch 600 Loss 15.9330
  Batch 700 Loss 16.1414
Resetting 15102 PBs
Finished epoch 11 in 107.0 seconds
Perplexity training: 10.265
Measuring development set...
Recognition iteration 0 Loss 23.804
Recognition finished, iteration 100 Loss 6.051
Recognition iteration 0 Loss 22.731
Recognition finished, iteration 100 Loss 5.254
Recognition iteration 0 Loss 24.065
Recognition finished, iteration 100 Loss 5.805
Recognition iteration 0 Loss 24.243
Recognition finished, iteration 100 Loss 5.683
Perplexity dev: 6.104

==== Starting epoch 12 ====
  Batch 0 Loss 18.3498
  Batch 100 Loss 16.1545
  Batch 200 Loss 15.2698
  Batch 300 Loss 14.1680
  Batch 400 Loss 16.7408
  Batch 500 Loss 15.6991
  Batch 600 Loss 15.4317
  Batch 700 Loss 14.6756
Resetting 14943 PBs
Finished epoch 12 in 106.0 seconds
Perplexity training: 9.845

==== Starting epoch 13 ====
  Batch 0 Loss 17.1414
  Batch 100 Loss 15.7780
  Batch 200 Loss 13.8027
  Batch 300 Loss 13.3308
  Batch 400 Loss 16.6952
  Batch 500 Loss 14.5664
  Batch 600 Loss 14.6600
  Batch 700 Loss 15.0108
Resetting 14899 PBs
Finished epoch 13 in 107.0 seconds
Perplexity training: 9.460
Measuring development set...
Recognition iteration 0 Loss 23.990
Recognition finished, iteration 100 Loss 4.470
Recognition iteration 0 Loss 22.707
Recognition finished, iteration 100 Loss 3.901
Recognition iteration 0 Loss 23.806
Recognition finished, iteration 100 Loss 4.389
Recognition iteration 0 Loss 23.918
Recognition finished, iteration 100 Loss 4.255
Perplexity dev: 5.564

==== Starting epoch 14 ====
  Batch 0 Loss 17.3502
  Batch 100 Loss 14.7959
  Batch 200 Loss 12.7175
  Batch 300 Loss 13.1973
  Batch 400 Loss 17.3772
  Batch 500 Loss 14.3227
  Batch 600 Loss 16.1571
  Batch 700 Loss 14.1598
Resetting 15206 PBs
Finished epoch 14 in 108.0 seconds
Perplexity training: 9.039

==== Starting epoch 15 ====
  Batch 0 Loss 17.0650
  Batch 100 Loss 15.3713
  Batch 200 Loss 12.9045
  Batch 300 Loss 13.1588
  Batch 400 Loss 15.6689
  Batch 500 Loss 14.4939
  Batch 600 Loss 14.8537
  Batch 700 Loss 14.6420
Resetting 14914 PBs
Finished epoch 15 in 109.0 seconds
Perplexity training: 8.893
Measuring development set...
Recognition iteration 0 Loss 23.660
Recognition finished, iteration 100 Loss 3.383
Recognition iteration 0 Loss 22.405
Recognition finished, iteration 100 Loss 2.935
Recognition iteration 0 Loss 23.854
Recognition finished, iteration 100 Loss 3.412
Recognition iteration 0 Loss 23.640
Recognition finished, iteration 100 Loss 3.340
Perplexity dev: 5.146

==== Starting epoch 16 ====
  Batch 0 Loss 15.8022
  Batch 100 Loss 13.9091
  Batch 200 Loss 11.5008
  Batch 300 Loss 13.3014
  Batch 400 Loss 14.9863
  Batch 500 Loss 14.7581
  Batch 600 Loss 13.2622
  Batch 700 Loss 12.9255
Resetting 14984 PBs
Finished epoch 16 in 109.0 seconds
Perplexity training: 8.534

==== Starting epoch 17 ====
  Batch 0 Loss 14.5208
  Batch 100 Loss 14.1026
  Batch 200 Loss 13.2295
  Batch 300 Loss 13.5222
  Batch 400 Loss 15.0052
  Batch 500 Loss 13.1404
  Batch 600 Loss 13.6546
  Batch 700 Loss 14.2837
Resetting 14905 PBs
Finished epoch 17 in 109.0 seconds
Perplexity training: 8.296
Measuring development set...
Recognition iteration 0 Loss 23.487
Recognition finished, iteration 100 Loss 2.484
Recognition iteration 0 Loss 22.328
Recognition finished, iteration 100 Loss 2.083
Recognition iteration 0 Loss 23.699
Recognition finished, iteration 100 Loss 2.593
Recognition iteration 0 Loss 23.573
Recognition finished, iteration 100 Loss 2.405
Perplexity dev: 4.996

==== Starting epoch 18 ====
  Batch 0 Loss 16.6746
  Batch 100 Loss 14.9326
  Batch 200 Loss 12.6963
  Batch 300 Loss 12.4316
  Batch 400 Loss 14.1964
  Batch 500 Loss 12.3204
  Batch 600 Loss 12.7914
  Batch 700 Loss 14.4986
Resetting 15101 PBs
Finished epoch 18 in 112.0 seconds
Perplexity training: 8.140

==== Starting epoch 19 ====
  Batch 0 Loss 17.1873
  Batch 100 Loss 13.3219
  Batch 200 Loss 12.1814
  Batch 300 Loss 12.3102
  Batch 400 Loss 14.2255
  Batch 500 Loss 12.7567
  Batch 600 Loss 13.3919
  Batch 700 Loss 13.4259
Resetting 15050 PBs
Finished epoch 19 in 114.0 seconds
Perplexity training: 7.937
Measuring development set...
Recognition iteration 0 Loss 23.617
Recognition finished, iteration 100 Loss 1.973
Recognition iteration 0 Loss 22.328
Recognition finished, iteration 100 Loss 1.543
Recognition iteration 0 Loss 23.586
Recognition finished, iteration 100 Loss 1.962
Recognition iteration 0 Loss 23.160
Recognition finished, iteration 100 Loss 1.805
Perplexity dev: 4.596

==== Starting epoch 20 ====
  Batch 0 Loss 14.5426
  Batch 100 Loss 12.4801
  Batch 200 Loss 11.5405
  Batch 300 Loss 12.3216
  Batch 400 Loss 12.3326
  Batch 500 Loss 11.3063
  Batch 600 Loss 13.3435
  Batch 700 Loss 13.3759
Resetting 15210 PBs
Finished epoch 20 in 99.0 seconds
Perplexity training: 7.789

==== Starting epoch 21 ====
  Batch 0 Loss 15.7237
  Batch 100 Loss 11.9836
  Batch 200 Loss 11.6108
  Batch 300 Loss 12.7562
  Batch 400 Loss 12.8427
  Batch 500 Loss 11.8728
  Batch 600 Loss 11.5496
  Batch 700 Loss 11.3773
Resetting 14991 PBs
Finished epoch 21 in 93.0 seconds
Perplexity training: 7.664
Measuring development set...
Recognition iteration 0 Loss 23.268
Recognition finished, iteration 100 Loss 1.420
Recognition iteration 0 Loss 22.246
Recognition finished, iteration 100 Loss 1.159
Recognition iteration 0 Loss 23.395
Recognition finished, iteration 100 Loss 1.606
Recognition iteration 0 Loss 23.269
Recognition finished, iteration 100 Loss 1.429
Perplexity dev: 4.526

==== Starting epoch 22 ====
  Batch 0 Loss 15.6329
  Batch 100 Loss 12.9440
  Batch 200 Loss 12.2999
  Batch 300 Loss 11.2258
  Batch 400 Loss 13.0027
  Batch 500 Loss 12.0791
  Batch 600 Loss 12.4918
  Batch 700 Loss 10.0835
Resetting 14864 PBs
Finished epoch 22 in 92.0 seconds
Perplexity training: 7.503

==== Starting epoch 23 ====
  Batch 0 Loss 13.7797
  Batch 100 Loss 13.4394
  Batch 200 Loss 10.9528
  Batch 300 Loss 11.9437
  Batch 400 Loss 13.0631
  Batch 500 Loss 11.8265
  Batch 600 Loss 12.5144
  Batch 700 Loss 10.8424
Resetting 14868 PBs
Finished epoch 23 in 94.0 seconds
Perplexity training: 7.359
Measuring development set...
Recognition iteration 0 Loss 23.472
Recognition finished, iteration 100 Loss 1.134
Recognition iteration 0 Loss 22.254
Recognition finished, iteration 100 Loss 0.815
Recognition iteration 0 Loss 23.339
Recognition finished, iteration 100 Loss 1.229
Recognition iteration 0 Loss 23.239
Recognition finished, iteration 100 Loss 1.089
Perplexity dev: 4.560

==== Starting epoch 24 ====
  Batch 0 Loss 13.6121
  Batch 100 Loss 13.1602
  Batch 200 Loss 9.3029
  Batch 300 Loss 12.3227
  Batch 400 Loss 12.0497
  Batch 500 Loss 12.4335
  Batch 600 Loss 11.2735
  Batch 700 Loss 12.0430
Resetting 15034 PBs
Finished epoch 24 in 94.0 seconds
Perplexity training: 7.232

==== Starting epoch 25 ====
  Batch 0 Loss 13.7342
  Batch 100 Loss 12.2820
  Batch 200 Loss 10.8121
  Batch 300 Loss 11.9620
  Batch 400 Loss 11.6385
  Batch 500 Loss 11.5847
  Batch 600 Loss 11.6969
  Batch 700 Loss 12.2836
Resetting 15073 PBs
Finished epoch 25 in 95.0 seconds
Perplexity training: 7.185
Measuring development set...
Recognition iteration 0 Loss 23.594
Recognition finished, iteration 100 Loss 0.844
Recognition iteration 0 Loss 22.158
Recognition finished, iteration 100 Loss 0.630
Recognition iteration 0 Loss 23.378
Recognition finished, iteration 100 Loss 0.968
Recognition iteration 0 Loss 23.074
Recognition finished, iteration 100 Loss 0.929
Perplexity dev: 5.832

==== Starting epoch 26 ====
  Batch 0 Loss 14.3076
  Batch 100 Loss 12.1867
  Batch 200 Loss 11.0343
  Batch 300 Loss 11.1661
  Batch 400 Loss 12.5580
  Batch 500 Loss 12.2516
  Batch 600 Loss 11.7263
  Batch 700 Loss 11.8637
Resetting 15113 PBs
Finished epoch 26 in 95.0 seconds
Perplexity training: 7.085

==== Starting epoch 27 ====
  Batch 0 Loss 15.7358
  Batch 100 Loss 11.9189
  Batch 200 Loss 9.9597
  Batch 300 Loss 11.0787
  Batch 400 Loss 11.6648
  Batch 500 Loss 11.3874
  Batch 600 Loss 12.3542
  Batch 700 Loss 10.7444
Resetting 14990 PBs
Finished epoch 27 in 96.0 seconds
Perplexity training: 6.991
Measuring development set...
Recognition iteration 0 Loss 23.305
Recognition finished, iteration 100 Loss 0.668
Recognition iteration 0 Loss 22.200
Recognition finished, iteration 100 Loss 0.491
Recognition iteration 0 Loss 22.972
Recognition finished, iteration 100 Loss 0.853
Recognition iteration 0 Loss 23.201
Recognition finished, iteration 100 Loss 0.705
Perplexity dev: 3.870

==== Starting epoch 28 ====
  Batch 0 Loss 13.3860
  Batch 100 Loss 10.3532
  Batch 200 Loss 10.7967
  Batch 300 Loss 10.3178
  Batch 400 Loss 12.8171
  Batch 500 Loss 10.8804
  Batch 600 Loss 12.8176
  Batch 700 Loss 10.7909
Resetting 14967 PBs
Finished epoch 28 in 97.0 seconds
Perplexity training: 6.918

==== Starting epoch 29 ====
  Batch 0 Loss 12.0630
  Batch 100 Loss 10.7015
  Batch 200 Loss 9.5847
  Batch 300 Loss 9.6421
  Batch 400 Loss 12.5470
  Batch 500 Loss 10.1777
  Batch 600 Loss 12.3870
  Batch 700 Loss 8.5938
Resetting 14829 PBs
Finished epoch 29 in 98.0 seconds
Perplexity training: 6.775
Measuring development set...
Recognition iteration 0 Loss 23.335
Recognition finished, iteration 100 Loss 0.515
Recognition iteration 0 Loss 22.053
Recognition finished, iteration 100 Loss 0.400
Recognition iteration 0 Loss 23.276
Recognition finished, iteration 100 Loss 0.708
Recognition iteration 0 Loss 22.748
Recognition finished, iteration 100 Loss 0.552
Perplexity dev: 3.741

==== Starting epoch 30 ====
  Batch 0 Loss 12.8425
  Batch 100 Loss 12.3949
  Batch 200 Loss 9.3076
  Batch 300 Loss 9.8591
  Batch 400 Loss 11.6933
  Batch 500 Loss 10.0688
  Batch 600 Loss 11.0959
  Batch 700 Loss 10.5258
Resetting 15098 PBs
Finished epoch 30 in 99.0 seconds
Perplexity training: 6.656

==== Starting epoch 31 ====
  Batch 0 Loss 12.2211
  Batch 100 Loss 9.8074
  Batch 200 Loss 9.4901
  Batch 300 Loss 10.1159
  Batch 400 Loss 11.4388
  Batch 500 Loss 10.5858
  Batch 600 Loss 10.2025
  Batch 700 Loss 10.6591
Resetting 14821 PBs
Finished epoch 31 in 98.0 seconds
Perplexity training: 6.629
Measuring development set...
Recognition iteration 0 Loss 23.165
Recognition finished, iteration 100 Loss 0.438
Recognition iteration 0 Loss 22.138
Recognition finished, iteration 100 Loss 0.288
Recognition iteration 0 Loss 23.047
Recognition finished, iteration 100 Loss 0.561
Recognition iteration 0 Loss 22.795
Recognition finished, iteration 100 Loss 0.456
Perplexity dev: 4.003

==== Starting epoch 32 ====
  Batch 0 Loss 12.5528
  Batch 100 Loss 9.3590
  Batch 200 Loss 10.2346
  Batch 300 Loss 10.8922
  Batch 400 Loss 12.0488
  Batch 500 Loss 8.0382
  Batch 600 Loss 10.9139
  Batch 700 Loss 9.0227
Resetting 15126 PBs
Finished epoch 32 in 98.0 seconds
Perplexity training: 6.525

==== Starting epoch 33 ====
  Batch 0 Loss 13.2987
  Batch 100 Loss 10.2795
  Batch 200 Loss 9.4099
  Batch 300 Loss 9.0067
  Batch 400 Loss 11.2777
  Batch 500 Loss 9.0572
  Batch 600 Loss 10.0917
  Batch 700 Loss 9.8965
Resetting 14916 PBs
Finished epoch 33 in 100.0 seconds
Perplexity training: 6.563
Measuring development set...
Recognition iteration 0 Loss 23.096
Recognition finished, iteration 100 Loss 0.359
Recognition iteration 0 Loss 22.100
Recognition finished, iteration 100 Loss 0.274
Recognition iteration 0 Loss 23.251
Recognition finished, iteration 100 Loss 0.486
Recognition iteration 0 Loss 22.832
Recognition finished, iteration 100 Loss 0.419
Perplexity dev: 3.994

==== Starting epoch 34 ====
  Batch 0 Loss 12.1424
  Batch 100 Loss 9.1624
  Batch 200 Loss 10.6045
  Batch 300 Loss 10.1302
  Batch 400 Loss 13.2827
  Batch 500 Loss 9.3264
  Batch 600 Loss 11.0255
  Batch 700 Loss 9.3617
Resetting 15134 PBs
Finished epoch 34 in 100.0 seconds
Perplexity training: 6.430

==== Starting epoch 35 ====
  Batch 0 Loss 12.5048
  Batch 100 Loss 9.3282
  Batch 200 Loss 9.6373
  Batch 300 Loss 9.8755
  Batch 400 Loss 9.7623
  Batch 500 Loss 10.1731
  Batch 600 Loss 10.9398
  Batch 700 Loss 9.9616
Resetting 15000 PBs
Finished epoch 35 in 102.0 seconds
Perplexity training: 6.451
Measuring development set...
Recognition iteration 0 Loss 23.182
Recognition finished, iteration 100 Loss 0.287
Recognition iteration 0 Loss 22.158
Recognition finished, iteration 100 Loss 0.223
Recognition iteration 0 Loss 23.331
Recognition finished, iteration 100 Loss 0.393
Recognition iteration 0 Loss 23.020
Recognition finished, iteration 100 Loss 0.290
Perplexity dev: 3.820

==== Starting epoch 36 ====
  Batch 0 Loss 10.2405
  Batch 100 Loss 8.5431
  Batch 200 Loss 9.6924
  Batch 300 Loss 11.3884
  Batch 400 Loss 11.7464
  Batch 500 Loss 9.4452
  Batch 600 Loss 9.8345
  Batch 700 Loss 10.2223
Resetting 15075 PBs
Finished epoch 36 in 119.0 seconds
Perplexity training: 6.388

==== Starting epoch 37 ====
  Batch 0 Loss 12.2061
  Batch 100 Loss 9.2742
  Batch 200 Loss 9.4504
  Batch 300 Loss 9.6678
  Batch 400 Loss 11.3790
  Batch 500 Loss 8.6244
  Batch 600 Loss 8.5303
  Batch 700 Loss 9.8773
Resetting 14792 PBs
Finished epoch 37 in 116.0 seconds
Perplexity training: 6.382
Measuring development set...
Recognition iteration 0 Loss 22.942
Recognition finished, iteration 100 Loss 0.237
Recognition iteration 0 Loss 22.272
Recognition finished, iteration 100 Loss 0.164
Recognition iteration 0 Loss 23.318
Recognition finished, iteration 100 Loss 0.378
Recognition iteration 0 Loss 22.717
Recognition finished, iteration 93 Loss 0.327
Perplexity dev: 4.200

==== Starting epoch 38 ====
  Batch 0 Loss 13.2225
  Batch 100 Loss 8.3754
  Batch 200 Loss 8.8803
  Batch 300 Loss 8.5396
  Batch 400 Loss 11.0711
  Batch 500 Loss 7.6823
  Batch 600 Loss 10.7851
  Batch 700 Loss 7.7577
Resetting 15139 PBs
Finished epoch 38 in 113.0 seconds
Perplexity training: 6.235

==== Starting epoch 39 ====
  Batch 0 Loss 11.6692
  Batch 100 Loss 10.7531
  Batch 200 Loss 8.0341
  Batch 300 Loss 9.1971
  Batch 400 Loss 9.7856
  Batch 500 Loss 9.5016
  Batch 600 Loss 9.9605
  Batch 700 Loss 9.2131
Resetting 15210 PBs
Finished epoch 39 in 113.0 seconds
Perplexity training: 6.265
Measuring development set...
Recognition iteration 0 Loss 22.888
Recognition finished, iteration 100 Loss 0.193
Recognition iteration 0 Loss 22.098
Recognition finished, iteration 100 Loss 0.152
Recognition iteration 0 Loss 23.116
Recognition finished, iteration 100 Loss 0.313
Recognition iteration 0 Loss 22.775
Recognition finished, iteration 100 Loss 0.242
Perplexity dev: 3.220

==== Starting epoch 40 ====
  Batch 0 Loss 10.8329
  Batch 100 Loss 9.1533
  Batch 200 Loss 9.1553
  Batch 300 Loss 11.4679
  Batch 400 Loss 11.4306
  Batch 500 Loss 8.9175
  Batch 600 Loss 9.3917
  Batch 700 Loss 8.8205
Resetting 15109 PBs
Finished epoch 40 in 113.0 seconds
Perplexity training: 6.221

==== Starting epoch 41 ====
  Batch 0 Loss 12.6773
  Batch 100 Loss 10.0471
  Batch 200 Loss 7.7344
  Batch 300 Loss 8.5718
  Batch 400 Loss 10.1677
  Batch 500 Loss 9.6021
  Batch 600 Loss 10.0461
  Batch 700 Loss 9.1476
Resetting 15008 PBs
Finished epoch 41 in 113.0 seconds
Perplexity training: 6.110
Measuring development set...
Recognition iteration 0 Loss 22.830
Recognition finished, iteration 100 Loss 0.176
Recognition iteration 0 Loss 21.841
Recognition finished, iteration 100 Loss 0.123
Recognition iteration 0 Loss 22.960
Recognition finished, iteration 100 Loss 0.265
Recognition iteration 0 Loss 22.821
Recognition finished, iteration 100 Loss 0.204
Perplexity dev: 4.052

==== Starting epoch 42 ====
  Batch 0 Loss 10.0917
  Batch 100 Loss 8.9598
  Batch 200 Loss 9.0186
  Batch 300 Loss 8.1432
  Batch 400 Loss 10.7285
  Batch 500 Loss 8.8205
  Batch 600 Loss 10.8428
  Batch 700 Loss 8.4856
Resetting 14865 PBs
Finished epoch 42 in 104.0 seconds
Perplexity training: 6.098

==== Starting epoch 43 ====
  Batch 0 Loss 12.8364
  Batch 100 Loss 9.0215
  Batch 200 Loss 10.0680
  Batch 300 Loss 7.9243
  Batch 400 Loss 12.2294
  Batch 500 Loss 9.4750
  Batch 600 Loss 12.9720
  Batch 700 Loss 8.4765
Resetting 14806 PBs
Finished epoch 43 in 104.0 seconds
Perplexity training: 6.076
Measuring development set...
Recognition iteration 0 Loss 22.780
Recognition finished, iteration 100 Loss 0.151
Recognition iteration 0 Loss 21.998
Recognition finished, iteration 100 Loss 0.104
Recognition iteration 0 Loss 23.031
Recognition finished, iteration 100 Loss 0.254
Recognition iteration 0 Loss 22.760
Recognition finished, iteration 100 Loss 0.224
Perplexity dev: 2.882

==== Starting epoch 44 ====
  Batch 0 Loss 12.5742
  Batch 100 Loss 9.1422
  Batch 200 Loss 8.2713
  Batch 300 Loss 9.0326
  Batch 400 Loss 9.2074
  Batch 500 Loss 8.9788
  Batch 600 Loss 11.3169
  Batch 700 Loss 9.1671
Resetting 14984 PBs
Finished epoch 44 in 109.0 seconds
Perplexity training: 5.986

==== Starting epoch 45 ====
  Batch 0 Loss 10.6152
  Batch 100 Loss 9.6896
  Batch 200 Loss 9.7077
  Batch 300 Loss 10.0761
  Batch 400 Loss 9.8529
  Batch 500 Loss 8.6961
  Batch 600 Loss 10.6353
  Batch 700 Loss 8.3524
Resetting 15033 PBs
Finished epoch 45 in 124.0 seconds
Perplexity training: 5.985
Measuring development set...
Recognition iteration 0 Loss 22.695
Recognition finished, iteration 100 Loss 0.127
Recognition iteration 0 Loss 21.856
Recognition finished, iteration 100 Loss 0.091
Recognition iteration 0 Loss 23.127
Recognition finished, iteration 100 Loss 0.168
Recognition iteration 0 Loss 22.881
Recognition finished, iteration 100 Loss 0.147
Perplexity dev: 2.940

==== Starting epoch 46 ====
  Batch 0 Loss 9.4792
  Batch 100 Loss 10.4039
  Batch 200 Loss 7.8487
  Batch 300 Loss 7.7756
  Batch 400 Loss 9.3167
  Batch 500 Loss 9.5491
  Batch 600 Loss 9.9518
  Batch 700 Loss 7.6332
Resetting 14912 PBs
Finished epoch 46 in 122.0 seconds
Perplexity training: 5.919

==== Starting epoch 47 ====
  Batch 0 Loss 11.2904
  Batch 100 Loss 9.7645
  Batch 200 Loss 7.9129
  Batch 300 Loss 7.6307
  Batch 400 Loss 10.0239
  Batch 500 Loss 11.2853
  Batch 600 Loss 8.1180
  Batch 700 Loss 9.6766
Resetting 15059 PBs
Finished epoch 47 in 124.0 seconds
Perplexity training: 5.944
Measuring development set...
Recognition iteration 0 Loss 22.605
Recognition finished, iteration 100 Loss 0.114
Recognition iteration 0 Loss 21.663
Recognition finished, iteration 100 Loss 0.088
Recognition iteration 0 Loss 23.183
Recognition finished, iteration 100 Loss 0.165
Recognition iteration 0 Loss 22.807
Recognition finished, iteration 100 Loss 0.157
Perplexity dev: 3.473

==== Starting epoch 48 ====
  Batch 0 Loss 12.2437
  Batch 100 Loss 9.1615
  Batch 200 Loss 8.2001
  Batch 300 Loss 6.8047
  Batch 400 Loss 10.0663
  Batch 500 Loss 8.0879
  Batch 600 Loss 11.6935
  Batch 700 Loss 7.3017
Resetting 15161 PBs
Finished epoch 48 in 131.0 seconds
Perplexity training: 5.881

==== Starting epoch 49 ====
  Batch 0 Loss 11.5454
  Batch 100 Loss 9.1552
  Batch 200 Loss 8.1370
  Batch 300 Loss 8.8219
  Batch 400 Loss 8.9999
  Batch 500 Loss 7.9396
  Batch 600 Loss 9.3820
  Batch 700 Loss 8.8269
Resetting 14996 PBs
Finished epoch 49 in 134.0 seconds
Perplexity training: 5.885
Measuring development set...
Recognition iteration 0 Loss 22.742
Recognition finished, iteration 100 Loss 0.120
Recognition iteration 0 Loss 21.758
Recognition finished, iteration 100 Loss 0.077
Recognition iteration 0 Loss 23.211
Recognition finished, iteration 100 Loss 0.148
Recognition iteration 0 Loss 22.796
Recognition finished, iteration 100 Loss 0.158
Perplexity dev: 4.581

==== Starting epoch 50 ====
  Batch 0 Loss 10.3282
  Batch 100 Loss 8.3868
  Batch 200 Loss 8.6356
  Batch 300 Loss 7.6081
  Batch 400 Loss 9.8885
  Batch 500 Loss 7.8759
  Batch 600 Loss 10.1959
  Batch 700 Loss 8.7229
Resetting 15038 PBs
Finished epoch 50 in 155.0 seconds
Perplexity training: 5.872

==== Starting epoch 51 ====
  Batch 0 Loss 11.9080
  Batch 100 Loss 8.3573
  Batch 200 Loss 7.9061
  Batch 300 Loss 10.3823
  Batch 400 Loss 10.1564
  Batch 500 Loss 9.1132
  Batch 600 Loss 8.5506
  Batch 700 Loss 8.4111
Resetting 14977 PBs
Finished epoch 51 in 143.0 seconds
Perplexity training: 6.034
Measuring development set...
Recognition iteration 0 Loss 22.884
Recognition finished, iteration 100 Loss 0.103
Recognition iteration 0 Loss 21.784
Recognition finished, iteration 100 Loss 0.071
Recognition iteration 0 Loss 23.011
Recognition finished, iteration 100 Loss 0.135
Recognition iteration 0 Loss 22.880
Recognition finished, iteration 100 Loss 0.134
Perplexity dev: 3.832

==== Starting epoch 52 ====
  Batch 0 Loss 8.5614
  Batch 100 Loss 9.7645
  Batch 200 Loss 7.6874
  Batch 300 Loss 8.8812
  Batch 400 Loss 10.7091
  Batch 500 Loss 9.7419
  Batch 600 Loss 9.3329
  Batch 700 Loss 7.6160
Resetting 14907 PBs
Finished epoch 52 in 145.0 seconds
Perplexity training: 5.824

==== Starting epoch 53 ====
  Batch 0 Loss 10.2361
  Batch 100 Loss 8.3723
  Batch 200 Loss 6.0949
  Batch 300 Loss 8.0245
  Batch 400 Loss 11.4718
  Batch 500 Loss 8.2506
  Batch 600 Loss 7.4798
  Batch 700 Loss 9.8243
Resetting 15013 PBs
Finished epoch 53 in 146.0 seconds
Perplexity training: 5.766
Measuring development set...
Recognition iteration 0 Loss 23.106
Recognition finished, iteration 100 Loss 0.085
Recognition iteration 0 Loss 21.741
Recognition finished, iteration 100 Loss 0.067
Recognition iteration 0 Loss 22.875
Recognition finished, iteration 100 Loss 0.105
Recognition iteration 0 Loss 22.649
Recognition finished, iteration 100 Loss 0.090
Perplexity dev: 2.943

==== Starting epoch 54 ====
  Batch 0 Loss 9.2327
  Batch 100 Loss 8.0029
  Batch 200 Loss 9.6515
  Batch 300 Loss 8.8720
  Batch 400 Loss 9.1975
  Batch 500 Loss 7.5566
  Batch 600 Loss 9.0373
  Batch 700 Loss 6.7136
Resetting 15127 PBs
Finished epoch 54 in 145.0 seconds
Perplexity training: 5.723

==== Starting epoch 55 ====
  Batch 0 Loss 9.6403
  Batch 100 Loss 10.3473
  Batch 200 Loss 7.2059
  Batch 300 Loss 11.3572
  Batch 400 Loss 8.3280
  Batch 500 Loss 7.4288
  Batch 600 Loss 7.8164
  Batch 700 Loss 8.3405
Resetting 15017 PBs
Finished epoch 55 in 144.0 seconds
Perplexity training: 5.727
Measuring development set...
Recognition iteration 0 Loss 22.768
Recognition finished, iteration 100 Loss 0.084
Recognition iteration 0 Loss 21.797
Recognition finished, iteration 100 Loss 0.057
Recognition iteration 0 Loss 22.894
Recognition finished, iteration 100 Loss 0.090
Recognition iteration 0 Loss 22.478
Recognition finished, iteration 100 Loss 0.094
Perplexity dev: 2.846

==== Starting epoch 56 ====
  Batch 0 Loss 8.4576
  Batch 100 Loss 9.0519
  Batch 200 Loss 7.3929
  Batch 300 Loss 9.4084
  Batch 400 Loss 9.4453
  Batch 500 Loss 6.5302
  Batch 600 Loss 7.2181
  Batch 700 Loss 7.4962
Resetting 14857 PBs
Finished epoch 56 in 147.0 seconds
Perplexity training: 5.708

==== Starting epoch 57 ====
  Batch 0 Loss 9.3474
  Batch 100 Loss 8.4514
  Batch 200 Loss 8.0329
  Batch 300 Loss 8.7399
  Batch 400 Loss 8.5740
  Batch 500 Loss 8.4287
  Batch 600 Loss 8.5179
  Batch 700 Loss 9.9468
Resetting 15067 PBs
Finished epoch 57 in 148.0 seconds
Perplexity training: 5.635
Measuring development set...
Recognition iteration 0 Loss 22.709
Recognition finished, iteration 100 Loss 0.073
Recognition iteration 0 Loss 21.865
Recognition finished, iteration 100 Loss 0.054
Recognition iteration 0 Loss 23.046
Recognition finished, iteration 100 Loss 0.092
Recognition iteration 0 Loss 22.605
Recognition finished, iteration 100 Loss 0.084
Perplexity dev: 3.424

==== Starting epoch 58 ====
  Batch 0 Loss 11.0934
  Batch 100 Loss 8.8313
  Batch 200 Loss 9.9011
  Batch 300 Loss 8.6022
  Batch 400 Loss 11.4756
  Batch 500 Loss 7.4860
  Batch 600 Loss 8.9955
  Batch 700 Loss 9.1551
Resetting 15159 PBs
Finished epoch 58 in 147.0 seconds
Perplexity training: 5.609

==== Starting epoch 59 ====
  Batch 0 Loss 11.0208
  Batch 100 Loss 10.9328
  Batch 200 Loss 6.9788
  Batch 300 Loss 10.6013
  Batch 400 Loss 8.7572
  Batch 500 Loss 7.4052
  Batch 600 Loss 8.1564
  Batch 700 Loss 8.5925
Resetting 14893 PBs
Finished epoch 59 in 148.0 seconds
Perplexity training: 5.648
Measuring development set...
Recognition iteration 0 Loss 22.959
Recognition finished, iteration 100 Loss 0.071
Recognition iteration 0 Loss 22.119
Recognition finished, iteration 100 Loss 0.050
Recognition iteration 0 Loss 22.937
Recognition finished, iteration 100 Loss 0.076
Recognition iteration 0 Loss 22.554
Recognition finished, iteration 100 Loss 0.078
Perplexity dev: 3.064

==== Starting epoch 60 ====
  Batch 0 Loss 10.1575
  Batch 100 Loss 8.6517
  Batch 200 Loss 7.9262
  Batch 300 Loss 9.0019
  Batch 400 Loss 7.9860
  Batch 500 Loss 8.9505
  Batch 600 Loss 8.7987
  Batch 700 Loss 7.8730
Resetting 15014 PBs
Finished epoch 60 in 148.0 seconds
Perplexity training: 5.518

==== Starting epoch 61 ====
  Batch 0 Loss 10.7104
  Batch 100 Loss 9.0385
  Batch 200 Loss 7.4312
  Batch 300 Loss 7.4291
  Batch 400 Loss 7.9276
  Batch 500 Loss 8.0438
  Batch 600 Loss 7.5914
  Batch 700 Loss 9.0910
Resetting 15024 PBs
Finished epoch 61 in 149.0 seconds
Perplexity training: 5.530
Measuring development set...
Recognition iteration 0 Loss 22.923
Recognition finished, iteration 100 Loss 0.062
Recognition iteration 0 Loss 21.999
Recognition finished, iteration 100 Loss 0.049
Recognition iteration 0 Loss 23.163
Recognition finished, iteration 100 Loss 0.073
Recognition iteration 0 Loss 22.550
Recognition finished, iteration 100 Loss 0.067
Perplexity dev: 2.845

==== Starting epoch 62 ====
  Batch 0 Loss 9.6895
  Batch 100 Loss 9.3023
  Batch 200 Loss 8.8817
  Batch 300 Loss 8.6328
  Batch 400 Loss 8.4713
  Batch 500 Loss 7.9066
  Batch 600 Loss 7.7060
  Batch 700 Loss 7.6401
Resetting 14958 PBs
Finished epoch 62 in 151.0 seconds
Perplexity training: 5.512

==== Starting epoch 63 ====
  Batch 0 Loss 10.0752
  Batch 100 Loss 8.2647
  Batch 200 Loss 8.7816
  Batch 300 Loss 8.0266
  Batch 400 Loss 8.1034
  Batch 500 Loss 9.0860
  Batch 600 Loss 8.7309
  Batch 700 Loss 9.2535
Resetting 15058 PBs
Finished epoch 63 in 152.0 seconds
Perplexity training: 5.469
Measuring development set...
Recognition iteration 0 Loss 23.011
Recognition finished, iteration 100 Loss 0.058
Recognition iteration 0 Loss 21.759
Recognition finished, iteration 100 Loss 0.044
Recognition iteration 0 Loss 23.213
Recognition finished, iteration 100 Loss 0.067
Recognition iteration 0 Loss 22.543
Recognition finished, iteration 100 Loss 0.065
Perplexity dev: 3.225

==== Starting epoch 64 ====
  Batch 0 Loss 10.6287
  Batch 100 Loss 9.1881
  Batch 200 Loss 7.1195
  Batch 300 Loss 8.2633
  Batch 400 Loss 9.2607
  Batch 500 Loss 9.4665
  Batch 600 Loss 7.8043
  Batch 700 Loss 6.1278
Resetting 15195 PBs
Finished epoch 64 in 152.0 seconds
Perplexity training: 5.459

==== Starting epoch 65 ====
  Batch 0 Loss 9.4206
  Batch 100 Loss 7.8614
  Batch 200 Loss 8.5617
  Batch 300 Loss 8.9257
  Batch 400 Loss 10.4344
  Batch 500 Loss 8.4239
  Batch 600 Loss 6.0313
  Batch 700 Loss 8.3128
Resetting 15187 PBs
Finished epoch 65 in 153.0 seconds
Perplexity training: 5.466
Measuring development set...
Recognition iteration 0 Loss 22.876
Recognition finished, iteration 100 Loss 0.052
Recognition iteration 0 Loss 22.092
Recognition finished, iteration 100 Loss 0.041
Recognition iteration 0 Loss 22.960
Recognition finished, iteration 100 Loss 0.066
Recognition iteration 0 Loss 22.367
Recognition finished, iteration 100 Loss 0.051
Perplexity dev: 3.372

==== Starting epoch 66 ====
  Batch 0 Loss 8.2987
  Batch 100 Loss 9.4235
  Batch 200 Loss 8.6185
  Batch 300 Loss 8.6906
  Batch 400 Loss 11.2367
  Batch 500 Loss 8.2036
  Batch 600 Loss 6.1452
  Batch 700 Loss 7.2454
Resetting 15041 PBs
Finished epoch 66 in 154.0 seconds
Perplexity training: 5.504

==== Starting epoch 67 ====
  Batch 0 Loss 12.1446
  Batch 100 Loss 8.3620
  Batch 200 Loss 7.8519
  Batch 300 Loss 7.0466
  Batch 400 Loss 9.7120
  Batch 500 Loss 7.4086
  Batch 600 Loss 6.7958
  Batch 700 Loss 7.5339
Resetting 15065 PBs
Finished epoch 67 in 154.0 seconds
Perplexity training: 5.430
Measuring development set...
Recognition iteration 0 Loss 22.864
Recognition finished, iteration 100 Loss 0.053
Recognition iteration 0 Loss 21.652
Recognition finished, iteration 100 Loss 0.038
Recognition iteration 0 Loss 23.108
Recognition finished, iteration 100 Loss 0.058
Recognition iteration 0 Loss 22.535
Recognition finished, iteration 100 Loss 0.050
Perplexity dev: 2.613

==== Starting epoch 68 ====
  Batch 0 Loss 9.9079
  Batch 100 Loss 8.6842
  Batch 200 Loss 7.0132
  Batch 300 Loss 5.2447
  Batch 400 Loss 9.0136
  Batch 500 Loss 7.4333
  Batch 600 Loss 8.1400
  Batch 700 Loss 9.2289
Resetting 14975 PBs
Finished epoch 68 in 156.0 seconds
Perplexity training: 5.420

==== Starting epoch 69 ====
  Batch 0 Loss 8.7867
  Batch 100 Loss 8.3350
  Batch 200 Loss 7.2439
  Batch 300 Loss 8.2777
  Batch 400 Loss 7.6183
  Batch 500 Loss 8.4452
  Batch 600 Loss 6.8945
  Batch 700 Loss 7.7459
Resetting 14958 PBs
Finished epoch 69 in 159.0 seconds
Perplexity training: 5.355
Measuring development set...
Recognition iteration 0 Loss 22.856
Recognition finished, iteration 100 Loss 0.051
Recognition iteration 0 Loss 21.888
Recognition finished, iteration 100 Loss 0.035
Recognition iteration 0 Loss 22.886
Recognition finished, iteration 100 Loss 0.059
Recognition iteration 0 Loss 22.393
Recognition finished, iteration 100 Loss 0.046
Perplexity dev: 2.514

==== Starting epoch 70 ====
  Batch 0 Loss 9.7605
  Batch 100 Loss 8.6402
  Batch 200 Loss 6.5348
  Batch 300 Loss 9.7791
  Batch 400 Loss 9.6961
  Batch 500 Loss 7.0888
  Batch 600 Loss 7.8098
  Batch 700 Loss 8.3303
Resetting 15052 PBs
Finished epoch 70 in 155.0 seconds
Perplexity training: 5.364

==== Starting epoch 71 ====
  Batch 0 Loss 11.5724
  Batch 100 Loss 8.7132
  Batch 200 Loss 7.9564
  Batch 300 Loss 8.8763
  Batch 400 Loss 10.4580
  Batch 500 Loss 8.2486
  Batch 600 Loss 9.2197
  Batch 700 Loss 7.3492
Resetting 14916 PBs
Finished epoch 71 in 157.0 seconds
Perplexity training: 5.359
Measuring development set...
Recognition iteration 0 Loss 22.777
Recognition finished, iteration 100 Loss 0.048
Recognition iteration 0 Loss 21.567
Recognition finished, iteration 100 Loss 0.031
Recognition iteration 0 Loss 23.053
Recognition finished, iteration 100 Loss 0.059
Recognition iteration 0 Loss 22.407
Recognition finished, iteration 100 Loss 0.042
Perplexity dev: 2.409

==== Starting epoch 72 ====
  Batch 0 Loss 9.0418
  Batch 100 Loss 9.8017
  Batch 200 Loss 7.7218
  Batch 300 Loss 8.1097
  Batch 400 Loss 6.3638
  Batch 500 Loss 7.5689
  Batch 600 Loss 8.6242
  Batch 700 Loss 7.2716
Resetting 15065 PBs
Finished epoch 72 in 158.0 seconds
Perplexity training: 5.271

==== Starting epoch 73 ====
  Batch 0 Loss 10.0320
  Batch 100 Loss 9.1744
  Batch 200 Loss 6.6130
  Batch 300 Loss 7.9872
  Batch 400 Loss 8.3024
  Batch 500 Loss 7.9193
  Batch 600 Loss 8.1288
  Batch 700 Loss 8.2101
Resetting 15110 PBs
Finished epoch 73 in 157.0 seconds
Perplexity training: 5.336
Measuring development set...
Recognition iteration 0 Loss 22.850
Recognition finished, iteration 100 Loss 0.039
Recognition iteration 0 Loss 21.844
Recognition finished, iteration 100 Loss 0.033
Recognition iteration 0 Loss 23.167
Recognition finished, iteration 100 Loss 0.058
Recognition iteration 0 Loss 22.544
Recognition finished, iteration 100 Loss 0.045
Perplexity dev: 2.650

==== Starting epoch 74 ====
  Batch 0 Loss 10.7428
  Batch 100 Loss 8.1301
  Batch 200 Loss 8.1035
  Batch 300 Loss 9.2344
  Batch 400 Loss 8.1983
  Batch 500 Loss 8.8911
  Batch 600 Loss 9.0673
  Batch 700 Loss 10.5647
Resetting 15124 PBs
Finished epoch 74 in 159.0 seconds
Perplexity training: 5.331

==== Starting epoch 75 ====
  Batch 0 Loss 9.5453
  Batch 100 Loss 8.4691
  Batch 200 Loss 7.1528
  Batch 300 Loss 7.1113
  Batch 400 Loss 9.9723
  Batch 500 Loss 6.4521
  Batch 600 Loss 9.1103
  Batch 700 Loss 6.7764
Resetting 15009 PBs
Finished epoch 75 in 160.0 seconds
Perplexity training: 5.353
Measuring development set...
Recognition iteration 0 Loss 22.627
Recognition finished, iteration 100 Loss 0.039
Recognition iteration 0 Loss 21.939
Recognition finished, iteration 100 Loss 0.033
Recognition iteration 0 Loss 22.994
Recognition finished, iteration 100 Loss 0.058
Recognition iteration 0 Loss 22.591
Recognition finished, iteration 100 Loss 0.041
Perplexity dev: 2.333

==== Starting epoch 76 ====
  Batch 0 Loss 10.3755
  Batch 100 Loss 7.9170
  Batch 200 Loss 7.2779
  Batch 300 Loss 8.0465
  Batch 400 Loss 8.9566
  Batch 500 Loss 6.3140
  Batch 600 Loss 8.3198
  Batch 700 Loss 8.6841
Resetting 15125 PBs
Finished epoch 76 in 162.0 seconds
Perplexity training: 5.253

==== Starting epoch 77 ====
  Batch 0 Loss 9.5942
  Batch 100 Loss 8.2819
  Batch 200 Loss 7.0826
  Batch 300 Loss 7.8005
  Batch 400 Loss 8.7857
  Batch 500 Loss 7.5597
  Batch 600 Loss 8.3039
  Batch 700 Loss 9.0060
Resetting 14951 PBs
Finished epoch 77 in 162.0 seconds
Perplexity training: 5.274
Measuring development set...
Recognition iteration 0 Loss 22.877
Recognition finished, iteration 100 Loss 0.037
Recognition iteration 0 Loss 21.920
Recognition finished, iteration 100 Loss 0.032
Recognition iteration 0 Loss 23.159
Recognition finished, iteration 100 Loss 0.049
Recognition iteration 0 Loss 22.757
Recognition finished, iteration 100 Loss 0.039
Perplexity dev: 2.404

==== Starting epoch 78 ====
  Batch 0 Loss 9.1071
  Batch 100 Loss 8.1614
  Batch 200 Loss 8.5596
  Batch 300 Loss 8.4103
  Batch 400 Loss 9.2648
  Batch 500 Loss 7.9499
  Batch 600 Loss 6.6851
  Batch 700 Loss 7.5909
Resetting 15146 PBs
Finished epoch 78 in 162.0 seconds
Perplexity training: 5.242

==== Starting epoch 79 ====
  Batch 0 Loss 8.0918
  Batch 100 Loss 7.7739
  Batch 200 Loss 7.2386
  Batch 300 Loss 9.5847
  Batch 400 Loss 8.7673
  Batch 500 Loss 8.0915
  Batch 600 Loss 9.9298
  Batch 700 Loss 7.6897
Resetting 14785 PBs
Finished epoch 79 in 162.0 seconds
Perplexity training: 5.240
Measuring development set...
Recognition iteration 0 Loss 22.495
Recognition finished, iteration 100 Loss 0.035
Recognition iteration 0 Loss 21.643
Recognition finished, iteration 100 Loss 0.030
Recognition iteration 0 Loss 23.244
Recognition finished, iteration 100 Loss 0.056
Recognition iteration 0 Loss 22.676
Recognition finished, iteration 100 Loss 0.035
Perplexity dev: 2.502

==== Starting epoch 80 ====
  Batch 0 Loss 10.4025
  Batch 100 Loss 8.2013
  Batch 200 Loss 7.3129
  Batch 300 Loss 8.8319
  Batch 400 Loss 9.3903
  Batch 500 Loss 6.9380
  Batch 600 Loss 8.2783
  Batch 700 Loss 5.5715
Resetting 15096 PBs
Finished epoch 80 in 162.0 seconds
Perplexity training: 5.111

==== Starting epoch 81 ====
  Batch 0 Loss 12.2996
  Batch 100 Loss 9.2866
  Batch 200 Loss 8.5463
  Batch 300 Loss 7.4361
  Batch 400 Loss 9.6672
  Batch 500 Loss 7.6623
  Batch 600 Loss 7.7803
  Batch 700 Loss 6.7915
Resetting 14832 PBs
Finished epoch 81 in 163.0 seconds
Perplexity training: 5.254
Measuring development set...
Recognition iteration 0 Loss 22.724
Recognition finished, iteration 100 Loss 0.036
Recognition iteration 0 Loss 21.664
Recognition finished, iteration 100 Loss 0.026
Recognition iteration 0 Loss 23.125
Recognition finished, iteration 100 Loss 0.044
Recognition iteration 0 Loss 22.394
Recognition finished, iteration 100 Loss 0.039
Perplexity dev: 2.580

==== Starting epoch 82 ====
  Batch 0 Loss 9.4328
  Batch 100 Loss 7.7231
  Batch 200 Loss 6.0869
  Batch 300 Loss 6.0209
  Batch 400 Loss 7.4120
  Batch 500 Loss 8.1154
  Batch 600 Loss 7.7559
  Batch 700 Loss 7.1855
Resetting 14904 PBs
Finished epoch 82 in 164.0 seconds
Perplexity training: 5.122

==== Starting epoch 83 ====
  Batch 0 Loss 10.9687
  Batch 100 Loss 8.5404
  Batch 200 Loss 7.5519
  Batch 300 Loss 7.7664
  Batch 400 Loss 7.4614
  Batch 500 Loss 7.9226
  Batch 600 Loss 9.2894
  Batch 700 Loss 7.3185
Resetting 15027 PBs
Finished epoch 83 in 164.0 seconds
Perplexity training: 5.156
Measuring development set...
Recognition iteration 0 Loss 22.737
Recognition finished, iteration 100 Loss 0.033
Recognition iteration 0 Loss 21.726
Recognition finished, iteration 100 Loss 0.024
Recognition iteration 0 Loss 23.180
Recognition finished, iteration 100 Loss 0.046
Recognition iteration 0 Loss 22.587
Recognition finished, iteration 100 Loss 0.034
Perplexity dev: 2.673

==== Starting epoch 84 ====
  Batch 0 Loss 7.3610
  Batch 100 Loss 9.8552
  Batch 200 Loss 6.2460
  Batch 300 Loss 6.9164
  Batch 400 Loss 9.1966
  Batch 500 Loss 8.5517
  Batch 600 Loss 8.2236
  Batch 700 Loss 8.0389
Resetting 15179 PBs
Finished epoch 84 in 165.0 seconds
Perplexity training: 5.167

==== Starting epoch 85 ====
  Batch 0 Loss 8.1449
  Batch 100 Loss 6.8170
  Batch 200 Loss 7.5813
  Batch 300 Loss 9.5518
  Batch 400 Loss 10.1439
  Batch 500 Loss 7.7526
  Batch 600 Loss 7.0660
  Batch 700 Loss 6.1760
Resetting 14916 PBs
Finished epoch 85 in 168.0 seconds
Perplexity training: 5.176
Measuring development set...
Recognition iteration 0 Loss 22.642
Recognition finished, iteration 100 Loss 0.033
Recognition iteration 0 Loss 21.443
Recognition finished, iteration 100 Loss 0.024
Recognition iteration 0 Loss 23.066
Recognition finished, iteration 100 Loss 0.042
Recognition iteration 0 Loss 22.450
Recognition finished, iteration 100 Loss 0.031
Perplexity dev: 2.927

==== Starting epoch 86 ====
  Batch 0 Loss 10.6690
  Batch 100 Loss 9.0936
  Batch 200 Loss 6.1958
  Batch 300 Loss 8.5616
  Batch 400 Loss 8.8451
  Batch 500 Loss 8.2177
  Batch 600 Loss 7.8287
  Batch 700 Loss 9.2876
Resetting 15008 PBs
Finished epoch 86 in 191.0 seconds
Perplexity training: 5.125

==== Starting epoch 87 ====
  Batch 0 Loss 8.6557
  Batch 100 Loss 10.3154
  Batch 200 Loss 6.2903
  Batch 300 Loss 8.2013
  Batch 400 Loss 8.1268
  Batch 500 Loss 7.1299
  Batch 600 Loss 8.2931
  Batch 700 Loss 7.9487
Resetting 15085 PBs
Finished epoch 87 in 192.0 seconds
Perplexity training: 5.061
Measuring development set...
Recognition iteration 0 Loss 22.652
Recognition finished, iteration 100 Loss 0.029
Recognition iteration 0 Loss 21.748
Recognition finished, iteration 100 Loss 0.022
Recognition iteration 0 Loss 23.050
Recognition finished, iteration 100 Loss 0.042
Recognition iteration 0 Loss 22.534
Recognition finished, iteration 100 Loss 0.035
Perplexity dev: 2.598

==== Starting epoch 88 ====
  Batch 0 Loss 9.2492
  Batch 100 Loss 6.8213
  Batch 200 Loss 6.5295
  Batch 300 Loss 7.9558
  Batch 400 Loss 7.6224
  Batch 500 Loss 6.7385
  Batch 600 Loss 9.5354
  Batch 700 Loss 6.5792
Resetting 14903 PBs
Finished epoch 88 in 208.0 seconds
Perplexity training: 5.147

==== Starting epoch 89 ====
  Batch 0 Loss 6.0694
  Batch 100 Loss 7.8258
  Batch 200 Loss 8.4071
  Batch 300 Loss 6.5950
  Batch 400 Loss 6.6499
  Batch 500 Loss 7.8735
  Batch 600 Loss 10.1447
  Batch 700 Loss 6.4559
Resetting 15058 PBs
Finished epoch 89 in 224.0 seconds
Perplexity training: 5.043
Measuring development set...
Recognition iteration 0 Loss 22.558
Recognition finished, iteration 100 Loss 0.028
Recognition iteration 0 Loss 21.624
Recognition finished, iteration 100 Loss 0.021
Recognition iteration 0 Loss 23.026
Recognition finished, iteration 100 Loss 0.035
Recognition iteration 0 Loss 22.252
Recognition finished, iteration 100 Loss 0.030
Perplexity dev: 2.722

==== Starting epoch 90 ====
  Batch 0 Loss 8.9688
  Batch 100 Loss 6.1489
  Batch 200 Loss 6.9820
  Batch 300 Loss 8.8569
  Batch 400 Loss 9.6917
  Batch 500 Loss 7.5890
  Batch 600 Loss 7.7796
  Batch 700 Loss 6.8125
Resetting 15003 PBs
Finished epoch 90 in 225.0 seconds
Perplexity training: 5.096

==== Starting epoch 91 ====
  Batch 0 Loss 9.1041
  Batch 100 Loss 9.8975
  Batch 200 Loss 7.4650
  Batch 300 Loss 7.9520
  Batch 400 Loss 10.8540
  Batch 500 Loss 7.7384
  Batch 600 Loss 9.5209
  Batch 700 Loss 7.4777
Resetting 14971 PBs
Finished epoch 91 in 209.0 seconds
Perplexity training: 5.049
Measuring development set...
Recognition iteration 0 Loss 22.765
Recognition finished, iteration 100 Loss 0.026
Recognition iteration 0 Loss 21.729
Recognition finished, iteration 100 Loss 0.020
Recognition iteration 0 Loss 23.169
Recognition finished, iteration 100 Loss 0.037
Recognition iteration 0 Loss 22.212
Recognition finished, iteration 100 Loss 0.036
Perplexity dev: 2.737

==== Starting epoch 92 ====
  Batch 0 Loss 9.2380
  Batch 100 Loss 7.8594
  Batch 200 Loss 6.4512
  Batch 300 Loss 8.3296
  Batch 400 Loss 10.3872
  Batch 500 Loss 7.2999
  Batch 600 Loss 6.9879
  Batch 700 Loss 8.1964
Resetting 14994 PBs
Finished epoch 92 in 207.0 seconds
Perplexity training: 5.007

==== Starting epoch 93 ====
  Batch 0 Loss 10.4181
  Batch 100 Loss 7.3611
  Batch 200 Loss 7.9386
  Batch 300 Loss 7.8722
  Batch 400 Loss 8.9147
  Batch 500 Loss 8.5991
  Batch 600 Loss 7.0389
  Batch 700 Loss 9.1956
Resetting 15118 PBs
Finished epoch 93 in 173.0 seconds
Perplexity training: 5.012
Measuring development set...
Recognition iteration 0 Loss 22.871
Recognition finished, iteration 100 Loss 0.026
Recognition iteration 0 Loss 21.824
Recognition finished, iteration 100 Loss 0.020
Recognition iteration 0 Loss 22.870
Recognition finished, iteration 100 Loss 0.038
Recognition iteration 0 Loss 22.400
Recognition finished, iteration 100 Loss 0.031
Perplexity dev: 2.584

==== Starting epoch 94 ====
  Batch 0 Loss 8.4661
  Batch 100 Loss 6.3476
  Batch 200 Loss 6.7278
  Batch 300 Loss 8.3775
  Batch 400 Loss 11.4197
  Batch 500 Loss 7.2917
  Batch 600 Loss 8.4390
  Batch 700 Loss 8.3222
Resetting 15071 PBs
Finished epoch 94 in 172.0 seconds
Perplexity training: 5.053

==== Starting epoch 95 ====
  Batch 0 Loss 8.9762
  Batch 100 Loss 7.6021
  Batch 200 Loss 6.9649
  Batch 300 Loss 8.2929
  Batch 400 Loss 7.0850
  Batch 500 Loss 7.1362
  Batch 600 Loss 8.7352
  Batch 700 Loss 7.6661
Resetting 14934 PBs
Finished epoch 95 in 161.0 seconds
Perplexity training: 4.992
Measuring development set...
Recognition iteration 0 Loss 22.867
Recognition finished, iteration 100 Loss 0.027
Recognition iteration 0 Loss 21.506
Recognition finished, iteration 100 Loss 0.018
Recognition iteration 0 Loss 22.951
Recognition finished, iteration 100 Loss 0.029
Recognition iteration 0 Loss 22.321
Recognition finished, iteration 100 Loss 0.033
Perplexity dev: 2.859
Finished training in 13907.91 seconds
Finished training after development set stopped improving.
