2019-07-05 16:08:07.993935: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-05 16:08:08.002636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2019-07-05 16:08:08.003807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-07-05 16:08:08.004014: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-05 16:08:08.005651: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-05 16:08:08.006965: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-05 16:08:08.007271: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-05 16:08:08.009080: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-05 16:08:08.010619: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-05 16:08:08.013870: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-05 16:08:08.016660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
Starting training procedure.
Loading training set...
2019-07-05 16:08:08.855288: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-07-05 16:08:09.193148: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5720f70 executing computations on platform CUDA. Devices:
2019-07-05 16:08:09.193192: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-07-05 16:08:09.212922: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2019-07-05 16:08:09.216082: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3877680 executing computations on platform Host. Devices:
2019-07-05 16:08:09.216136: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-05 16:08:09.217320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-07-05 16:08:09.217428: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-05 16:08:09.217457: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-05 16:08:09.217478: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-05 16:08:09.217505: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-05 16:08:09.217523: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-05 16:08:09.217541: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-05 16:08:09.217560: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-05 16:08:09.219115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 1
2019-07-05 16:08:09.219150: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-05 16:08:09.220975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 16:08:09.220993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      1 
2019-07-05 16:08:09.220998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N 
2019-07-05 16:08:09.223051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30060 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
Saving vocab defined by training set...
Loading development set...
Loading mono set...
Writing params file...
Instantiating model...
Ready for training.

Training summery:

=== Data ===

Training set:
Source language:
  Num sentences: 50000 (781 batches)
  Num words: 463025
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 2566 (original 2562)
  Longest: 28
  Reversed: False

Target language:
  Num sentences: 50000 (781 batches)
  Num words: 463876
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1136 (original 1132)
  Longest: 30


Development set:
Source language:
  Num sentences: 256 (4 batches)
  Num words: 2393
  Num UNKS: 4 (0.0 per sentence)
  Vocab size: 2566 (original 2562)
  Longest: 28
  Reversed: False

Target language:
  Num sentences: 256 (4 batches)
  Num words: 2403
  Num UNKS: 0 (0.0 per sentence)
  Vocab size: 1136 (original 1132)
  Longest: 30


=== Model ===
Name: rnnpbnmt
Num layers: 2
Units per layer: 256
Embedding size: 128
Batch size: 64
Learning rate: 0.001
Max translation ratio: 1.5
Gradient clip: 1.0
Dropout: 0.4
Num PBs: 1024
Bind hard: True
Binding strength: 1.0
Autoencode: False
PB learning rate: 0.01
Sigma: 0
p_reset: 0.1
Max recog epochs: 100
p_mono: 0.5


=== Training ===
Max epochs: 150
Early stopping steps: 0
Warm start: False


==== Starting epoch 1 ====
Initializing epoch state
2019-07-05 16:08:17.963641: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-05 16:08:19.365715: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
WARNING: Logging before flag parsing goes to stderr.
W0705 16:08:19.720140 140565933471552 deprecation.py:323] From /lhome/daniesso/venv/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
  Batch 0 Loss 59.6540 Mono loss 65.6383
  Batch 100 Loss 34.4675 Mono loss 37.2424
  Batch 200 Loss 35.1900 Mono loss 35.4642
  Batch 300 Loss 32.1277 Mono loss 36.2490
  Batch 400 Loss 32.0233 Mono loss 36.0881
  Batch 500 Loss 33.2596 Mono loss 34.1264
  Batch 600 Loss 27.9200 Mono loss 31.3848
  Batch 700 Loss 30.1306 Mono loss 34.2166
Resetting 31248 PBs
Finished epoch 1 in 165.0 seconds
Perplexity training: 85.719
Measuring development set...
Recognition iteration 0 Loss 30.608
Recognition finished, iteration 100 Loss 29.076
Recognition iteration 0 Loss 30.569
Recognition finished, iteration 100 Loss 29.145
Recognition iteration 0 Loss 27.701
Recognition finished, iteration 100 Loss 26.417
Recognition iteration 0 Loss 30.540
Recognition finished, iteration 100 Loss 28.967
Perplexity dev: 45.664

==== Starting epoch 2 ====
  Batch 0 Loss 28.2239 Mono loss 31.7260
  Batch 100 Loss 27.1625 Mono loss 31.1196
  Batch 200 Loss 30.6684 Mono loss 29.3015
  Batch 300 Loss 27.9258 Mono loss 28.0516
  Batch 400 Loss 28.2599 Mono loss 27.7238
  Batch 500 Loss 29.5170 Mono loss 28.2266
  Batch 600 Loss 25.0362 Mono loss 26.5626
  Batch 700 Loss 26.6353 Mono loss 27.2003
Resetting 31761 PBs
Finished epoch 2 in 159.0 seconds
Perplexity training: 36.187

==== Starting epoch 3 ====
  Batch 0 Loss 26.6428 Mono loss -1.0000
  Batch 100 Loss 23.3208 Mono loss 27.2184
  Batch 200 Loss 26.3666 Mono loss 27.1241
  Batch 300 Loss 24.7156 Mono loss 27.2639
  Batch 400 Loss 25.5066 Mono loss 29.4627
  Batch 500 Loss 26.0347 Mono loss 26.8491
  Batch 600 Loss 21.4052 Mono loss 27.1025
  Batch 700 Loss 23.2362 Mono loss 30.0450
Resetting 31538 PBs
Finished epoch 3 in 158.0 seconds
Perplexity training: 24.772
Measuring development set...
Recognition iteration 0 Loss 26.695
Recognition finished, iteration 100 Loss 17.737
Recognition iteration 0 Loss 26.373
Recognition finished, iteration 100 Loss 17.680
Recognition iteration 0 Loss 23.925
Recognition finished, iteration 100 Loss 15.802
Recognition iteration 0 Loss 26.696
Recognition finished, iteration 100 Loss 17.608
Perplexity dev: 32.076

==== Starting epoch 4 ====
  Batch 0 Loss 22.9053 Mono loss 26.9043
  Batch 100 Loss 20.8468 Mono loss 25.0632
  Batch 200 Loss 24.0201 Mono loss 23.6915
  Batch 300 Loss 22.1274 Mono loss 24.3548
  Batch 400 Loss 22.0905 Mono loss 22.9605
  Batch 500 Loss 22.9947 Mono loss 23.9960
  Batch 600 Loss 18.7646 Mono loss 23.1464
  Batch 700 Loss 21.8448 Mono loss 23.1606
Resetting 31604 PBs
Finished epoch 4 in 148.0 seconds
Perplexity training: 18.319

==== Starting epoch 5 ====
  Batch 0 Loss 19.2351 Mono loss -1.0000
  Batch 100 Loss 18.6817 Mono loss 24.8714
  Batch 200 Loss 21.6327 Mono loss 25.9580
  Batch 300 Loss 20.0665 Mono loss 24.1126
  Batch 400 Loss 19.9307 Mono loss 22.6968
  Batch 500 Loss 20.9246 Mono loss 21.4438
  Batch 600 Loss 17.4507 Mono loss 21.7143
  Batch 700 Loss 19.7021 Mono loss 21.6862
Resetting 31383 PBs
Finished epoch 5 in 155.0 seconds
Perplexity training: 14.510
Measuring development set...
Recognition iteration 0 Loss 25.733
Recognition finished, iteration 100 Loss 12.227
Recognition iteration 0 Loss 25.248
Recognition finished, iteration 100 Loss 12.169
Recognition iteration 0 Loss 23.005
Recognition finished, iteration 100 Loss 10.978
Recognition iteration 0 Loss 25.811
Recognition finished, iteration 100 Loss 12.493
Perplexity dev: 30.117

==== Starting epoch 6 ====
  Batch 0 Loss 17.6026 Mono loss 22.9642
  Batch 100 Loss 16.8911 Mono loss 21.9957
  Batch 200 Loss 19.3112 Mono loss 24.0821
  Batch 300 Loss 18.4216 Mono loss 22.7536
  Batch 400 Loss 18.7007 Mono loss 23.6825
  Batch 500 Loss 20.2514 Mono loss 23.5045
  Batch 600 Loss 16.0098 Mono loss 21.4623
  Batch 700 Loss 18.4066 Mono loss 20.8580
Resetting 31763 PBs
Finished epoch 6 in 152.0 seconds
Perplexity training: 12.034

==== Starting epoch 7 ====
  Batch 0 Loss 16.0394 Mono loss -1.0000
  Batch 100 Loss 14.9818 Mono loss 19.5921
  Batch 200 Loss 18.2842 Mono loss 19.7588
  Batch 300 Loss 16.8930 Mono loss 20.3900
  Batch 400 Loss 16.9532 Mono loss 20.2850
  Batch 500 Loss 18.5963 Mono loss 20.8400
  Batch 600 Loss 15.4059 Mono loss 19.5654
  Batch 700 Loss 17.2414 Mono loss 20.5026
Resetting 31650 PBs
Finished epoch 7 in 155.0 seconds
Perplexity training: 10.930
Measuring development set...
Recognition iteration 0 Loss 25.585
Recognition finished, iteration 100 Loss 8.902
Recognition iteration 0 Loss 25.107
Recognition finished, iteration 100 Loss 8.734
Recognition iteration 0 Loss 22.582
Recognition finished, iteration 100 Loss 8.033
Recognition iteration 0 Loss 25.631
Recognition finished, iteration 100 Loss 9.400
Perplexity dev: 36.971

==== Starting epoch 8 ====
  Batch 0 Loss 15.6113 Mono loss 19.6646
  Batch 100 Loss 14.5306 Mono loss 19.0180
  Batch 200 Loss 16.8998 Mono loss 19.3833
  Batch 300 Loss 15.8360 Mono loss 20.9525
  Batch 400 Loss 16.1584 Mono loss 20.4579
  Batch 500 Loss 17.4534 Mono loss 18.9774
  Batch 600 Loss 14.0343 Mono loss 16.8766
  Batch 700 Loss 16.2074 Mono loss 20.1641
Resetting 31646 PBs
Finished epoch 8 in 153.0 seconds
Perplexity training: 9.577

==== Starting epoch 9 ====
  Batch 0 Loss 13.7729 Mono loss -1.0000
  Batch 100 Loss 13.6236 Mono loss 17.3901
  Batch 200 Loss 16.5736 Mono loss 17.5049
  Batch 300 Loss 15.3258 Mono loss 16.9988
  Batch 400 Loss 14.9235 Mono loss 18.4650
  Batch 500 Loss 16.9936 Mono loss 22.6251
  Batch 600 Loss 13.4322 Mono loss 18.1824
  Batch 700 Loss 15.7916 Mono loss 16.5666
Resetting 31359 PBs
Finished epoch 9 in 155.0 seconds
Perplexity training: 8.868
Measuring development set...
Recognition iteration 0 Loss 25.047
Recognition finished, iteration 100 Loss 6.836
Recognition iteration 0 Loss 24.483
Recognition finished, iteration 100 Loss 6.606
Recognition iteration 0 Loss 22.121
Recognition finished, iteration 100 Loss 6.061
Recognition iteration 0 Loss 24.884
Recognition finished, iteration 100 Loss 7.383
Perplexity dev: 37.671

==== Starting epoch 10 ====
  Batch 0 Loss 13.2879 Mono loss -1.0000
  Batch 100 Loss 13.0112 Mono loss 16.8590
  Batch 200 Loss 16.3021 Mono loss 18.6348
  Batch 300 Loss 15.1199 Mono loss 17.1058
  Batch 400 Loss 14.2126 Mono loss 16.0922
  Batch 500 Loss 16.1847 Mono loss 19.6871
  Batch 600 Loss 12.3496 Mono loss 17.0546
  Batch 700 Loss 14.6845 Mono loss 18.2443
Resetting 31560 PBs
Finished epoch 10 in 162.0 seconds
Perplexity training: 8.291

==== Starting epoch 11 ====
  Batch 0 Loss 12.5448 Mono loss 15.7773
  Batch 100 Loss 11.5662 Mono loss 18.3936
  Batch 200 Loss 14.9719 Mono loss 18.1055
  Batch 300 Loss 13.6302 Mono loss 16.6523
  Batch 400 Loss 13.9556 Mono loss 15.8259
  Batch 500 Loss 15.7539 Mono loss 17.6159
  Batch 600 Loss 12.3170 Mono loss 14.4462
  Batch 700 Loss 14.4928 Mono loss 18.4704
Resetting 31443 PBs
Finished epoch 11 in 160.0 seconds
Perplexity training: 7.911
Measuring development set...
Recognition iteration 0 Loss 25.184
Recognition finished, iteration 100 Loss 5.232
Recognition iteration 0 Loss 24.460
Recognition finished, iteration 100 Loss 5.092
Recognition iteration 0 Loss 22.466
Recognition finished, iteration 100 Loss 4.789
Recognition iteration 0 Loss 25.039
Recognition finished, iteration 100 Loss 5.907
Perplexity dev: 40.835

==== Starting epoch 12 ====
  Batch 0 Loss 12.7839 Mono loss -1.0000
  Batch 100 Loss 11.5575 Mono loss 16.4499
  Batch 200 Loss 14.7079 Mono loss 17.5078
  Batch 300 Loss 13.7348 Mono loss 15.9145
  Batch 400 Loss 14.0264 Mono loss 18.0722
  Batch 500 Loss 14.8912 Mono loss 15.8499
  Batch 600 Loss 12.1016 Mono loss 14.1988
  Batch 700 Loss 13.8609 Mono loss 16.6434
Resetting 31488 PBs
Finished epoch 12 in 164.0 seconds
Perplexity training: 7.443

==== Starting epoch 13 ====
  Batch 0 Loss 12.1060 Mono loss 15.4090
  Batch 100 Loss 11.1996 Mono loss 14.5592
  Batch 200 Loss 12.9837 Mono loss 15.6430
  Batch 300 Loss 12.2676 Mono loss 17.7776
  Batch 400 Loss 13.0580 Mono loss 14.7891
  Batch 500 Loss 13.4924 Mono loss 19.7468
  Batch 600 Loss 11.4588 Mono loss 16.7789
  Batch 700 Loss 14.1285 Mono loss 14.5187
Resetting 31536 PBs
Finished epoch 13 in 159.0 seconds
Perplexity training: 7.174
Measuring development set...
Recognition iteration 0 Loss 25.118
Recognition finished, iteration 100 Loss 3.978
Recognition iteration 0 Loss 24.024
Recognition finished, iteration 100 Loss 3.864
Recognition iteration 0 Loss 21.979
Recognition finished, iteration 100 Loss 3.573
Recognition iteration 0 Loss 24.743
Recognition finished, iteration 100 Loss 4.736
Perplexity dev: 42.435

==== Starting epoch 14 ====
  Batch 0 Loss 10.9509 Mono loss 16.1171
  Batch 100 Loss 10.4453 Mono loss 15.9472
  Batch 200 Loss 12.4843 Mono loss 16.3440
  Batch 300 Loss 12.0319 Mono loss 15.4431
  Batch 400 Loss 12.9564 Mono loss 15.3955
  Batch 500 Loss 13.8158 Mono loss 15.5286
  Batch 600 Loss 11.2368 Mono loss 16.0718
  Batch 700 Loss 12.5938 Mono loss 14.4156
Resetting 31660 PBs
Finished epoch 14 in 175.0 seconds
Perplexity training: 6.947

==== Starting epoch 15 ====
  Batch 0 Loss 10.3194 Mono loss 15.6004
  Batch 100 Loss 9.6332 Mono loss 16.1815
  Batch 200 Loss 12.9922 Mono loss 17.2426
  Batch 300 Loss 11.1941 Mono loss 15.4332
  Batch 400 Loss 11.7558 Mono loss 17.4380
  Batch 500 Loss 13.8580 Mono loss 14.4471
  Batch 600 Loss 10.1304 Mono loss 14.5284
  Batch 700 Loss 12.5307 Mono loss 16.7440
Resetting 31252 PBs
Finished epoch 15 in 182.0 seconds
Perplexity training: 6.731
Measuring development set...
Recognition iteration 0 Loss 25.460
Recognition finished, iteration 100 Loss 3.502
Recognition iteration 0 Loss 24.600
Recognition finished, iteration 100 Loss 3.254
Recognition iteration 0 Loss 22.624
Recognition finished, iteration 100 Loss 2.957
Recognition iteration 0 Loss 25.148
Recognition finished, iteration 100 Loss 4.099
Perplexity dev: 41.662

==== Starting epoch 16 ====
  Batch 0 Loss 9.5132 Mono loss 15.1120
  Batch 100 Loss 8.9401 Mono loss 14.4483
  Batch 200 Loss 11.9295 Mono loss 15.8260
  Batch 300 Loss 12.7700 Mono loss 13.4865
  Batch 400 Loss 10.8710 Mono loss 15.4316
  Batch 500 Loss 12.6736 Mono loss 14.4941
  Batch 600 Loss 11.3875 Mono loss 15.1443
  Batch 700 Loss 12.7224 Mono loss 14.9784
Resetting 31134 PBs
Finished epoch 16 in 176.0 seconds
Perplexity training: 6.498

==== Starting epoch 17 ====
  Batch 0 Loss 9.4519 Mono loss 15.0908
  Batch 100 Loss 9.0760 Mono loss 13.5492
  Batch 200 Loss 13.4565 Mono loss 14.3206
  Batch 300 Loss 12.7568 Mono loss 15.4783
  Batch 400 Loss 10.5863 Mono loss 12.7813
  Batch 500 Loss 11.5561 Mono loss 16.8108
  Batch 600 Loss 9.5991 Mono loss 13.9323
  Batch 700 Loss 11.7303 Mono loss 15.1708
Resetting 31456 PBs
Finished epoch 17 in 170.0 seconds
Perplexity training: 6.238
Measuring development set...
Recognition iteration 0 Loss 24.675
Recognition finished, iteration 100 Loss 2.394
Recognition iteration 0 Loss 23.749
Recognition finished, iteration 100 Loss 2.257
Recognition iteration 0 Loss 21.807
Recognition finished, iteration 100 Loss 2.018
Recognition iteration 0 Loss 24.464
Recognition finished, iteration 100 Loss 3.114
Perplexity dev: 89.168

==== Starting epoch 18 ====
  Batch 0 Loss 10.2853 Mono loss 15.4728
  Batch 100 Loss 10.0803 Mono loss 15.7421
  Batch 200 Loss 12.3789 Mono loss 13.0126
  Batch 300 Loss 11.3017 Mono loss 13.4646
  Batch 400 Loss 9.8958 Mono loss 14.3648
  Batch 500 Loss 13.7948 Mono loss 12.5818
  Batch 600 Loss 8.7177 Mono loss 14.8147
  Batch 700 Loss 10.6109 Mono loss 15.4328
Resetting 31378 PBs
Finished epoch 18 in 185.0 seconds
Perplexity training: 6.151

==== Starting epoch 19 ====
  Batch 0 Loss 9.9087 Mono loss 14.5187
  Batch 100 Loss 9.9174 Mono loss 15.1874
  Batch 200 Loss 11.4902 Mono loss 14.0291
  Batch 300 Loss 11.3268 Mono loss 14.0787
  Batch 400 Loss 10.8276 Mono loss 13.8826
  Batch 500 Loss 12.8534 Mono loss 14.3102
  Batch 600 Loss 9.4145 Mono loss 15.0002
  Batch 700 Loss 10.3900 Mono loss 14.7711
Resetting 31523 PBs
Finished epoch 19 in 173.0 seconds
Perplexity training: 5.991
Measuring development set...
Recognition iteration 0 Loss 24.724
Recognition finished, iteration 100 Loss 1.856
Recognition iteration 0 Loss 23.700
Recognition finished, iteration 100 Loss 1.746
Recognition iteration 0 Loss 21.631
Recognition finished, iteration 100 Loss 1.536
Recognition iteration 0 Loss 24.541
Recognition finished, iteration 100 Loss 2.572
Perplexity dev: 77.176

==== Starting epoch 20 ====
  Batch 0 Loss 10.4955 Mono loss 12.7134
  Batch 100 Loss 9.2789 Mono loss 14.6248
  Batch 200 Loss 11.7723 Mono loss 15.0992
  Batch 300 Loss 10.3992 Mono loss 12.6092
  Batch 400 Loss 11.1075 Mono loss 14.7306
  Batch 500 Loss 11.2176 Mono loss 14.0064
  Batch 600 Loss 8.7886 Mono loss 14.0726
  Batch 700 Loss 10.8148 Mono loss 13.8834
Resetting 31280 PBs
Finished epoch 20 in 181.0 seconds
Perplexity training: 5.893

==== Starting epoch 21 ====
  Batch 0 Loss 10.5718 Mono loss 13.1337
  Batch 100 Loss 8.4672 Mono loss 12.2643
  Batch 200 Loss 10.5474 Mono loss 13.1077
  Batch 300 Loss 9.8746 Mono loss 13.0365
  Batch 400 Loss 11.3833 Mono loss 12.5222
  Batch 500 Loss 11.7994 Mono loss 13.1558
  Batch 600 Loss 10.5623 Mono loss 14.2952
  Batch 700 Loss 11.2381 Mono loss 15.5450
Resetting 31301 PBs
Finished epoch 21 in 180.0 seconds
Perplexity training: 5.660
Measuring development set...
Recognition iteration 0 Loss 24.651
Recognition finished, iteration 100 Loss 1.549
Recognition iteration 0 Loss 23.868
Recognition finished, iteration 100 Loss 1.387
Recognition iteration 0 Loss 21.724
Recognition finished, iteration 100 Loss 1.291
Recognition iteration 0 Loss 24.782
Recognition finished, iteration 100 Loss 2.094
Perplexity dev: 64.476

==== Starting epoch 22 ====
  Batch 0 Loss 8.9200 Mono loss 13.5351
  Batch 100 Loss 8.3859 Mono loss 12.9596
  Batch 200 Loss 9.9257 Mono loss 15.6077
  Batch 300 Loss 9.2756 Mono loss 11.9273
  Batch 400 Loss 11.7690 Mono loss 13.3194
  Batch 500 Loss 11.2764 Mono loss 15.4076
  Batch 600 Loss 8.7641 Mono loss 13.2028
  Batch 700 Loss 10.3180 Mono loss 13.3977
Resetting 31787 PBs
Finished epoch 22 in 187.0 seconds
Perplexity training: 5.500

==== Starting epoch 23 ====
  Batch 0 Loss 8.3180 Mono loss -1.0000
  Batch 100 Loss 6.9839 Mono loss 13.5690
  Batch 200 Loss 9.4257 Mono loss 12.4118
  Batch 300 Loss 10.2311 Mono loss 14.0059
  Batch 400 Loss 9.5562 Mono loss 13.4946
  Batch 500 Loss 9.9449 Mono loss 12.4641
  Batch 600 Loss 7.8700 Mono loss 12.3719
  Batch 700 Loss 10.1792 Mono loss 12.2485
Resetting 31704 PBs
Finished epoch 23 in 192.0 seconds
Perplexity training: 5.474
Measuring development set...
Recognition iteration 0 Loss 25.830
Recognition finished, iteration 100 Loss 1.260
Recognition iteration 0 Loss 24.714
Recognition finished, iteration 100 Loss 1.116
Recognition iteration 0 Loss 22.831
Recognition finished, iteration 100 Loss 1.047
Recognition iteration 0 Loss 25.885
Recognition finished, iteration 100 Loss 1.836
Perplexity dev: 46.575

==== Starting epoch 24 ====
  Batch 0 Loss 9.6533 Mono loss -1.0000
  Batch 100 Loss 7.2591 Mono loss 13.6903
  Batch 200 Loss 11.3083 Mono loss 12.1543
  Batch 300 Loss 9.9043 Mono loss 11.7611
  Batch 400 Loss 10.6437 Mono loss 11.6582
  Batch 500 Loss 11.9688 Mono loss 11.9215
  Batch 600 Loss 7.6844 Mono loss 11.6028
  Batch 700 Loss 9.5346 Mono loss 12.3431
Resetting 31887 PBs
Finished epoch 24 in 185.0 seconds
Perplexity training: 5.432

==== Starting epoch 25 ====
  Batch 0 Loss 9.4086 Mono loss -1.0000
  Batch 100 Loss 6.5582 Mono loss 10.9619
  Batch 200 Loss 9.5922 Mono loss 12.4981
  Batch 300 Loss 8.9222 Mono loss 9.2654
  Batch 400 Loss 9.8337 Mono loss 11.9872
  Batch 500 Loss 10.7054 Mono loss 13.8320
  Batch 600 Loss 8.0422 Mono loss 11.4024
  Batch 700 Loss 9.6098 Mono loss 14.3862
Resetting 31420 PBs
Finished epoch 25 in 177.0 seconds
Perplexity training: 5.384
Measuring development set...
Recognition iteration 0 Loss 24.746
Recognition finished, iteration 100 Loss 0.968
Recognition iteration 0 Loss 23.747
Recognition finished, iteration 100 Loss 0.840
Recognition iteration 0 Loss 21.586
Recognition finished, iteration 100 Loss 0.754
Recognition iteration 0 Loss 24.547
Recognition finished, iteration 100 Loss 1.411
Perplexity dev: 67.361

==== Starting epoch 26 ====
  Batch 0 Loss 7.7432 Mono loss -1.0000
  Batch 100 Loss 8.2454 Mono loss 11.9931
  Batch 200 Loss 9.9577 Mono loss 11.0795
  Batch 300 Loss 8.5079 Mono loss 12.0268
  Batch 400 Loss 9.5621 Mono loss 11.1307
  Batch 500 Loss 10.3221 Mono loss 11.2938
  Batch 600 Loss 8.4994 Mono loss 11.1636
  Batch 700 Loss 10.2618 Mono loss 13.1074
Resetting 31759 PBs
Finished epoch 26 in 178.0 seconds
Perplexity training: 5.191

==== Starting epoch 27 ====
  Batch 0 Loss 7.1688 Mono loss -1.0000
  Batch 100 Loss 6.8484 Mono loss 12.5739
  Batch 200 Loss 10.7271 Mono loss 12.7475
  Batch 300 Loss 9.7034 Mono loss 10.7934
  Batch 400 Loss 9.4234 Mono loss 12.4686
  Batch 500 Loss 10.5749 Mono loss 11.4998
  Batch 600 Loss 7.9435 Mono loss 10.2839
  Batch 700 Loss 9.0987 Mono loss 15.2116
Resetting 31631 PBs
Finished epoch 27 in 200.0 seconds
Perplexity training: 5.223
Measuring development set...
Recognition iteration 0 Loss 24.962
Recognition finished, iteration 100 Loss 0.716
Recognition iteration 0 Loss 23.909
Recognition finished, iteration 100 Loss 0.688
Recognition iteration 0 Loss 22.045
Recognition finished, iteration 100 Loss 0.597
Recognition iteration 0 Loss 24.870
Recognition finished, iteration 100 Loss 1.091
Perplexity dev: 94.520

==== Starting epoch 28 ====
  Batch 0 Loss 7.5692 Mono loss -1.0000
  Batch 100 Loss 8.4209 Mono loss 12.8754
  Batch 200 Loss 12.3776 Mono loss 11.8375
  Batch 300 Loss 8.3174 Mono loss 11.5706
  Batch 400 Loss 8.5123 Mono loss 10.6657
  Batch 500 Loss 11.9161 Mono loss 10.3591
  Batch 600 Loss 7.9652 Mono loss 12.8572
  Batch 700 Loss 9.6264 Mono loss 13.4938
Resetting 31362 PBs
Finished epoch 28 in 201.0 seconds
Perplexity training: 5.169

==== Starting epoch 29 ====
  Batch 0 Loss 7.7382 Mono loss 12.1563
  Batch 100 Loss 8.6178 Mono loss 11.6419
  Batch 200 Loss 9.8940 Mono loss 10.2841
  Batch 300 Loss 8.6029 Mono loss 12.5503
  Batch 400 Loss 7.5408 Mono loss 11.7338
  Batch 500 Loss 9.5285 Mono loss 11.8899
  Batch 600 Loss 6.9306 Mono loss 10.2717
  Batch 700 Loss 10.2625 Mono loss 12.3946
Resetting 31444 PBs
Finished epoch 29 in 193.0 seconds
Perplexity training: 5.010
Measuring development set...
Recognition iteration 0 Loss 24.327
Recognition finished, iteration 100 Loss 0.516
Recognition iteration 0 Loss 23.593
Recognition finished, iteration 100 Loss 0.516
Recognition iteration 0 Loss 21.601
Recognition finished, iteration 100 Loss 0.459
Recognition iteration 0 Loss 24.209
Recognition finished, iteration 100 Loss 0.905
Perplexity dev: 93.180

==== Starting epoch 30 ====
  Batch 0 Loss 6.6814 Mono loss -1.0000
  Batch 100 Loss 8.8126 Mono loss 11.8325
  Batch 200 Loss 9.6147 Mono loss 11.5306
  Batch 300 Loss 10.0599 Mono loss 12.9245
  Batch 400 Loss 9.3100 Mono loss 11.9185
  Batch 500 Loss 9.0893 Mono loss 11.2914
  Batch 600 Loss 6.7202 Mono loss 11.5642
  Batch 700 Loss 8.8497 Mono loss 11.1348
Resetting 31237 PBs
Finished epoch 30 in 210.0 seconds
Perplexity training: 4.924

==== Starting epoch 31 ====
  Batch 0 Loss 6.2295 Mono loss 11.7761
  Batch 100 Loss 7.2118 Mono loss 12.0720
  Batch 200 Loss 8.7231 Mono loss 10.3253
  Batch 300 Loss 8.7047 Mono loss 12.6255
  Batch 400 Loss 8.5792 Mono loss 12.4938
  Batch 500 Loss 7.6957 Mono loss 12.3671
  Batch 600 Loss 6.4742 Mono loss 10.8335
  Batch 700 Loss 7.7492 Mono loss 9.7436
Resetting 31746 PBs
Finished epoch 31 in 209.0 seconds
Perplexity training: 5.103
Measuring development set...
Recognition iteration 0 Loss 24.587
Recognition finished, iteration 100 Loss 0.440
Recognition iteration 0 Loss 23.843
Recognition finished, iteration 100 Loss 0.473
Recognition iteration 0 Loss 22.053
Recognition finished, iteration 100 Loss 0.390
Recognition iteration 0 Loss 24.355
Recognition finished, iteration 100 Loss 0.757
Perplexity dev: 102.334

==== Starting epoch 32 ====
  Batch 0 Loss 7.4158 Mono loss -1.0000
  Batch 100 Loss 7.2081 Mono loss 11.1264
  Batch 200 Loss 9.5876 Mono loss 10.9494
  Batch 300 Loss 8.2466 Mono loss 9.9872
  Batch 400 Loss 7.7542 Mono loss 8.3902
  Batch 500 Loss 7.9579 Mono loss 12.2806
  Batch 600 Loss 6.3836 Mono loss 11.7094
  Batch 700 Loss 8.0129 Mono loss 11.7345
Resetting 31425 PBs
Finished epoch 32 in 213.0 seconds
Perplexity training: 4.860

==== Starting epoch 33 ====
  Batch 0 Loss 7.1864 Mono loss 11.8235
  Batch 100 Loss 7.1827 Mono loss 9.3639
  Batch 200 Loss 7.8272 Mono loss 10.6255
  Batch 300 Loss 7.3950 Mono loss 10.7962
  Batch 400 Loss 9.1627 Mono loss 9.5219
  Batch 500 Loss 7.5419 Mono loss 11.9282
  Batch 600 Loss 6.0914 Mono loss 13.5108
  Batch 700 Loss 8.8304 Mono loss 12.4267
Resetting 31265 PBs
Finished epoch 33 in 209.0 seconds
Perplexity training: 4.833
Measuring development set...
Recognition iteration 0 Loss 24.518
Recognition finished, iteration 100 Loss 0.342
Recognition iteration 0 Loss 23.829
Recognition finished, iteration 100 Loss 0.355
Recognition iteration 0 Loss 22.014
Recognition finished, iteration 100 Loss 0.309
Recognition iteration 0 Loss 24.641
Recognition finished, iteration 100 Loss 0.639
Perplexity dev: 104.329

==== Starting epoch 34 ====
  Batch 0 Loss 7.3710 Mono loss -1.0000
  Batch 100 Loss 6.4971 Mono loss 11.3056
  Batch 200 Loss 8.8210 Mono loss 11.4222
  Batch 300 Loss 7.5249 Mono loss 11.3469
  Batch 400 Loss 9.7037 Mono loss 10.6081
  Batch 500 Loss 6.8464 Mono loss 10.7055
  Batch 600 Loss 6.7076 Mono loss 9.1988
  Batch 700 Loss 8.7406 Mono loss 9.8599
Resetting 31557 PBs
Finished epoch 34 in 210.0 seconds
Perplexity training: 4.695

==== Starting epoch 35 ====
  Batch 0 Loss 6.7272 Mono loss -1.0000
  Batch 100 Loss 5.5375 Mono loss 10.4701
  Batch 200 Loss 7.3084 Mono loss 11.5539
  Batch 300 Loss 7.9280 Mono loss 9.5172
  Batch 400 Loss 8.5819 Mono loss 13.0688
  Batch 500 Loss 9.0567 Mono loss 12.5999
  Batch 600 Loss 5.6795 Mono loss 9.6542
  Batch 700 Loss 8.4269 Mono loss 10.4468
Resetting 31583 PBs
Finished epoch 35 in 219.0 seconds
Perplexity training: 4.762
Measuring development set...
Recognition iteration 0 Loss 25.734
Recognition finished, iteration 100 Loss 0.339
Recognition iteration 0 Loss 25.155
Recognition finished, iteration 100 Loss 0.312
Recognition iteration 0 Loss 23.051
Recognition finished, iteration 100 Loss 0.270
Recognition iteration 0 Loss 25.553
Recognition finished, iteration 100 Loss 0.538
Perplexity dev: 73.367

==== Starting epoch 36 ====
  Batch 0 Loss 5.8675 Mono loss -1.0000
  Batch 100 Loss 6.5076 Mono loss 10.1504
  Batch 200 Loss 8.4014 Mono loss 10.8006
  Batch 300 Loss 8.3386 Mono loss 11.4951
  Batch 400 Loss 8.2428 Mono loss 7.7929
  Batch 500 Loss 7.9052 Mono loss 10.4537
  Batch 600 Loss 5.6403 Mono loss 11.7128
  Batch 700 Loss 7.2559 Mono loss 10.5630
Resetting 31763 PBs
Finished epoch 36 in 220.0 seconds
Perplexity training: 4.710

==== Starting epoch 37 ====
  Batch 0 Loss 5.7430 Mono loss 9.6955
  Batch 100 Loss 5.7212 Mono loss 10.6541
  Batch 200 Loss 8.1328 Mono loss 9.8259
  Batch 300 Loss 7.0370 Mono loss 8.4970
  Batch 400 Loss 7.9600 Mono loss 9.3629
  Batch 500 Loss 10.4655 Mono loss 10.4325
  Batch 600 Loss 6.5919 Mono loss 9.0730
  Batch 700 Loss 6.6792 Mono loss 11.6592
Resetting 31340 PBs
Finished epoch 37 in 225.0 seconds
Perplexity training: 4.711
Measuring development set...
Recognition iteration 0 Loss 24.753
Recognition finished, iteration 100 Loss 0.255
Recognition iteration 0 Loss 24.106
Recognition finished, iteration 100 Loss 0.241
Recognition iteration 0 Loss 22.153
Recognition finished, iteration 100 Loss 0.208
Recognition iteration 0 Loss 24.714
Recognition finished, iteration 100 Loss 0.468
Perplexity dev: 145.066

==== Starting epoch 38 ====
  Batch 0 Loss 6.0082 Mono loss -1.0000
  Batch 100 Loss 5.5103 Mono loss 9.0060
  Batch 200 Loss 9.0556 Mono loss 10.2158
  Batch 300 Loss 7.0322 Mono loss 11.4493
  Batch 400 Loss 7.8439 Mono loss 8.7315
  Batch 500 Loss 10.1016 Mono loss 9.6068
  Batch 600 Loss 6.1186 Mono loss 8.5268
  Batch 700 Loss 7.9858 Mono loss 10.8394
Resetting 31364 PBs
Finished epoch 38 in 213.0 seconds
Perplexity training: 4.540

==== Starting epoch 39 ====
  Batch 0 Loss 6.4509 Mono loss 10.0574
  Batch 100 Loss 5.6000 Mono loss 10.2540
  Batch 200 Loss 8.1629 Mono loss 10.1629
  Batch 300 Loss 6.6896 Mono loss 10.6789
  Batch 400 Loss 7.1023 Mono loss 10.7656
  Batch 500 Loss 9.5617 Mono loss 10.7725
  Batch 600 Loss 5.3077 Mono loss 9.8389
  Batch 700 Loss 8.2862 Mono loss 10.6445
Resetting 31601 PBs
Finished epoch 39 in 216.0 seconds
Perplexity training: 4.500
Measuring development set...
Recognition iteration 0 Loss 24.587
Recognition finished, iteration 100 Loss 0.200
Recognition iteration 0 Loss 24.010
Recognition finished, iteration 100 Loss 0.185
Recognition iteration 0 Loss 22.030
Recognition finished, iteration 100 Loss 0.130
Recognition iteration 0 Loss 24.213
Recognition finished, iteration 100 Loss 0.348
Perplexity dev: 148.607

==== Starting epoch 40 ====
  Batch 0 Loss 6.7285 Mono loss -1.0000
  Batch 100 Loss 5.5447 Mono loss 11.1554
  Batch 200 Loss 7.7733 Mono loss 9.8368
  Batch 300 Loss 8.3932 Mono loss 11.1579
  Batch 400 Loss 7.8629 Mono loss 10.0209
  Batch 500 Loss 8.0261 Mono loss 9.2551
  Batch 600 Loss 6.2053 Mono loss 10.4820
  Batch 700 Loss 8.5377 Mono loss 11.0139
Resetting 31497 PBs
Finished epoch 40 in 231.0 seconds
Perplexity training: 4.513

==== Starting epoch 41 ====
  Batch 0 Loss 5.1800 Mono loss -1.0000
  Batch 100 Loss 6.8371 Mono loss 7.4435
  Batch 200 Loss 6.7316 Mono loss 10.5726
  Batch 300 Loss 6.2556 Mono loss 10.2677
  Batch 400 Loss 7.5555 Mono loss 11.0883
  Batch 500 Loss 8.8050 Mono loss 8.4769
  Batch 600 Loss 4.9506 Mono loss 8.8913
  Batch 700 Loss 8.3552 Mono loss 10.4531
Resetting 31481 PBs
Finished epoch 41 in 228.0 seconds
Perplexity training: 4.489
Measuring development set...
Recognition iteration 0 Loss 24.359
Recognition finished, iteration 100 Loss 0.146
Recognition iteration 0 Loss 23.993
Recognition finished, iteration 100 Loss 0.161
Recognition iteration 0 Loss 22.160
Recognition finished, iteration 100 Loss 0.114
Recognition iteration 0 Loss 24.044
Recognition finished, iteration 100 Loss 0.293
Perplexity dev: 165.860

==== Starting epoch 42 ====
  Batch 0 Loss 5.9515 Mono loss -1.0000
  Batch 100 Loss 6.4475 Mono loss 10.5985
  Batch 200 Loss 8.0385 Mono loss 9.0557
  Batch 300 Loss 5.9396 Mono loss 10.4301
  Batch 400 Loss 6.7659 Mono loss 9.5904
  Batch 500 Loss 9.7188 Mono loss 12.2519
  Batch 600 Loss 4.9841 Mono loss 8.6145
  Batch 700 Loss 7.9880 Mono loss 11.5637
Resetting 31321 PBs
Finished epoch 42 in 232.0 seconds
Perplexity training: 4.357

==== Starting epoch 43 ====
  Batch 0 Loss 6.0544 Mono loss -1.0000
  Batch 100 Loss 4.9638 Mono loss 8.3927
  Batch 200 Loss 7.3780 Mono loss 10.1960
  Batch 300 Loss 7.0029 Mono loss 8.6704
  Batch 400 Loss 6.8377 Mono loss 9.2641
  Batch 500 Loss 7.3584 Mono loss 10.1815
  Batch 600 Loss 5.3629 Mono loss 10.2352
  Batch 700 Loss 6.9780 Mono loss 9.5967
Resetting 31355 PBs
Finished epoch 43 in 236.0 seconds
Perplexity training: 4.339
Measuring development set...
Recognition iteration 0 Loss 25.446
Recognition finished, iteration 100 Loss 0.155
Recognition iteration 0 Loss 24.939
Recognition finished, iteration 100 Loss 0.146
Recognition iteration 0 Loss 22.824
Recognition finished, iteration 100 Loss 0.109
Recognition iteration 0 Loss 25.241
Recognition finished, iteration 100 Loss 0.271
Perplexity dev: 117.645

==== Starting epoch 44 ====
  Batch 0 Loss 5.0987 Mono loss 9.6442
  Batch 100 Loss 6.0690 Mono loss 9.3112
  Batch 200 Loss 6.6984 Mono loss 9.3435
  Batch 300 Loss 6.6312 Mono loss 11.1717
  Batch 400 Loss 7.7100 Mono loss 7.6097
  Batch 500 Loss 7.8069 Mono loss 11.2622
  Batch 600 Loss 5.4075 Mono loss 9.4938
  Batch 700 Loss 9.0873 Mono loss 9.9810
Resetting 31565 PBs
Finished epoch 44 in 249.0 seconds
Perplexity training: 4.397

==== Starting epoch 45 ====
  Batch 0 Loss 6.8812 Mono loss 8.8541
  Batch 100 Loss 6.5488 Mono loss 10.0232
  Batch 200 Loss 6.7184 Mono loss 9.7421
  Batch 300 Loss 6.5614 Mono loss 9.3802
  Batch 400 Loss 7.5711 Mono loss 9.1945
  Batch 500 Loss 7.7175 Mono loss 9.8403
  Batch 600 Loss 4.7036 Mono loss 10.8618
  Batch 700 Loss 7.5713 Mono loss 7.7723
Resetting 31202 PBs
Finished epoch 45 in 233.0 seconds
Perplexity training: 4.298
Measuring development set...
Recognition iteration 0 Loss 24.455
Recognition finished, iteration 100 Loss 0.112
Recognition iteration 0 Loss 23.836
Recognition finished, iteration 100 Loss 0.106
Recognition iteration 0 Loss 21.797
Recognition finished, iteration 100 Loss 0.075
Recognition iteration 0 Loss 24.316
Recognition finished, iteration 100 Loss 0.191
Perplexity dev: 146.698

==== Starting epoch 46 ====
  Batch 0 Loss 6.7767 Mono loss 7.6255
  Batch 100 Loss 6.2152 Mono loss 10.0223
  Batch 200 Loss 6.1290 Mono loss 10.4692
  Batch 300 Loss 6.6925 Mono loss 8.5037
  Batch 400 Loss 6.6098 Mono loss 8.8776
  Batch 500 Loss 7.3825 Mono loss 10.1625
  Batch 600 Loss 5.0404 Mono loss 10.1211
  Batch 700 Loss 7.2594 Mono loss 8.2364
Resetting 31612 PBs
Finished epoch 46 in 247.0 seconds
Perplexity training: 4.299

==== Starting epoch 47 ====
  Batch 0 Loss 6.4202 Mono loss -1.0000
  Batch 100 Loss 6.1440 Mono loss 9.0507
  Batch 200 Loss 6.2304 Mono loss 8.1716
  Batch 300 Loss 6.3366 Mono loss 8.6422
  Batch 400 Loss 5.5532 Mono loss 8.9236
  Batch 500 Loss 6.8738 Mono loss 9.5559
  Batch 600 Loss 6.4751 Mono loss 9.6533
  Batch 700 Loss 8.0844 Mono loss 8.3143
Resetting 31599 PBs
Finished epoch 47 in 242.0 seconds
Perplexity training: 4.286
Measuring development set...
Recognition iteration 0 Loss 24.130
Recognition finished, iteration 100 Loss 0.095
Recognition iteration 0 Loss 23.522
Recognition finished, iteration 100 Loss 0.091
Recognition iteration 0 Loss 21.868
Recognition finished, iteration 100 Loss 0.079
Recognition iteration 0 Loss 23.949
Recognition finished, iteration 100 Loss 0.160
Perplexity dev: 153.724

==== Starting epoch 48 ====
  Batch 0 Loss 6.1359 Mono loss 8.8550
  Batch 100 Loss 6.4895 Mono loss 8.5403
  Batch 200 Loss 7.3897 Mono loss 8.2797
  Batch 300 Loss 7.4658 Mono loss 7.3629
  Batch 400 Loss 7.1793 Mono loss 9.7424
  Batch 500 Loss 7.2100 Mono loss 8.0047
  Batch 600 Loss 6.5566 Mono loss 9.6706
  Batch 700 Loss 7.8245 Mono loss 9.5947
Resetting 31277 PBs
Finished epoch 48 in 250.0 seconds
Perplexity training: 4.326

==== Starting epoch 49 ====
  Batch 0 Loss 6.6884 Mono loss 9.8491
  Batch 100 Loss 5.2203 Mono loss 8.2183
  Batch 200 Loss 6.7267 Mono loss 9.4710
  Batch 300 Loss 5.4764 Mono loss 7.6700
  Batch 400 Loss 5.1150 Mono loss 7.8085
  Batch 500 Loss 6.6765 Mono loss 8.4508
  Batch 600 Loss 6.3336 Mono loss 8.6111
  Batch 700 Loss 6.5472 Mono loss 9.6830
Resetting 31347 PBs
Finished epoch 49 in 249.0 seconds
Perplexity training: 4.271
Measuring development set...
Recognition iteration 0 Loss 24.328
Recognition finished, iteration 100 Loss 0.078
Recognition iteration 0 Loss 23.670
Recognition finished, iteration 100 Loss 0.079
Recognition iteration 0 Loss 21.743
Recognition finished, iteration 100 Loss 0.059
Recognition iteration 0 Loss 24.078
Recognition finished, iteration 100 Loss 0.143
Perplexity dev: 166.921

==== Starting epoch 50 ====
  Batch 0 Loss 7.1730 Mono loss 7.9483
  Batch 100 Loss 5.8143 Mono loss 8.9003
  Batch 200 Loss 7.0740 Mono loss 8.8852
  Batch 300 Loss 5.8632 Mono loss 10.1869
  Batch 400 Loss 6.0340 Mono loss 11.2030
  Batch 500 Loss 6.8293 Mono loss 10.3076
  Batch 600 Loss 5.4724 Mono loss 9.4550
  Batch 700 Loss 7.2383 Mono loss 10.1952
Resetting 31353 PBs
Finished epoch 50 in 261.0 seconds
Perplexity training: 4.224

==== Starting epoch 51 ====
  Batch 0 Loss 6.4829 Mono loss -1.0000
  Batch 100 Loss 4.3285 Mono loss 8.6388
  Batch 200 Loss 7.2659 Mono loss 10.1787
  Batch 300 Loss 6.5587 Mono loss 7.4555
  Batch 400 Loss 5.1267 Mono loss 7.7018
  Batch 500 Loss 5.9037 Mono loss 8.3600
  Batch 600 Loss 7.3918 Mono loss 9.5490
  Batch 700 Loss 4.8654 Mono loss 8.3208
Resetting 31761 PBs
Finished epoch 51 in 252.0 seconds
Perplexity training: 4.211
Measuring development set...
Recognition iteration 0 Loss 24.400
Recognition finished, iteration 100 Loss 0.069
Recognition iteration 0 Loss 23.600
Recognition finished, iteration 100 Loss 0.072
Recognition iteration 0 Loss 21.909
Recognition finished, iteration 100 Loss 0.054
Recognition iteration 0 Loss 23.958
Recognition finished, iteration 100 Loss 0.118
Perplexity dev: 143.627

==== Starting epoch 52 ====
  Batch 0 Loss 4.6510 Mono loss -1.0000
  Batch 100 Loss 4.3915 Mono loss 9.9923
  Batch 200 Loss 5.7635 Mono loss 8.6508
  Batch 300 Loss 5.9469 Mono loss 8.1935
  Batch 400 Loss 5.8017 Mono loss 9.9339
  Batch 500 Loss 5.3006 Mono loss 9.1222
  Batch 600 Loss 5.5496 Mono loss 10.8948
  Batch 700 Loss 7.9631 Mono loss 9.3459
Resetting 31536 PBs
Finished epoch 52 in 263.0 seconds
Perplexity training: 4.230

==== Starting epoch 53 ====
  Batch 0 Loss 6.4903 Mono loss -1.0000
  Batch 100 Loss 5.6799 Mono loss 7.6237
  Batch 200 Loss 6.5239 Mono loss 10.1574
  Batch 300 Loss 4.4223 Mono loss 8.5095
  Batch 400 Loss 4.3640 Mono loss 7.9396
  Batch 500 Loss 6.1292 Mono loss 9.3575
  Batch 600 Loss 4.8577 Mono loss 9.9289
  Batch 700 Loss 6.4488 Mono loss 7.9288
Resetting 31380 PBs
Finished epoch 53 in 293.0 seconds
Perplexity training: 4.205
Measuring development set...
Recognition iteration 0 Loss 23.742
Recognition finished, iteration 100 Loss 0.058
Recognition iteration 0 Loss 23.532
Recognition finished, iteration 100 Loss 0.054
Recognition iteration 0 Loss 21.771
Recognition finished, iteration 100 Loss 0.043
Recognition iteration 0 Loss 24.002
Recognition finished, iteration 100 Loss 0.102
Perplexity dev: 181.434

==== Starting epoch 54 ====
  Batch 0 Loss 7.0972 Mono loss -1.0000
  Batch 100 Loss 3.5338 Mono loss 10.7987
  Batch 200 Loss 5.8040 Mono loss 8.6515
  Batch 300 Loss 4.8469 Mono loss 8.6613
  Batch 400 Loss 5.3679 Mono loss 7.2220
  Batch 500 Loss 5.6572 Mono loss 7.5978
  Batch 600 Loss 4.6756 Mono loss 7.1592
  Batch 700 Loss 5.6685 Mono loss 8.2057
Resetting 31768 PBs
Finished epoch 54 in 275.0 seconds
Perplexity training: 3.999

==== Starting epoch 55 ====
  Batch 0 Loss 4.7061 Mono loss -1.0000
  Batch 100 Loss 5.2925 Mono loss 7.5558
  Batch 200 Loss 6.0105 Mono loss 8.0481
  Batch 300 Loss 5.3066 Mono loss 9.1398
  Batch 400 Loss 6.5455 Mono loss 7.3837
  Batch 500 Loss 5.5028 Mono loss 8.9298
  Batch 600 Loss 4.5699 Mono loss 9.9763
  Batch 700 Loss 5.6679 Mono loss 9.5418
Resetting 31486 PBs
Finished epoch 55 in 281.0 seconds
Perplexity training: 4.151
Measuring development set...
Recognition iteration 0 Loss 24.265
Recognition finished, iteration 100 Loss 0.054
Recognition iteration 0 Loss 23.875
Recognition finished, iteration 100 Loss 0.057
Recognition iteration 0 Loss 21.824
Recognition finished, iteration 100 Loss 0.037
Recognition iteration 0 Loss 24.155
Recognition finished, iteration 100 Loss 0.090
Perplexity dev: 141.585

==== Starting epoch 56 ====
  Batch 0 Loss 4.6490 Mono loss -1.0000
  Batch 100 Loss 4.1052 Mono loss 7.0840
  Batch 200 Loss 6.8331 Mono loss 8.3581
  Batch 300 Loss 5.2168 Mono loss 8.8807
  Batch 400 Loss 5.8015 Mono loss 9.4582
  Batch 500 Loss 4.9853 Mono loss 8.0173
  Batch 600 Loss 5.9498 Mono loss 6.8668
  Batch 700 Loss 6.7732 Mono loss 9.5698
Resetting 31530 PBs
Finished epoch 56 in 275.0 seconds
Perplexity training: 4.036

==== Starting epoch 57 ====
  Batch 0 Loss 5.9058 Mono loss 8.1999
  Batch 100 Loss 4.8859 Mono loss 8.1741
  Batch 200 Loss 6.0214 Mono loss 6.7737
  Batch 300 Loss 5.0921 Mono loss 7.0160
  Batch 400 Loss 6.4356 Mono loss 7.9313
  Batch 500 Loss 5.4952 Mono loss 8.0422
  Batch 600 Loss 5.0965 Mono loss 7.3308
  Batch 700 Loss 6.3036 Mono loss 9.3750
Resetting 31634 PBs
Finished epoch 57 in 309.0 seconds
Perplexity training: 4.043
Measuring development set...
Recognition iteration 0 Loss 24.527
Recognition finished, iteration 100 Loss 0.060
Recognition iteration 0 Loss 23.743
Recognition finished, iteration 100 Loss 0.055
Recognition iteration 0 Loss 21.952
Recognition finished, iteration 100 Loss 0.035
Recognition iteration 0 Loss 24.089
Recognition finished, iteration 100 Loss 0.104
Perplexity dev: 102.153

==== Starting epoch 58 ====
  Batch 0 Loss 6.6969 Mono loss 8.2482
  Batch 100 Loss 3.6840 Mono loss 8.2492
  Batch 200 Loss 5.7291 Mono loss 9.4719
  Batch 300 Loss 7.2210 Mono loss 8.0099
  Batch 400 Loss 6.2704 Mono loss 7.0062
  Batch 500 Loss 6.1363 Mono loss 9.4667
  Batch 600 Loss 5.6403 Mono loss 7.9191
  Batch 700 Loss 5.1349 Mono loss 9.1219
Resetting 31675 PBs
Finished epoch 58 in 292.0 seconds
Perplexity training: 4.053

==== Starting epoch 59 ====
  Batch 0 Loss 5.8015 Mono loss -1.0000
  Batch 100 Loss 4.3030 Mono loss 8.5526
  Batch 200 Loss 5.0798 Mono loss 8.3675
  Batch 300 Loss 6.5497 Mono loss 6.5986
  Batch 400 Loss 5.5021 Mono loss 9.4771
  Batch 500 Loss 6.5548 Mono loss 6.9943
  Batch 600 Loss 4.6939 Mono loss 9.4391
  Batch 700 Loss 6.9061 Mono loss 10.2647
Resetting 31471 PBs
Finished epoch 59 in 317.0 seconds
Perplexity training: 4.035
Measuring development set...
Recognition iteration 0 Loss 23.868
Recognition finished, iteration 100 Loss 0.044
Recognition iteration 0 Loss 23.552
Recognition finished, iteration 100 Loss 0.044
Recognition iteration 0 Loss 21.813
Recognition finished, iteration 100 Loss 0.034
Recognition iteration 0 Loss 23.832
Recognition finished, iteration 100 Loss 0.076
Perplexity dev: 143.500

==== Starting epoch 60 ====
  Batch 0 Loss 4.8839 Mono loss -1.0000
  Batch 100 Loss 4.6743 Mono loss 9.2050
  Batch 200 Loss 4.9062 Mono loss 8.5779
  Batch 300 Loss 5.3956 Mono loss 8.1478
  Batch 400 Loss 3.7120 Mono loss 9.3068
  Batch 500 Loss 7.4193 Mono loss 7.7176
  Batch 600 Loss 3.8874 Mono loss 9.1997
  Batch 700 Loss 6.0469 Mono loss 6.2402
Resetting 31902 PBs
Finished epoch 60 in 292.0 seconds
Perplexity training: 4.009

==== Starting epoch 61 ====
  Batch 0 Loss 2.9547 Mono loss -1.0000
  Batch 100 Loss 4.8967 Mono loss 8.6063
  Batch 200 Loss 5.3910 Mono loss 7.6335
  Batch 300 Loss 4.7462 Mono loss 7.3686
  Batch 400 Loss 4.2202 Mono loss 6.8424
  Batch 500 Loss 6.3918 Mono loss 7.9608
  Batch 600 Loss 4.5065 Mono loss 12.0663
  Batch 700 Loss 5.3381 Mono loss 11.6398
Resetting 31645 PBs
Finished epoch 61 in 316.0 seconds
Perplexity training: 3.974
Measuring development set...
Recognition iteration 0 Loss 24.422
Recognition finished, iteration 100 Loss 0.044
Recognition iteration 0 Loss 24.049
Recognition finished, iteration 100 Loss 0.037
Recognition iteration 0 Loss 21.802
Recognition finished, iteration 100 Loss 0.025
Recognition iteration 0 Loss 24.391
Recognition finished, iteration 100 Loss 0.075
Perplexity dev: 163.699

==== Starting epoch 62 ====
  Batch 0 Loss 4.6462 Mono loss -1.0000
  Batch 100 Loss 3.6246 Mono loss 7.0284
  Batch 200 Loss 4.7234 Mono loss 7.3128
  Batch 300 Loss 5.5570 Mono loss 7.6417
  Batch 400 Loss 3.7389 Mono loss 7.7053
  Batch 500 Loss 5.6381 Mono loss 6.9788
  Batch 600 Loss 3.9615 Mono loss 8.3404
  Batch 700 Loss 5.3666 Mono loss 9.9246
Resetting 31483 PBs
Finished epoch 62 in 320.0 seconds
Perplexity training: 4.022

==== Starting epoch 63 ====
  Batch 0 Loss 4.1779 Mono loss 7.3828
  Batch 100 Loss 4.1978 Mono loss 7.5510
  Batch 200 Loss 4.4452 Mono loss 8.6075
  Batch 300 Loss 5.6531 Mono loss 7.2899
  Batch 400 Loss 5.7496 Mono loss 8.9444
  Batch 500 Loss 6.2784 Mono loss 7.7152
  Batch 600 Loss 4.2614 Mono loss 7.7536
  Batch 700 Loss 4.4184 Mono loss 6.6227
Resetting 31458 PBs
Finished epoch 63 in 307.0 seconds
Perplexity training: 3.920
Measuring development set...
Recognition iteration 0 Loss 23.860
Recognition finished, iteration 100 Loss 0.036
Recognition iteration 0 Loss 23.603
Recognition finished, iteration 100 Loss 0.031
Recognition iteration 0 Loss 21.636
Recognition finished, iteration 100 Loss 0.027
Recognition iteration 0 Loss 23.825
Recognition finished, iteration 100 Loss 0.054
Perplexity dev: 131.096

==== Starting epoch 64 ====
  Batch 0 Loss 6.2566 Mono loss 9.0809
  Batch 100 Loss 2.9708 Mono loss 8.4000
  Batch 200 Loss 4.0335 Mono loss 8.0929
  Batch 300 Loss 4.9165 Mono loss 9.0252
  Batch 400 Loss 4.6662 Mono loss 8.5032
  Batch 500 Loss 6.4415 Mono loss 7.7896
  Batch 600 Loss 4.2847 Mono loss 8.3397
  Batch 700 Loss 5.8071 Mono loss 6.5836
Resetting 31348 PBs
Finished epoch 64 in 310.0 seconds
Perplexity training: 3.946

==== Starting epoch 65 ====
  Batch 0 Loss 6.7852 Mono loss 6.0777
  Batch 100 Loss 5.0353 Mono loss 9.1414
  Batch 200 Loss 4.9991 Mono loss 8.0311
  Batch 300 Loss 4.7685 Mono loss 6.8739
  Batch 400 Loss 4.9169 Mono loss 8.7043
  Batch 500 Loss 6.0130 Mono loss 8.0074
  Batch 600 Loss 5.3936 Mono loss 7.3867
  Batch 700 Loss 3.8596 Mono loss 8.7268
Resetting 31702 PBs
Finished epoch 65 in 304.0 seconds
Perplexity training: 3.966
Measuring development set...
Recognition iteration 0 Loss 24.552
Recognition finished, iteration 100 Loss 0.033
Recognition iteration 0 Loss 23.824
Recognition finished, iteration 100 Loss 0.026
Recognition iteration 0 Loss 22.173
Recognition finished, iteration 100 Loss 0.020
Recognition iteration 0 Loss 24.192
Recognition finished, iteration 100 Loss 0.051
Perplexity dev: 156.893

==== Starting epoch 66 ====
  Batch 0 Loss 5.2036 Mono loss -1.0000
  Batch 100 Loss 3.8591 Mono loss 6.6457
  Batch 200 Loss 5.9719 Mono loss 8.3245
  Batch 300 Loss 5.4437 Mono loss 6.5003
  Batch 400 Loss 6.7010 Mono loss 8.4997
  Batch 500 Loss 5.4780 Mono loss 8.0601
  Batch 600 Loss 5.6269 Mono loss 8.5239
  Batch 700 Loss 4.5085 Mono loss 5.1884
Resetting 31553 PBs
Finished epoch 66 in 316.0 seconds
Perplexity training: 3.927

==== Starting epoch 67 ====
  Batch 0 Loss 4.9799 Mono loss 7.8566
  Batch 100 Loss 4.1111 Mono loss 7.6616
  Batch 200 Loss 5.3064 Mono loss 8.7907
  Batch 300 Loss 7.1782 Mono loss 5.9473
  Batch 400 Loss 5.0322 Mono loss 6.7578
  Batch 500 Loss 6.2767 Mono loss 8.3498
  Batch 600 Loss 5.4389 Mono loss 8.3405
  Batch 700 Loss 5.5173 Mono loss 6.4715
Resetting 31294 PBs
Finished epoch 67 in 320.0 seconds
Perplexity training: 3.975
Measuring development set...
Recognition iteration 0 Loss 24.200
Recognition finished, iteration 100 Loss 0.032
Recognition iteration 0 Loss 23.774
Recognition finished, iteration 100 Loss 0.026
Recognition iteration 0 Loss 22.149
Recognition finished, iteration 100 Loss 0.021
Recognition iteration 0 Loss 24.150
Recognition finished, iteration 100 Loss 0.052
Perplexity dev: 157.978

==== Starting epoch 68 ====
  Batch 0 Loss 4.8264 Mono loss 8.7922
  Batch 100 Loss 4.3959 Mono loss 6.5527
  Batch 200 Loss 4.3329 Mono loss 7.2527
  Batch 300 Loss 5.6333 Mono loss 7.1327
  Batch 400 Loss 5.6049 Mono loss 7.5114
  Batch 500 Loss 5.6262 Mono loss 6.7177
  Batch 600 Loss 4.2202 Mono loss 7.0833
  Batch 700 Loss 6.5253 Mono loss 8.5843
Resetting 31407 PBs
Finished epoch 68 in 339.0 seconds
Perplexity training: 3.953

==== Starting epoch 69 ====
  Batch 0 Loss 4.6214 Mono loss 7.6670
  Batch 100 Loss 4.7535 Mono loss 7.2815
  Batch 200 Loss 5.2239 Mono loss 8.3776
  Batch 300 Loss 5.9896 Mono loss 7.1114
  Batch 400 Loss 5.9420 Mono loss 8.7358
  Batch 500 Loss 6.8110 Mono loss 8.4961
  Batch 600 Loss 5.0264 Mono loss 6.5337
  Batch 700 Loss 7.3593 Mono loss 7.7882
Resetting 31634 PBs
Finished epoch 69 in 329.0 seconds
Perplexity training: 3.904
Measuring development set...
Recognition iteration 0 Loss 23.548
Recognition finished, iteration 100 Loss 0.033
Recognition iteration 0 Loss 23.548
Recognition finished, iteration 100 Loss 0.023
Recognition iteration 0 Loss 21.643
Recognition finished, iteration 100 Loss 0.021
Recognition iteration 0 Loss 23.953
Recognition finished, iteration 100 Loss 0.046
Perplexity dev: 216.097

==== Starting epoch 70 ====
  Batch 0 Loss 2.8519 Mono loss 7.5036
  Batch 100 Loss 4.0337 Mono loss 8.2284
  Batch 200 Loss 5.6497 Mono loss 9.1597
  Batch 300 Loss 4.4595 Mono loss 6.7626
  Batch 400 Loss 4.8181 Mono loss 9.2026
  Batch 500 Loss 4.9219 Mono loss 7.2062
  Batch 600 Loss 4.2177 Mono loss 8.0813
  Batch 700 Loss 7.3114 Mono loss 7.7301
Resetting 31554 PBs
Finished epoch 70 in 315.0 seconds
Perplexity training: 3.861

==== Starting epoch 71 ====
  Batch 0 Loss 4.6409 Mono loss -1.0000
  Batch 100 Loss 4.6054 Mono loss 6.8303
  Batch 200 Loss 5.4116 Mono loss 5.8175
  Batch 300 Loss 4.8331 Mono loss 7.5382
  Batch 400 Loss 7.2937 Mono loss 7.1194
  Batch 500 Loss 5.1426 Mono loss 7.7777
  Batch 600 Loss 5.4205 Mono loss 6.9769
  Batch 700 Loss 6.5270 Mono loss 8.1921
Resetting 31555 PBs
Finished epoch 71 in 323.0 seconds
Perplexity training: 3.974
Measuring development set...
Recognition iteration 0 Loss 24.265
Recognition finished, iteration 100 Loss 0.026
Recognition iteration 0 Loss 23.897
Recognition finished, iteration 100 Loss 0.025
Recognition iteration 0 Loss 21.978
Recognition finished, iteration 100 Loss 0.017
Recognition iteration 0 Loss 24.014
Recognition finished, iteration 100 Loss 0.041
Perplexity dev: 120.091

==== Starting epoch 72 ====
  Batch 0 Loss 4.0664 Mono loss 8.0199
  Batch 100 Loss 4.9634 Mono loss 7.2419
  Batch 200 Loss 6.9675 Mono loss 7.9114
  Batch 300 Loss 4.9398 Mono loss 7.7163
  Batch 400 Loss 5.8254 Mono loss 7.6300
  Batch 500 Loss 5.4365 Mono loss 7.7644
  Batch 600 Loss 4.4009 Mono loss 7.9151
  Batch 700 Loss 6.0927 Mono loss 7.2797
Resetting 31553 PBs
Finished epoch 72 in 318.0 seconds
Perplexity training: 3.876

==== Starting epoch 73 ====
  Batch 0 Loss 4.8752 Mono loss 5.5341
  Batch 100 Loss 5.3930 Mono loss 6.4187
  Batch 200 Loss 4.9442 Mono loss 6.1062
  Batch 300 Loss 5.0996 Mono loss 7.6447
  Batch 400 Loss 7.0074 Mono loss 7.6619
  Batch 500 Loss 6.3252 Mono loss 8.8324
  Batch 600 Loss 4.1264 Mono loss 6.6717
  Batch 700 Loss 5.4504 Mono loss 9.1137
Resetting 31675 PBs
Finished epoch 73 in 317.0 seconds
Perplexity training: 3.875
Measuring development set...
Recognition iteration 0 Loss 23.682
Recognition finished, iteration 100 Loss 0.022
Recognition iteration 0 Loss 23.648
Recognition finished, iteration 100 Loss 0.020
Recognition iteration 0 Loss 21.643
Recognition finished, iteration 100 Loss 0.015
Recognition iteration 0 Loss 23.833
Recognition finished, iteration 100 Loss 0.035
Perplexity dev: 148.824

==== Starting epoch 74 ====
  Batch 0 Loss 5.0572 Mono loss 6.0279
  Batch 100 Loss 4.4613 Mono loss 5.4298
  Batch 200 Loss 4.7373 Mono loss 6.6814
  Batch 300 Loss 6.4740 Mono loss 6.0305
  Batch 400 Loss 7.2706 Mono loss 8.7641
  Batch 500 Loss 6.6196 Mono loss 6.3970
  Batch 600 Loss 4.2308 Mono loss 5.7086
  Batch 700 Loss 5.2991 Mono loss 6.9174
Resetting 31423 PBs
Finished epoch 74 in 309.0 seconds
Perplexity training: 3.837

==== Starting epoch 75 ====
  Batch 0 Loss 5.9549 Mono loss 8.5411
  Batch 100 Loss 4.0488 Mono loss 7.3133
  Batch 200 Loss 6.6601 Mono loss 8.6764
  Batch 300 Loss 5.8032 Mono loss 5.7294
  Batch 400 Loss 4.3832 Mono loss 5.8020
  Batch 500 Loss 4.7854 Mono loss 7.0829
  Batch 600 Loss 4.8785 Mono loss 6.2433
  Batch 700 Loss 5.3543 Mono loss 11.3498
Resetting 31606 PBs
Finished epoch 75 in 326.0 seconds
Perplexity training: 3.810
Measuring development set...
Recognition iteration 0 Loss 24.311
Recognition finished, iteration 100 Loss 0.023
Recognition iteration 0 Loss 24.177
Recognition finished, iteration 100 Loss 0.020
Recognition iteration 0 Loss 22.376
Recognition finished, iteration 100 Loss 0.014
Recognition iteration 0 Loss 24.319
Recognition finished, iteration 100 Loss 0.035
Perplexity dev: 165.589

==== Starting epoch 76 ====
  Batch 0 Loss 4.1770 Mono loss -1.0000
  Batch 100 Loss 3.7251 Mono loss 6.9658
  Batch 200 Loss 7.1157 Mono loss 6.0823
  Batch 300 Loss 5.9498 Mono loss 7.5488
  Batch 400 Loss 4.3196 Mono loss 5.4239
  Batch 500 Loss 5.7223 Mono loss 9.0506
  Batch 600 Loss 4.7428 Mono loss 6.5637
  Batch 700 Loss 4.7658 Mono loss 6.6473
Resetting 31262 PBs
Finished epoch 76 in 305.0 seconds
Perplexity training: 3.817

==== Starting epoch 77 ====
  Batch 0 Loss 3.9427 Mono loss -1.0000
  Batch 100 Loss 3.9169 Mono loss 7.1496
  Batch 200 Loss 6.0759 Mono loss 8.0566
  Batch 300 Loss 6.1046 Mono loss 8.3572
  Batch 400 Loss 5.0613 Mono loss 6.7829
  Batch 500 Loss 5.3521 Mono loss 7.1737
  Batch 600 Loss 3.8422 Mono loss 5.9729
  Batch 700 Loss 4.5581 Mono loss 7.6840
Resetting 31299 PBs
Finished epoch 77 in 323.0 seconds
Perplexity training: 3.767
Measuring development set...
Recognition iteration 0 Loss 23.459
Recognition finished, iteration 100 Loss 0.019
Recognition iteration 0 Loss 23.300
Recognition finished, iteration 100 Loss 0.017
Recognition iteration 0 Loss 21.629
Recognition finished, iteration 100 Loss 0.013
Recognition iteration 0 Loss 23.634
Recognition finished, iteration 100 Loss 0.032
Perplexity dev: 123.543

==== Starting epoch 78 ====
  Batch 0 Loss 3.9781 Mono loss -1.0000
  Batch 100 Loss 3.6333 Mono loss 5.5987
  Batch 200 Loss 5.6179 Mono loss 7.8342
  Batch 300 Loss 5.7861 Mono loss 7.3554
  Batch 400 Loss 5.1834 Mono loss 7.4126
  Batch 500 Loss 4.7215 Mono loss 6.2093
  Batch 600 Loss 5.2969 Mono loss 7.1843
  Batch 700 Loss 4.8031 Mono loss 6.1093
Resetting 31540 PBs
Finished epoch 78 in 310.0 seconds
Perplexity training: 3.712

==== Starting epoch 79 ====
  Batch 0 Loss 3.8225 Mono loss -1.0000
  Batch 100 Loss 4.9727 Mono loss 6.0586
  Batch 200 Loss 5.5178 Mono loss 7.3234
  Batch 300 Loss 6.7781 Mono loss 7.2118
  Batch 400 Loss 4.7810 Mono loss 8.3496
  Batch 500 Loss 5.1023 Mono loss 6.9813
  Batch 600 Loss 5.0056 Mono loss 6.6543
  Batch 700 Loss 4.0980 Mono loss 7.5683
Resetting 31559 PBs
Finished epoch 79 in 327.0 seconds
Perplexity training: 3.784
Measuring development set...
Recognition iteration 0 Loss 23.415
Recognition finished, iteration 100 Loss 0.019
Recognition iteration 0 Loss 23.042
Recognition finished, iteration 100 Loss 0.016
Recognition iteration 0 Loss 21.686
Recognition finished, iteration 100 Loss 0.013
Recognition iteration 0 Loss 23.724
Recognition finished, iteration 100 Loss 0.029
Perplexity dev: 151.463

==== Starting epoch 80 ====
  Batch 0 Loss 5.0855 Mono loss -1.0000
  Batch 100 Loss 3.1929 Mono loss 6.5379
  Batch 200 Loss 4.9688 Mono loss 7.1929
  Batch 300 Loss 4.7791 Mono loss 8.6465
  Batch 400 Loss 5.6964 Mono loss 5.4146
  Batch 500 Loss 5.4470 Mono loss 8.0573
  Batch 600 Loss 5.9407 Mono loss 6.9162
  Batch 700 Loss 5.6426 Mono loss 5.4007
Resetting 31246 PBs
Finished epoch 80 in 328.0 seconds
Perplexity training: 3.746

==== Starting epoch 81 ====
  Batch 0 Loss 4.4420 Mono loss -1.0000
  Batch 100 Loss 4.8982 Mono loss 7.6301
  Batch 200 Loss 5.2135 Mono loss 6.6526
  Batch 300 Loss 4.0110 Mono loss 5.4097
  Batch 400 Loss 6.1374 Mono loss 6.6076
  Batch 500 Loss 7.7258 Mono loss 5.4006
  Batch 600 Loss 4.7385 Mono loss 7.3747
  Batch 700 Loss 5.0212 Mono loss 8.8458
Resetting 31264 PBs
Finished epoch 81 in 328.0 seconds
Perplexity training: 3.746
Measuring development set...
Recognition iteration 0 Loss 23.291
Recognition finished, iteration 100 Loss 0.015
Recognition iteration 0 Loss 23.283
Recognition finished, iteration 100 Loss 0.013
Recognition iteration 0 Loss 21.718
Recognition finished, iteration 100 Loss 0.011
Recognition iteration 0 Loss 23.681
Recognition finished, iteration 100 Loss 0.028
Perplexity dev: 234.983

==== Starting epoch 82 ====
  Batch 0 Loss 4.1321 Mono loss -1.0000
  Batch 100 Loss 3.0697 Mono loss 5.7133
  Batch 200 Loss 4.0781 Mono loss 6.7785
  Batch 300 Loss 4.2220 Mono loss 6.4344
  Batch 400 Loss 3.8075 Mono loss 7.1276
  Batch 500 Loss 5.1586 Mono loss 7.2607
  Batch 600 Loss 4.9117 Mono loss 6.4971
  Batch 700 Loss 5.5617 Mono loss 7.8629
Resetting 31661 PBs
Finished epoch 82 in 338.0 seconds
Perplexity training: 3.695

==== Starting epoch 83 ====
  Batch 0 Loss 3.6134 Mono loss 7.8289
  Batch 100 Loss 3.5032 Mono loss 5.8986
  Batch 200 Loss 4.8443 Mono loss 5.1874
  Batch 300 Loss 3.6455 Mono loss 8.4638
  Batch 400 Loss 5.7711 Mono loss 5.4816
  Batch 500 Loss 4.0974 Mono loss 9.3570
  Batch 600 Loss 2.7982 Mono loss 7.3704
  Batch 700 Loss 4.6626 Mono loss 7.5238
Resetting 31507 PBs
Finished epoch 83 in 333.0 seconds
Perplexity training: 3.797
Measuring development set...
Recognition iteration 0 Loss 23.254
Recognition finished, iteration 100 Loss 0.015
Recognition iteration 0 Loss 22.974
Recognition finished, iteration 100 Loss 0.014
Recognition iteration 0 Loss 21.472
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 23.561
Recognition finished, iteration 100 Loss 0.027
Perplexity dev: 144.277

==== Starting epoch 84 ====
  Batch 0 Loss 3.8865 Mono loss 7.4260
  Batch 100 Loss 4.2704 Mono loss 6.7232
  Batch 200 Loss 3.3848 Mono loss 7.9831
  Batch 300 Loss 4.4670 Mono loss 7.2972
  Batch 400 Loss 5.0566 Mono loss 7.6506
  Batch 500 Loss 8.3172 Mono loss 8.3017
  Batch 600 Loss 3.1068 Mono loss 5.4011
  Batch 700 Loss 4.4191 Mono loss 8.5447
Resetting 32022 PBs
Finished epoch 84 in 340.0 seconds
Perplexity training: 3.773

==== Starting epoch 85 ====
  Batch 0 Loss 4.1895 Mono loss 7.7335
  Batch 100 Loss 3.6097 Mono loss 8.5345
  Batch 200 Loss 5.4640 Mono loss 8.3775
  Batch 300 Loss 4.9119 Mono loss 8.5734
  Batch 400 Loss 4.5498 Mono loss 6.1684
  Batch 500 Loss 6.4586 Mono loss 7.6771
  Batch 600 Loss 4.3758 Mono loss 5.0882
  Batch 700 Loss 4.5787 Mono loss 8.2627
Resetting 31564 PBs
Finished epoch 85 in 340.0 seconds
Perplexity training: 3.767
Measuring development set...
Recognition iteration 0 Loss 23.520
Recognition finished, iteration 100 Loss 0.014
Recognition iteration 0 Loss 23.225
Recognition finished, iteration 100 Loss 0.013
Recognition iteration 0 Loss 21.644
Recognition finished, iteration 100 Loss 0.011
Recognition iteration 0 Loss 23.790
Recognition finished, iteration 100 Loss 0.026
Perplexity dev: 158.925

==== Starting epoch 86 ====
  Batch 0 Loss 4.6533 Mono loss -1.0000
  Batch 100 Loss 3.6108 Mono loss 7.1861
  Batch 200 Loss 6.1816 Mono loss 9.0236
  Batch 300 Loss 5.0918 Mono loss 6.0764
  Batch 400 Loss 4.9924 Mono loss 8.7991
  Batch 500 Loss 6.6694 Mono loss 4.5413
  Batch 600 Loss 3.9923 Mono loss 9.4750
  Batch 700 Loss 5.2275 Mono loss 7.2653
Resetting 31549 PBs
Finished epoch 86 in 345.0 seconds
Perplexity training: 3.788

==== Starting epoch 87 ====
  Batch 0 Loss 4.8231 Mono loss -1.0000
  Batch 100 Loss 4.0401 Mono loss 7.8860
  Batch 200 Loss 5.6645 Mono loss 6.9719
  Batch 300 Loss 5.6402 Mono loss 6.4187
  Batch 400 Loss 3.5350 Mono loss 8.4024
  Batch 500 Loss 7.0430 Mono loss 7.8129
  Batch 600 Loss 3.4315 Mono loss 8.9203
  Batch 700 Loss 4.2481 Mono loss 6.9707
Resetting 31826 PBs
Finished epoch 87 in 357.0 seconds
Perplexity training: 3.716
Measuring development set...
Recognition iteration 0 Loss 24.129
Recognition finished, iteration 100 Loss 0.014
Recognition iteration 0 Loss 23.826
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 22.151
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 24.021
Recognition finished, iteration 100 Loss 0.026
Perplexity dev: 246.889

==== Starting epoch 88 ====
  Batch 0 Loss 4.1415 Mono loss -1.0000
  Batch 100 Loss 3.9677 Mono loss 5.9680
  Batch 200 Loss 4.8186 Mono loss 6.4516
  Batch 300 Loss 4.1839 Mono loss 6.3385
  Batch 400 Loss 5.6451 Mono loss 7.1979
  Batch 500 Loss 5.7745 Mono loss 7.9035
  Batch 600 Loss 4.1237 Mono loss 6.9343
  Batch 700 Loss 3.9068 Mono loss 6.1190
Resetting 31671 PBs
Finished epoch 88 in 337.0 seconds
Perplexity training: 3.677

==== Starting epoch 89 ====
  Batch 0 Loss 3.8250 Mono loss 6.9167
  Batch 100 Loss 3.1542 Mono loss 5.7569
  Batch 200 Loss 7.7873 Mono loss 7.4558
  Batch 300 Loss 5.2447 Mono loss 6.1203
  Batch 400 Loss 4.4750 Mono loss 6.9708
  Batch 500 Loss 6.5992 Mono loss 6.1090
  Batch 600 Loss 3.9580 Mono loss 6.2273
  Batch 700 Loss 4.0857 Mono loss 6.5171
Resetting 31732 PBs
Finished epoch 89 in 348.0 seconds
Perplexity training: 3.730
Measuring development set...
Recognition iteration 0 Loss 23.514
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 23.007
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 21.791
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 23.716
Recognition finished, iteration 100 Loss 0.021
Perplexity dev: 173.419

==== Starting epoch 90 ====
  Batch 0 Loss 2.9576 Mono loss 7.6874
  Batch 100 Loss 3.9183 Mono loss 5.8178
  Batch 200 Loss 4.7049 Mono loss 6.9596
  Batch 300 Loss 5.0884 Mono loss 8.0753
  Batch 400 Loss 5.8516 Mono loss 6.9611
  Batch 500 Loss 5.9773 Mono loss 7.2335
  Batch 600 Loss 3.1040 Mono loss 5.7511
  Batch 700 Loss 4.8286 Mono loss 7.2567
Resetting 31662 PBs
Finished epoch 90 in 346.0 seconds
Perplexity training: 3.715

==== Starting epoch 91 ====
  Batch 0 Loss 4.1802 Mono loss 7.1717
  Batch 100 Loss 3.5391 Mono loss 8.5206
  Batch 200 Loss 4.1935 Mono loss 4.4078
  Batch 300 Loss 4.9114 Mono loss 6.3361
  Batch 400 Loss 4.1740 Mono loss 5.8482
  Batch 500 Loss 6.3827 Mono loss 7.1664
  Batch 600 Loss 4.9242 Mono loss 5.4435
  Batch 700 Loss 5.2278 Mono loss 6.1605
Resetting 31347 PBs
Finished epoch 91 in 355.0 seconds
Perplexity training: 3.779
Measuring development set...
Recognition iteration 0 Loss 23.702
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 23.130
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 21.580
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 23.665
Recognition finished, iteration 100 Loss 0.024
Perplexity dev: 140.274

==== Starting epoch 92 ====
  Batch 0 Loss 4.3942 Mono loss 7.6517
  Batch 100 Loss 5.0761 Mono loss 5.0692
  Batch 200 Loss 4.3676 Mono loss 7.4780
  Batch 300 Loss 4.7764 Mono loss 7.5613
  Batch 400 Loss 5.0967 Mono loss 5.0634
  Batch 500 Loss 3.7382 Mono loss 5.7344
  Batch 600 Loss 3.8293 Mono loss 7.7532
  Batch 700 Loss 6.8360 Mono loss 7.8751
Resetting 31439 PBs
Finished epoch 92 in 366.0 seconds
Perplexity training: 3.700

==== Starting epoch 93 ====
  Batch 0 Loss 4.0442 Mono loss 5.8255
  Batch 100 Loss 5.3813 Mono loss 5.4776
  Batch 200 Loss 6.1743 Mono loss 7.4146
  Batch 300 Loss 5.4642 Mono loss 6.6230
  Batch 400 Loss 4.5325 Mono loss 6.4201
  Batch 500 Loss 4.3956 Mono loss 5.1772
  Batch 600 Loss 3.8892 Mono loss 8.5876
  Batch 700 Loss 6.2953 Mono loss 6.2152
Resetting 31614 PBs
Finished epoch 93 in 360.0 seconds
Perplexity training: 3.702
Measuring development set...
Recognition iteration 0 Loss 23.255
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 22.749
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 21.305
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 23.538
Recognition finished, iteration 100 Loss 0.021
Perplexity dev: 225.292

==== Starting epoch 94 ====
  Batch 0 Loss 4.1287 Mono loss 6.5891
  Batch 100 Loss 3.7775 Mono loss 7.8043
  Batch 200 Loss 4.8555 Mono loss 6.9158
  Batch 300 Loss 5.1432 Mono loss 8.3711
  Batch 400 Loss 5.0211 Mono loss 5.9231
  Batch 500 Loss 4.5453 Mono loss 7.1820
  Batch 600 Loss 5.3322 Mono loss 5.7509
  Batch 700 Loss 5.8659 Mono loss 6.7370
Resetting 31429 PBs
Finished epoch 94 in 356.0 seconds
Perplexity training: 3.624

==== Starting epoch 95 ====
  Batch 0 Loss 4.2809 Mono loss 6.7706
  Batch 100 Loss 3.3759 Mono loss 5.7198
  Batch 200 Loss 5.1527 Mono loss 6.7178
  Batch 300 Loss 4.6691 Mono loss 7.7322
  Batch 400 Loss 3.7500 Mono loss 5.4169
  Batch 500 Loss 6.3698 Mono loss 7.5483
  Batch 600 Loss 4.6398 Mono loss 7.0213
  Batch 700 Loss 4.5658 Mono loss 7.3048
Resetting 31633 PBs
Finished epoch 95 in 343.0 seconds
Perplexity training: 3.577
Measuring development set...
Recognition iteration 0 Loss 23.752
Recognition finished, iteration 100 Loss 0.011
Recognition iteration 0 Loss 22.928
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 21.553
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 23.600
Recognition finished, iteration 100 Loss 0.017
Perplexity dev: 159.937

==== Starting epoch 96 ====
  Batch 0 Loss 3.9879 Mono loss -1.0000
  Batch 100 Loss 5.1592 Mono loss 7.5404
  Batch 200 Loss 4.5647 Mono loss 6.9324
  Batch 300 Loss 5.0413 Mono loss 4.1678
  Batch 400 Loss 3.5221 Mono loss 6.8532
  Batch 500 Loss 4.8042 Mono loss 7.5962
  Batch 600 Loss 4.4626 Mono loss 5.7846
  Batch 700 Loss 6.2628 Mono loss 8.0307
Resetting 31423 PBs
Finished epoch 96 in 340.0 seconds
Perplexity training: 3.644

==== Starting epoch 97 ====
  Batch 0 Loss 3.0542 Mono loss -1.0000
  Batch 100 Loss 3.0844 Mono loss 4.2047
  Batch 200 Loss 4.1289 Mono loss 5.8111
  Batch 300 Loss 7.1070 Mono loss 5.8859
  Batch 400 Loss 4.9847 Mono loss 6.5819
  Batch 500 Loss 5.4205 Mono loss 5.9951
  Batch 600 Loss 4.7628 Mono loss 6.0927
  Batch 700 Loss 4.8089 Mono loss 10.2526
Resetting 31513 PBs
Finished epoch 97 in 376.0 seconds
Perplexity training: 3.702
Measuring development set...
Recognition iteration 0 Loss 23.906
Recognition finished, iteration 100 Loss 0.012
Recognition iteration 0 Loss 23.492
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 21.930
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 24.065
Recognition finished, iteration 100 Loss 0.019
Perplexity dev: 159.155

==== Starting epoch 98 ====
  Batch 0 Loss 5.3442 Mono loss -1.0000
  Batch 100 Loss 2.7789 Mono loss 6.8369
  Batch 200 Loss 4.1847 Mono loss 6.8818
  Batch 300 Loss 5.0228 Mono loss 5.8791
  Batch 400 Loss 5.3880 Mono loss 7.8509
  Batch 500 Loss 4.0246 Mono loss 8.1752
  Batch 600 Loss 5.5401 Mono loss 7.4894
  Batch 700 Loss 6.3395 Mono loss 6.9153
Resetting 31346 PBs
Finished epoch 98 in 364.0 seconds
Perplexity training: 3.738

==== Starting epoch 99 ====
  Batch 0 Loss 5.3195 Mono loss -1.0000
  Batch 100 Loss 2.8902 Mono loss 6.9079
  Batch 200 Loss 6.3905 Mono loss 6.3130
  Batch 300 Loss 5.1661 Mono loss 6.5926
  Batch 400 Loss 4.8284 Mono loss 6.0069
  Batch 500 Loss 6.5524 Mono loss 6.1791
  Batch 600 Loss 4.0182 Mono loss 5.8656
  Batch 700 Loss 5.3334 Mono loss 7.4745
Resetting 31662 PBs
Finished epoch 99 in 360.0 seconds
Perplexity training: 3.674
Measuring development set...
Recognition iteration 0 Loss 23.369
Recognition finished, iteration 100 Loss 0.010
Recognition iteration 0 Loss 23.199
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 21.631
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.452
Recognition finished, iteration 100 Loss 0.015
Perplexity dev: 217.294

==== Starting epoch 100 ====
  Batch 0 Loss 5.0683 Mono loss 6.7000
  Batch 100 Loss 2.2587 Mono loss 5.7107
  Batch 200 Loss 5.5561 Mono loss 6.2523
  Batch 300 Loss 6.6029 Mono loss 6.3945
  Batch 400 Loss 5.1593 Mono loss 6.9001
  Batch 500 Loss 5.7601 Mono loss 7.0854
  Batch 600 Loss 3.1260 Mono loss 7.3320
  Batch 700 Loss 4.6565 Mono loss 6.7979
Resetting 31969 PBs
Finished epoch 100 in 370.0 seconds
Perplexity training: 3.611

==== Starting epoch 101 ====
  Batch 0 Loss 4.4092 Mono loss -1.0000
  Batch 100 Loss 4.8734 Mono loss 5.1299
  Batch 200 Loss 4.2328 Mono loss 5.4354
  Batch 300 Loss 5.7359 Mono loss 6.2743
  Batch 400 Loss 4.2370 Mono loss 5.6189
  Batch 500 Loss 4.0324 Mono loss 6.1128
  Batch 600 Loss 4.1607 Mono loss 7.2316
  Batch 700 Loss 4.2881 Mono loss 6.5422
Resetting 31467 PBs
Finished epoch 101 in 371.0 seconds
Perplexity training: 3.658
Measuring development set...
Recognition iteration 0 Loss 22.979
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 23.121
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 21.507
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.397
Recognition finished, iteration 100 Loss 0.015
Perplexity dev: 171.753

==== Starting epoch 102 ====
  Batch 0 Loss 3.5556 Mono loss 8.1535
  Batch 100 Loss 4.5415 Mono loss 7.1413
  Batch 200 Loss 3.9173 Mono loss 4.9923
  Batch 300 Loss 4.1687 Mono loss 6.6440
  Batch 400 Loss 3.5935 Mono loss 5.2699
  Batch 500 Loss 5.2047 Mono loss 7.2091
  Batch 600 Loss 3.6320 Mono loss 5.3085
  Batch 700 Loss 4.5177 Mono loss 9.4027
Resetting 31668 PBs
Finished epoch 102 in 368.0 seconds
Perplexity training: 3.635

==== Starting epoch 103 ====
  Batch 0 Loss 3.5084 Mono loss 7.2016
  Batch 100 Loss 3.9589 Mono loss 5.2377
  Batch 200 Loss 5.3778 Mono loss 6.7977
  Batch 300 Loss 3.4540 Mono loss 6.5978
  Batch 400 Loss 4.8908 Mono loss 8.0353
  Batch 500 Loss 4.8851 Mono loss 8.9535
  Batch 600 Loss 5.2947 Mono loss 7.4237
  Batch 700 Loss 3.8124 Mono loss 6.9991
Resetting 31516 PBs
Finished epoch 103 in 390.0 seconds
Perplexity training: 3.636
Measuring development set...
Recognition iteration 0 Loss 24.087
Recognition finished, iteration 100 Loss 0.009
Recognition iteration 0 Loss 23.817
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 21.871
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 24.024
Recognition finished, iteration 100 Loss 0.016
Perplexity dev: 163.730

==== Starting epoch 104 ====
  Batch 0 Loss 3.1183 Mono loss 6.4066
  Batch 100 Loss 3.5190 Mono loss 4.7157
  Batch 200 Loss 4.7479 Mono loss 7.1930
  Batch 300 Loss 3.2536 Mono loss 6.4763
  Batch 400 Loss 5.1380 Mono loss 8.4906
  Batch 500 Loss 7.0681 Mono loss 6.7437
  Batch 600 Loss 4.8153 Mono loss 6.2600
  Batch 700 Loss 3.7694 Mono loss 7.2800
Resetting 31480 PBs
Finished epoch 104 in 374.0 seconds
Perplexity training: 3.657

==== Starting epoch 105 ====
  Batch 0 Loss 4.6917 Mono loss -1.0000
  Batch 100 Loss 4.7911 Mono loss 7.4035
  Batch 200 Loss 4.5022 Mono loss 8.1951
  Batch 300 Loss 4.1610 Mono loss 6.5832
  Batch 400 Loss 4.7652 Mono loss 6.3160
  Batch 500 Loss 4.7196 Mono loss 6.7709
  Batch 600 Loss 4.8328 Mono loss 7.6570
  Batch 700 Loss 4.5310 Mono loss 8.5685
Resetting 31627 PBs
Finished epoch 105 in 361.0 seconds
Perplexity training: 3.577
Measuring development set...
Recognition iteration 0 Loss 23.598
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 22.971
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 21.686
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.607
Recognition finished, iteration 100 Loss 0.014
Perplexity dev: 190.901

==== Starting epoch 106 ====
  Batch 0 Loss 4.2453 Mono loss -1.0000
  Batch 100 Loss 4.0851 Mono loss 5.8403
  Batch 200 Loss 4.1262 Mono loss 7.5346
  Batch 300 Loss 5.5470 Mono loss 5.3334
  Batch 400 Loss 5.3369 Mono loss 6.5510
  Batch 500 Loss 4.6421 Mono loss 5.9134
  Batch 600 Loss 3.8283 Mono loss 6.9919
  Batch 700 Loss 3.7741 Mono loss 5.0730
Resetting 31441 PBs
Finished epoch 106 in 580.0 seconds
Perplexity training: 3.679

==== Starting epoch 107 ====
  Batch 0 Loss 4.0039 Mono loss -1.0000
  Batch 100 Loss 3.3520 Mono loss 7.5711
  Batch 200 Loss 3.7829 Mono loss 7.5661
  Batch 300 Loss 4.5538 Mono loss 6.8435
  Batch 400 Loss 4.2425 Mono loss 7.0547
  Batch 500 Loss 3.1431 Mono loss 6.2785
  Batch 600 Loss 4.7991 Mono loss 6.7015
  Batch 700 Loss 4.1475 Mono loss 7.5415
Resetting 31462 PBs
Finished epoch 107 in 427.0 seconds
Perplexity training: 3.700
Measuring development set...
Recognition iteration 0 Loss 23.549
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 23.150
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 21.500
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.685
Recognition finished, iteration 100 Loss 0.013
Perplexity dev: 134.376

==== Starting epoch 108 ====
  Batch 0 Loss 3.6786 Mono loss -1.0000
  Batch 100 Loss 2.5900 Mono loss 6.8081
  Batch 200 Loss 5.5955 Mono loss 6.5293
  Batch 300 Loss 3.1447 Mono loss 7.6804
  Batch 400 Loss 4.9144 Mono loss 5.7937
  Batch 500 Loss 5.6412 Mono loss 6.4860
  Batch 600 Loss 3.7261 Mono loss 6.9505
  Batch 700 Loss 4.3274 Mono loss 6.6630
Resetting 31747 PBs
Finished epoch 108 in 408.0 seconds
Perplexity training: 3.567

==== Starting epoch 109 ====
  Batch 0 Loss 3.6742 Mono loss 6.1837
  Batch 100 Loss 4.3223 Mono loss 4.8724
  Batch 200 Loss 4.8228 Mono loss 5.3473
  Batch 300 Loss 4.5108 Mono loss 5.7504
  Batch 400 Loss 5.0274 Mono loss 5.9124
  Batch 500 Loss 4.7341 Mono loss 6.3509
  Batch 600 Loss 4.0124 Mono loss 6.5503
  Batch 700 Loss 3.7525 Mono loss 9.2788
Resetting 31241 PBs
Finished epoch 109 in 443.0 seconds
Perplexity training: 3.651
Measuring development set...
Recognition iteration 0 Loss 23.684
Recognition finished, iteration 100 Loss 0.008
Recognition iteration 0 Loss 23.278
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 21.655
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 23.826
Recognition finished, iteration 100 Loss 0.014
Perplexity dev: 149.179

==== Starting epoch 110 ====
  Batch 0 Loss 2.8103 Mono loss 6.9203
  Batch 100 Loss 3.2379 Mono loss 8.6431
  Batch 200 Loss 3.3110 Mono loss 6.5938
  Batch 300 Loss 5.0783 Mono loss 5.4528
  Batch 400 Loss 3.6527 Mono loss 5.9626
  Batch 500 Loss 5.2823 Mono loss 6.3909
  Batch 600 Loss 3.2781 Mono loss 5.7531
  Batch 700 Loss 4.3152 Mono loss 6.1621
Resetting 32042 PBs
Finished epoch 110 in 425.0 seconds
Perplexity training: 3.645

==== Starting epoch 111 ====
  Batch 0 Loss 3.8882 Mono loss 5.2288
  Batch 100 Loss 3.8405 Mono loss 6.5202
  Batch 200 Loss 3.5336 Mono loss 7.0898
  Batch 300 Loss 3.9775 Mono loss 6.3477
  Batch 400 Loss 3.2876 Mono loss 6.6318
  Batch 500 Loss 4.9251 Mono loss 7.2276
  Batch 600 Loss 3.8778 Mono loss 6.4860
  Batch 700 Loss 4.6296 Mono loss 7.5024
Resetting 31662 PBs
Finished epoch 111 in 441.0 seconds
Perplexity training: 3.605
Measuring development set...
Recognition iteration 0 Loss 23.259
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 22.936
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 21.587
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 23.638
Recognition finished, iteration 100 Loss 0.012
Perplexity dev: 174.807

==== Starting epoch 112 ====
  Batch 0 Loss 2.1307 Mono loss 8.4850
  Batch 100 Loss 3.9078 Mono loss 6.1858
  Batch 200 Loss 3.5107 Mono loss 7.7117
  Batch 300 Loss 3.8948 Mono loss 7.6683
  Batch 400 Loss 3.6130 Mono loss 6.0189
  Batch 500 Loss 5.2737 Mono loss 6.8796
  Batch 600 Loss 4.0680 Mono loss 6.4215
  Batch 700 Loss 4.3754 Mono loss 5.7844
Resetting 31596 PBs
Finished epoch 112 in 422.0 seconds
Perplexity training: 3.524

==== Starting epoch 113 ====
  Batch 0 Loss 3.2783 Mono loss 5.5035
  Batch 100 Loss 3.8864 Mono loss 7.7284
  Batch 200 Loss 4.3031 Mono loss 7.9482
  Batch 300 Loss 4.4853 Mono loss 8.2706
  Batch 400 Loss 3.1717 Mono loss 5.6143
  Batch 500 Loss 5.8644 Mono loss 5.7520
  Batch 600 Loss 3.7348 Mono loss 7.0426
  Batch 700 Loss 5.6358 Mono loss 5.9622
Resetting 31300 PBs
Finished epoch 113 in 425.0 seconds
Perplexity training: 3.508
Measuring development set...
Recognition iteration 0 Loss 23.411
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 22.929
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 21.508
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 23.530
Recognition finished, iteration 100 Loss 0.012
Perplexity dev: 180.891

==== Starting epoch 114 ====
  Batch 0 Loss 2.3149 Mono loss -1.0000
  Batch 100 Loss 4.3004 Mono loss 6.8070
  Batch 200 Loss 4.5299 Mono loss 5.6365
  Batch 300 Loss 4.9178 Mono loss 7.0427
  Batch 400 Loss 4.0198 Mono loss 7.9999
  Batch 500 Loss 5.2224 Mono loss 8.3279
  Batch 600 Loss 3.0791 Mono loss 4.3773
  Batch 700 Loss 5.3478 Mono loss 5.3896
Resetting 31556 PBs
Finished epoch 114 in 411.0 seconds
Perplexity training: 3.522

==== Starting epoch 115 ====
  Batch 0 Loss 4.1844 Mono loss 6.2169
  Batch 100 Loss 4.2641 Mono loss 5.2246
  Batch 200 Loss 5.1534 Mono loss 8.6323
  Batch 300 Loss 4.4708 Mono loss 7.8138
  Batch 400 Loss 4.7908 Mono loss 6.0033
  Batch 500 Loss 4.8848 Mono loss 6.4897
  Batch 600 Loss 1.9616 Mono loss 6.8402
  Batch 700 Loss 5.1131 Mono loss 6.1441
Resetting 31163 PBs
Finished epoch 115 in 410.0 seconds
Perplexity training: 3.571
Measuring development set...
Recognition iteration 0 Loss 23.479
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 22.925
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 21.277
Recognition finished, iteration 97 Loss 0.005
Recognition iteration 0 Loss 23.685
Recognition finished, iteration 100 Loss 0.013
Perplexity dev: 187.453

==== Starting epoch 116 ====
  Batch 0 Loss 3.0514 Mono loss 6.2542
  Batch 100 Loss 3.3332 Mono loss 5.0981
  Batch 200 Loss 5.3570 Mono loss 7.0409
  Batch 300 Loss 3.2532 Mono loss 6.3641
  Batch 400 Loss 4.5566 Mono loss 5.7198
  Batch 500 Loss 6.0703 Mono loss 6.9893
  Batch 600 Loss 5.7982 Mono loss 8.5303
  Batch 700 Loss 4.3888 Mono loss 7.2768
Resetting 31401 PBs
Finished epoch 116 in 432.0 seconds
Perplexity training: 3.551

==== Starting epoch 117 ====
  Batch 0 Loss 2.0640 Mono loss -1.0000
  Batch 100 Loss 4.7660 Mono loss 6.3006
  Batch 200 Loss 5.2388 Mono loss 7.7321
  Batch 300 Loss 5.7488 Mono loss 6.6380
  Batch 400 Loss 4.5249 Mono loss 6.4322
  Batch 500 Loss 4.2665 Mono loss 5.6028
  Batch 600 Loss 3.1238 Mono loss 5.2744
  Batch 700 Loss 4.0769 Mono loss 6.7182
Resetting 31479 PBs
Finished epoch 117 in 425.0 seconds
Perplexity training: 3.544
Measuring development set...
Recognition iteration 0 Loss 23.472
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.114
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 21.252
Recognition finished, iteration 94 Loss 0.004
Recognition iteration 0 Loss 23.656
Recognition finished, iteration 100 Loss 0.010
Perplexity dev: 161.759

==== Starting epoch 118 ====
  Batch 0 Loss 3.6590 Mono loss -1.0000
  Batch 100 Loss 5.4481 Mono loss 4.5960
  Batch 200 Loss 3.8760 Mono loss 6.4124
  Batch 300 Loss 5.8343 Mono loss 7.5542
  Batch 400 Loss 3.9681 Mono loss 4.7352
  Batch 500 Loss 4.2317 Mono loss 5.5282
  Batch 600 Loss 3.0767 Mono loss 6.6501
  Batch 700 Loss 4.2059 Mono loss 7.8327
Resetting 31506 PBs
Finished epoch 118 in 436.0 seconds
Perplexity training: 3.553

==== Starting epoch 119 ====
  Batch 0 Loss 2.9404 Mono loss 5.8277
  Batch 100 Loss 3.3572 Mono loss 6.0783
  Batch 200 Loss 4.1018 Mono loss 4.8843
  Batch 300 Loss 3.6699 Mono loss 7.9524
  Batch 400 Loss 5.1238 Mono loss 4.3792
  Batch 500 Loss 3.8633 Mono loss 5.8691
  Batch 600 Loss 3.3138 Mono loss 7.1799
  Batch 700 Loss 4.1205 Mono loss 6.4580
Resetting 31339 PBs
Finished epoch 119 in 447.0 seconds
Perplexity training: 3.562
Measuring development set...
Recognition iteration 0 Loss 23.537
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 23.156
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 21.751
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 23.689
Recognition finished, iteration 100 Loss 0.011
Perplexity dev: 353.111

==== Starting epoch 120 ====
  Batch 0 Loss 3.4516 Mono loss 6.4653
  Batch 100 Loss 3.6116 Mono loss 6.2099
  Batch 200 Loss 4.2669 Mono loss 5.9590
  Batch 300 Loss 3.9148 Mono loss 4.9895
  Batch 400 Loss 5.0802 Mono loss 5.8970
  Batch 500 Loss 4.7487 Mono loss 4.6741
  Batch 600 Loss 3.3743 Mono loss 6.3475
  Batch 700 Loss 4.6544 Mono loss 6.6713
Resetting 31679 PBs
Finished epoch 120 in 438.0 seconds
Perplexity training: 3.489

==== Starting epoch 121 ====
  Batch 0 Loss 4.9763 Mono loss 7.1427
  Batch 100 Loss 3.4118 Mono loss 5.9625
  Batch 200 Loss 5.7413 Mono loss 6.4828
  Batch 300 Loss 4.8095 Mono loss 6.2582
  Batch 400 Loss 4.1809 Mono loss 5.9278
  Batch 500 Loss 4.8388 Mono loss 7.7996
  Batch 600 Loss 2.8475 Mono loss 9.6886
  Batch 700 Loss 4.8689 Mono loss 7.0056
Resetting 31169 PBs
Finished epoch 121 in 456.0 seconds
Perplexity training: 3.599
Measuring development set...
Recognition iteration 0 Loss 23.927
Recognition finished, iteration 100 Loss 0.007
Recognition iteration 0 Loss 23.580
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.026
Recognition finished, iteration 96 Loss 0.004
Recognition iteration 0 Loss 24.002
Recognition finished, iteration 100 Loss 0.011
Perplexity dev: 230.959

==== Starting epoch 122 ====
  Batch 0 Loss 4.2259 Mono loss -1.0000
  Batch 100 Loss 2.7217 Mono loss 8.0766
  Batch 200 Loss 5.3383 Mono loss 6.3347
  Batch 300 Loss 4.3506 Mono loss 6.5451
  Batch 400 Loss 3.6121 Mono loss 6.8291
  Batch 500 Loss 3.0403 Mono loss 6.1230
  Batch 600 Loss 4.3161 Mono loss 6.7335
  Batch 700 Loss 2.8075 Mono loss 6.4516
Resetting 31306 PBs
Finished epoch 122 in 441.0 seconds
Perplexity training: 3.547

==== Starting epoch 123 ====
  Batch 0 Loss 5.3665 Mono loss -1.0000
  Batch 100 Loss 2.3243 Mono loss 5.4968
  Batch 200 Loss 3.1529 Mono loss 5.3307
  Batch 300 Loss 3.9649 Mono loss 6.3039
  Batch 400 Loss 4.3375 Mono loss 5.9868
  Batch 500 Loss 3.8295 Mono loss 6.1174
  Batch 600 Loss 3.6879 Mono loss 7.6436
  Batch 700 Loss 5.5488 Mono loss 6.7052
Resetting 31360 PBs
Finished epoch 123 in 444.0 seconds
Perplexity training: 3.520
Measuring development set...
Recognition iteration 0 Loss 22.882
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.889
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 21.111
Recognition finished, iteration 89 Loss 0.004
Recognition iteration 0 Loss 23.031
Recognition finished, iteration 100 Loss 0.010
Perplexity dev: 350.572

==== Starting epoch 124 ====
  Batch 0 Loss 3.4185 Mono loss -1.0000
  Batch 100 Loss 3.9721 Mono loss 5.0315
  Batch 200 Loss 4.6284 Mono loss 5.2175
  Batch 300 Loss 3.9680 Mono loss 4.2830
  Batch 400 Loss 4.5279 Mono loss 5.1440
  Batch 500 Loss 4.2986 Mono loss 4.9246
  Batch 600 Loss 4.8873 Mono loss 7.2241
  Batch 700 Loss 4.3496 Mono loss 7.4535
Resetting 31670 PBs
Finished epoch 124 in 456.0 seconds
Perplexity training: 3.511

==== Starting epoch 125 ====
  Batch 0 Loss 3.3666 Mono loss 6.1547
  Batch 100 Loss 2.0945 Mono loss 5.8786
  Batch 200 Loss 3.7987 Mono loss 5.6229
  Batch 300 Loss 4.2331 Mono loss 5.5130
  Batch 400 Loss 4.7038 Mono loss 6.0881
  Batch 500 Loss 5.5090 Mono loss 7.6442
  Batch 600 Loss 3.6870 Mono loss 6.3374
  Batch 700 Loss 4.8702 Mono loss 7.7993
Resetting 31532 PBs
Finished epoch 125 in 467.0 seconds
Perplexity training: 3.529
Measuring development set...
Recognition iteration 0 Loss 22.919
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 22.679
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 21.171
Recognition finished, iteration 92 Loss 0.004
Recognition iteration 0 Loss 23.379
Recognition finished, iteration 100 Loss 0.010
Perplexity dev: 175.025

==== Starting epoch 126 ====
  Batch 0 Loss 3.6984 Mono loss -1.0000
  Batch 100 Loss 3.0984 Mono loss 5.6965
  Batch 200 Loss 6.0942 Mono loss 8.0583
  Batch 300 Loss 4.3099 Mono loss 6.8562
  Batch 400 Loss 5.3432 Mono loss 6.3637
  Batch 500 Loss 6.1016 Mono loss 5.3656
  Batch 600 Loss 2.7602 Mono loss 5.0854
  Batch 700 Loss 4.6585 Mono loss 6.9078
Resetting 31544 PBs
Finished epoch 126 in 444.0 seconds
Perplexity training: 3.536

==== Starting epoch 127 ====
  Batch 0 Loss 2.8001 Mono loss 9.6917
  Batch 100 Loss 2.6058 Mono loss 6.5277
  Batch 200 Loss 7.2757 Mono loss 6.6112
  Batch 300 Loss 3.4818 Mono loss 5.8368
  Batch 400 Loss 5.3162 Mono loss 6.2216
  Batch 500 Loss 5.8764 Mono loss 5.7964
  Batch 600 Loss 3.0626 Mono loss 5.3249
  Batch 700 Loss 3.0845 Mono loss 5.6225
Resetting 31363 PBs
Finished epoch 127 in 466.0 seconds
Perplexity training: 3.499
Measuring development set...
Recognition iteration 0 Loss 23.611
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.398
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 21.825
Recognition finished, iteration 93 Loss 0.004
Recognition iteration 0 Loss 23.730
Recognition finished, iteration 100 Loss 0.010
Perplexity dev: 378.077

==== Starting epoch 128 ====
  Batch 0 Loss 4.0327 Mono loss 8.0721
  Batch 100 Loss 2.7414 Mono loss 6.7950
  Batch 200 Loss 6.2604 Mono loss 8.1919
  Batch 300 Loss 5.6075 Mono loss 6.6208
  Batch 400 Loss 3.5602 Mono loss 6.3298
  Batch 500 Loss 4.3020 Mono loss 6.3140
  Batch 600 Loss 4.2491 Mono loss 6.5578
  Batch 700 Loss 3.6060 Mono loss 6.2496
Resetting 31386 PBs
Finished epoch 128 in 475.0 seconds
Perplexity training: 3.558

==== Starting epoch 129 ====
  Batch 0 Loss 3.7713 Mono loss 6.6585
  Batch 100 Loss 4.5687 Mono loss 5.3654
  Batch 200 Loss 5.8075 Mono loss 7.7191
  Batch 300 Loss 3.7452 Mono loss 4.2304
  Batch 400 Loss 2.8283 Mono loss 5.4574
  Batch 500 Loss 5.3276 Mono loss 7.2849
  Batch 600 Loss 2.7678 Mono loss 5.7455
  Batch 700 Loss 4.7017 Mono loss 5.3369
Resetting 31446 PBs
Finished epoch 129 in 484.0 seconds
Perplexity training: 3.539
Measuring development set...
Recognition iteration 0 Loss 23.281
Recognition finished, iteration 100 Loss 0.006
Recognition iteration 0 Loss 23.070
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 21.629
Recognition finished, iteration 91 Loss 0.004
Recognition iteration 0 Loss 23.443
Recognition finished, iteration 100 Loss 0.010
Perplexity dev: 235.846

==== Starting epoch 130 ====
  Batch 0 Loss 5.9893 Mono loss 6.5858
  Batch 100 Loss 3.5368 Mono loss 5.7642
  Batch 200 Loss 5.2448 Mono loss 6.2766
  Batch 300 Loss 4.3580 Mono loss 5.3471
  Batch 400 Loss 5.3531 Mono loss 8.5357
  Batch 500 Loss 4.9329 Mono loss 5.2388
  Batch 600 Loss 3.4330 Mono loss 6.2148
  Batch 700 Loss 4.3926 Mono loss 5.3542
Resetting 31653 PBs
Finished epoch 130 in 474.0 seconds
Perplexity training: 3.561

==== Starting epoch 131 ====
  Batch 0 Loss 2.6946 Mono loss 6.0575
  Batch 100 Loss 4.3541 Mono loss 5.0776
  Batch 200 Loss 5.0937 Mono loss 6.4484
  Batch 300 Loss 5.6024 Mono loss 5.4608
  Batch 400 Loss 4.3335 Mono loss 6.2755
  Batch 500 Loss 4.1037 Mono loss 4.9984
  Batch 600 Loss 3.7367 Mono loss 6.8936
  Batch 700 Loss 4.8984 Mono loss 5.5041
Resetting 31670 PBs
Finished epoch 131 in 462.0 seconds
Perplexity training: 3.589
Measuring development set...
Recognition iteration 0 Loss 22.893
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 22.444
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 21.132
Recognition finished, iteration 83 Loss 0.004
Recognition iteration 0 Loss 23.340
Recognition finished, iteration 100 Loss 0.009
Perplexity dev: 205.722

==== Starting epoch 132 ====
  Batch 0 Loss 4.8638 Mono loss 5.4814
  Batch 100 Loss 3.3913 Mono loss 5.0288
  Batch 200 Loss 3.8829 Mono loss 7.7158
  Batch 300 Loss 3.7931 Mono loss 6.7317
  Batch 400 Loss 4.6110 Mono loss 6.1351
  Batch 500 Loss 2.7597 Mono loss 5.5733
  Batch 600 Loss 2.4713 Mono loss 5.0583
  Batch 700 Loss 4.0300 Mono loss 5.4490
Resetting 31359 PBs
Finished epoch 132 in 459.0 seconds
Perplexity training: 3.530

==== Starting epoch 133 ====
  Batch 0 Loss 3.1889 Mono loss -1.0000
  Batch 100 Loss 3.9497 Mono loss 5.2270
  Batch 200 Loss 4.0211 Mono loss 7.6003
  Batch 300 Loss 4.6486 Mono loss 6.1805
  Batch 400 Loss 6.0946 Mono loss 8.2532
  Batch 500 Loss 3.5891 Mono loss 5.1976
  Batch 600 Loss 3.1750 Mono loss 5.0538
  Batch 700 Loss 3.3249 Mono loss 6.7739
Resetting 31426 PBs
Finished epoch 133 in 447.0 seconds
Perplexity training: 3.479
Measuring development set...
Recognition iteration 0 Loss 23.036
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 22.482
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 21.054
Recognition finished, iteration 82 Loss 0.004
Recognition iteration 0 Loss 23.242
Recognition finished, iteration 100 Loss 0.008
Perplexity dev: 208.607

==== Starting epoch 134 ====
  Batch 0 Loss 3.8999 Mono loss -1.0000
  Batch 100 Loss 5.4422 Mono loss 7.2679
  Batch 200 Loss 4.5260 Mono loss 5.7581
  Batch 300 Loss 4.1377 Mono loss 6.3761
  Batch 400 Loss 4.3893 Mono loss 7.0051
  Batch 500 Loss 5.3188 Mono loss 7.1097
  Batch 600 Loss 3.3012 Mono loss 5.3021
  Batch 700 Loss 3.4037 Mono loss 7.8046
Resetting 31320 PBs
Finished epoch 134 in 475.0 seconds
Perplexity training: 3.488

==== Starting epoch 135 ====
  Batch 0 Loss 6.3271 Mono loss 8.5438
  Batch 100 Loss 2.7997 Mono loss 5.7888
  Batch 200 Loss 4.2959 Mono loss 5.3161
  Batch 300 Loss 5.1303 Mono loss 5.5078
  Batch 400 Loss 4.2605 Mono loss 5.9790
  Batch 500 Loss 3.9464 Mono loss 6.3535
  Batch 600 Loss 4.4730 Mono loss 7.6343
  Batch 700 Loss 4.5851 Mono loss 7.5486
Resetting 31329 PBs
Finished epoch 135 in 495.0 seconds
Perplexity training: 3.491
Measuring development set...
Recognition iteration 0 Loss 23.157
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 22.985
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 21.690
Recognition finished, iteration 85 Loss 0.004
Recognition iteration 0 Loss 23.506
Recognition finished, iteration 100 Loss 0.009
Perplexity dev: 378.135

==== Starting epoch 136 ====
  Batch 0 Loss 4.4745 Mono loss -1.0000
  Batch 100 Loss 2.8449 Mono loss 5.8408
  Batch 200 Loss 4.4496 Mono loss 6.0535
  Batch 300 Loss 4.9796 Mono loss 5.6782
  Batch 400 Loss 4.2564 Mono loss 5.4374
  Batch 500 Loss 4.4929 Mono loss 5.6539
  Batch 600 Loss 3.5495 Mono loss 5.3615
  Batch 700 Loss 5.2486 Mono loss 6.6831
Resetting 31181 PBs
Finished epoch 136 in 474.0 seconds
Perplexity training: 3.449

==== Starting epoch 137 ====
  Batch 0 Loss 3.4643 Mono loss -1.0000
  Batch 100 Loss 2.9026 Mono loss 4.7434
  Batch 200 Loss 4.9417 Mono loss 7.3198
  Batch 300 Loss 5.1433 Mono loss 4.9354
  Batch 400 Loss 4.6458 Mono loss 6.0409
  Batch 500 Loss 5.8286 Mono loss 5.3587
  Batch 600 Loss 3.8354 Mono loss 5.2370
  Batch 700 Loss 3.7473 Mono loss 5.3393
Resetting 31628 PBs
Finished epoch 137 in 472.0 seconds
Perplexity training: 3.448
Measuring development set...
Recognition iteration 0 Loss 23.338
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 22.453
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 21.393
Recognition finished, iteration 87 Loss 0.004
Recognition iteration 0 Loss 23.188
Recognition finished, iteration 100 Loss 0.010
Perplexity dev: 391.668

==== Starting epoch 138 ====
  Batch 0 Loss 2.1917 Mono loss -1.0000
  Batch 100 Loss 3.0651 Mono loss 5.7996
  Batch 200 Loss 4.0986 Mono loss 4.5942
  Batch 300 Loss 4.2865 Mono loss 6.3846
  Batch 400 Loss 4.2921 Mono loss 5.9069
  Batch 500 Loss 4.3757 Mono loss 5.4146
  Batch 600 Loss 3.0813 Mono loss 4.7577
  Batch 700 Loss 4.2522 Mono loss 5.8688
Resetting 31601 PBs
Finished epoch 138 in 476.0 seconds
Perplexity training: 3.468

==== Starting epoch 139 ====
  Batch 0 Loss 3.4487 Mono loss 5.9788
  Batch 100 Loss 3.8625 Mono loss 4.9677
  Batch 200 Loss 4.7155 Mono loss 8.0643
  Batch 300 Loss 4.3433 Mono loss 6.1354
  Batch 400 Loss 3.4475 Mono loss 5.6708
  Batch 500 Loss 5.1210 Mono loss 6.3000
  Batch 600 Loss 4.1213 Mono loss 5.5528
  Batch 700 Loss 3.1336 Mono loss 6.9658
Resetting 31322 PBs
Finished epoch 139 in 500.0 seconds
Perplexity training: 3.465
Measuring development set...
Recognition iteration 0 Loss 23.384
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 22.809
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 21.405
Recognition finished, iteration 90 Loss 0.004
Recognition iteration 0 Loss 23.580
Recognition finished, iteration 100 Loss 0.009
Perplexity dev: 293.959

==== Starting epoch 140 ====
  Batch 0 Loss 3.2892 Mono loss 5.4130
  Batch 100 Loss 4.0081 Mono loss 5.2556
  Batch 200 Loss 3.5495 Mono loss 6.0283
  Batch 300 Loss 4.3366 Mono loss 7.0429
  Batch 400 Loss 4.3361 Mono loss 5.4712
  Batch 500 Loss 5.9177 Mono loss 5.3526
  Batch 600 Loss 2.5531 Mono loss 4.7332
  Batch 700 Loss 5.1794 Mono loss 5.1433
Resetting 31453 PBs
Finished epoch 140 in 507.0 seconds
Perplexity training: 3.467

==== Starting epoch 141 ====
  Batch 0 Loss 4.3353 Mono loss -1.0000
  Batch 100 Loss 3.3242 Mono loss 4.5238
  Batch 200 Loss 4.2722 Mono loss 5.8239
  Batch 300 Loss 4.5038 Mono loss 8.5012
  Batch 400 Loss 3.8673 Mono loss 4.8796
  Batch 500 Loss 5.2469 Mono loss 6.8066
  Batch 600 Loss 1.9163 Mono loss 6.1734
  Batch 700 Loss 4.7792 Mono loss 5.8003
Resetting 31466 PBs
Finished epoch 141 in 506.0 seconds
Perplexity training: 3.441
Measuring development set...
Recognition iteration 0 Loss 22.874
Recognition finished, iteration 95 Loss 0.005
Recognition iteration 0 Loss 22.858
Recognition finished, iteration 100 Loss 0.004
Recognition iteration 0 Loss 21.239
Recognition finished, iteration 86 Loss 0.004
Recognition iteration 0 Loss 23.253
Recognition finished, iteration 100 Loss 0.009
Perplexity dev: 218.447

==== Starting epoch 142 ====
  Batch 0 Loss 4.1922 Mono loss 5.1685
  Batch 100 Loss 5.8761 Mono loss 6.7686
  Batch 200 Loss 5.3275 Mono loss 6.9331
  Batch 300 Loss 2.8183 Mono loss 6.3087
  Batch 400 Loss 5.2414 Mono loss 5.4550
  Batch 500 Loss 5.7393 Mono loss 4.8817
  Batch 600 Loss 2.9020 Mono loss 4.1873
  Batch 700 Loss 4.3720 Mono loss 5.7150
Resetting 31456 PBs
Finished epoch 142 in 488.0 seconds
Perplexity training: 3.450

==== Starting epoch 143 ====
  Batch 0 Loss 4.4103 Mono loss -1.0000
  Batch 100 Loss 5.3103 Mono loss 5.5870
  Batch 200 Loss 5.2267 Mono loss 7.9152
  Batch 300 Loss 3.2580 Mono loss 6.1558
  Batch 400 Loss 3.2318 Mono loss 5.8540
  Batch 500 Loss 5.3549 Mono loss 7.0828
  Batch 600 Loss 3.0975 Mono loss 6.3296
  Batch 700 Loss 4.7501 Mono loss 6.3668
Resetting 31195 PBs
Finished epoch 143 in 487.0 seconds
Perplexity training: 3.477
Measuring development set...
Recognition iteration 0 Loss 23.154
Recognition finished, iteration 98 Loss 0.004
Recognition iteration 0 Loss 22.410
Recognition finished, iteration 90 Loss 0.004
Recognition iteration 0 Loss 21.326
Recognition finished, iteration 84 Loss 0.004
Recognition iteration 0 Loss 23.099
Recognition finished, iteration 100 Loss 0.008
Perplexity dev: 383.604

==== Starting epoch 144 ====
  Batch 0 Loss 2.6443 Mono loss 5.4979
  Batch 100 Loss 2.7188 Mono loss 6.4901
  Batch 200 Loss 4.2825 Mono loss 5.5921
  Batch 300 Loss 4.3920 Mono loss 6.3201
  Batch 400 Loss 4.5378 Mono loss 5.9385
  Batch 500 Loss 4.6094 Mono loss 6.1133
  Batch 600 Loss 3.0540 Mono loss 5.6630
  Batch 700 Loss 6.6695 Mono loss 6.1330
Resetting 31543 PBs
Finished epoch 144 in 490.0 seconds
Perplexity training: 3.573

==== Starting epoch 145 ====
  Batch 0 Loss 4.2175 Mono loss 7.3275
  Batch 100 Loss 3.3027 Mono loss 6.1836
  Batch 200 Loss 3.2402 Mono loss 6.5127
  Batch 300 Loss 3.2499 Mono loss 5.1755
  Batch 400 Loss 4.2523 Mono loss 5.0893
  Batch 500 Loss 4.1984 Mono loss 5.8318
  Batch 600 Loss 3.5418 Mono loss 5.9376
  Batch 700 Loss 5.6655 Mono loss 5.1971
Resetting 31347 PBs
Finished epoch 145 in 499.0 seconds
Perplexity training: 3.505
Measuring development set...
Recognition iteration 0 Loss 23.453
Recognition finished, iteration 97 Loss 0.004
Recognition iteration 0 Loss 22.720
Recognition finished, iteration 87 Loss 0.004
Recognition iteration 0 Loss 21.698
Recognition finished, iteration 77 Loss 0.003
Recognition iteration 0 Loss 23.228
Recognition finished, iteration 100 Loss 0.008
Perplexity dev: 476.963

==== Starting epoch 146 ====
  Batch 0 Loss 4.3639 Mono loss -1.0000
  Batch 100 Loss 3.6735 Mono loss 6.2245
  Batch 200 Loss 5.6835 Mono loss 5.4760
  Batch 300 Loss 3.1974 Mono loss 5.0165
  Batch 400 Loss 3.2020 Mono loss 6.1486
  Batch 500 Loss 4.9261 Mono loss 6.6394
  Batch 600 Loss 2.5887 Mono loss 5.8197
  Batch 700 Loss 4.7978 Mono loss 7.2851
Resetting 31589 PBs
Finished epoch 146 in 505.0 seconds
Perplexity training: 3.444

==== Starting epoch 147 ====
  Batch 0 Loss 3.1905 Mono loss 7.1270
  Batch 100 Loss 4.6146 Mono loss 6.3484
  Batch 200 Loss 3.7965 Mono loss 4.9922
  Batch 300 Loss 4.1397 Mono loss 5.5052
  Batch 400 Loss 2.8416 Mono loss 4.9525
  Batch 500 Loss 5.1396 Mono loss 6.2235
  Batch 600 Loss 2.7092 Mono loss 7.8193
  Batch 700 Loss 4.2711 Mono loss 6.3633
Resetting 31469 PBs
Finished epoch 147 in 519.0 seconds
Perplexity training: 3.490
Measuring development set...
Recognition iteration 0 Loss 22.996
Recognition finished, iteration 100 Loss 0.005
Recognition iteration 0 Loss 22.777
Recognition finished, iteration 94 Loss 0.004
Recognition iteration 0 Loss 21.079
Recognition finished, iteration 74 Loss 0.003
Recognition iteration 0 Loss 23.193
Recognition finished, iteration 100 Loss 0.007
Perplexity dev: 577.667

==== Starting epoch 148 ====
  Batch 0 Loss 2.7030 Mono loss 6.4454
  Batch 100 Loss 2.7276 Mono loss 6.2995
  Batch 200 Loss 4.2508 Mono loss 6.7803
  Batch 300 Loss 3.4399 Mono loss 7.4841
  Batch 400 Loss 2.2561 Mono loss 6.7220
  Batch 500 Loss 4.3295 Mono loss 4.7326
  Batch 600 Loss 4.0727 Mono loss 4.2166
  Batch 700 Loss 5.1003 Mono loss 6.6384
Resetting 31396 PBs
Finished epoch 148 in 506.0 seconds
Perplexity training: 3.363

==== Starting epoch 149 ====
  Batch 0 Loss 4.4210 Mono loss -1.0000
  Batch 100 Loss 1.3725 Mono loss 6.5745
  Batch 200 Loss 3.7593 Mono loss 7.2888
  Batch 300 Loss 3.8766 Mono loss 6.1951
  Batch 400 Loss 3.4845 Mono loss 6.0606
  Batch 500 Loss 5.7964 Mono loss 6.0836
  Batch 600 Loss 5.1048 Mono loss 5.6208
  Batch 700 Loss 3.8650 Mono loss 5.6272
Resetting 31782 PBs
Finished epoch 149 in 516.0 seconds
Perplexity training: 3.455
Measuring development set...
Recognition iteration 0 Loss 23.274
Recognition finished, iteration 99 Loss 0.004
Recognition iteration 0 Loss 22.895
Recognition finished, iteration 92 Loss 0.004
Recognition iteration 0 Loss 21.263
Recognition finished, iteration 76 Loss 0.003
Recognition iteration 0 Loss 23.365
Recognition finished, iteration 100 Loss 0.007
Perplexity dev: 615.476

==== Starting epoch 150 ====
  Batch 0 Loss 3.0947 Mono loss 4.7435
  Batch 100 Loss 2.5464 Mono loss 6.3859
  Batch 200 Loss 3.2880 Mono loss 5.4987
  Batch 300 Loss 3.0120 Mono loss 6.5824
  Batch 400 Loss 4.7665 Mono loss 5.2950
  Batch 500 Loss 5.6238 Mono loss 6.9587
  Batch 600 Loss 3.8637 Mono loss 5.0002
  Batch 700 Loss 4.3125 Mono loss 6.5191
Resetting 31492 PBs
Finished epoch 150 in 517.0 seconds
Perplexity training: 3.475
Finished training in 51361.87 seconds
Finished training after exceeding max epochs.
