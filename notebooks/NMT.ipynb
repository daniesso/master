{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import time\n",
    "import random\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = open(\"data/deu-tok.en\", \"r\", encoding=\"utf-8\").readlines()\n",
    "de = open(\"data/deu-tok.de\", \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "en = [x.strip() for x in en]\n",
    "de = [x.strip() for x in de]\n",
    "\n",
    "# Remove the 10 longest\n",
    "for i in range(10):\n",
    "    i_mx = max(range(len(en)), key = lambda i : len(en[i].split(\" \")))\n",
    "    del en[i_mx]\n",
    "    del de[i_mx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(x.split(\" \")) for x in en], bins='auto', log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist([len(x.split(\" \")) for x in de], bins='auto', log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_subset(D1, D2, n):\n",
    "    c1, c2 = {}, {}\n",
    "    for d1, d2 in zip(D1, D2):\n",
    "        for w in d1.split(\" \"):\n",
    "            c1[w] = c1.get(w, 0) + 1\n",
    "        for w in d2.split(\" \"):\n",
    "            c2[w] = c2.get(w, 0) + 1\n",
    "    \n",
    "    inds = np.array(sorted(range(len(D1)), key = lambda i : min(c[w]  for d, c in [(D1, c1), (D2, c2)] for w in d[i].split(\" \")), reverse=True)[:n])\n",
    "    return np.take(D1, inds), np.take(D2, inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, de = easy_subset(en, de, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ints(D, max_vocab_size):\n",
    "    counts = {}\n",
    "    for s in D:\n",
    "        for w in s.split(\" \"):\n",
    "            counts[w] = counts.get(w, 0) + 1\n",
    "    \n",
    "    w2idx = {x[0]:4+i for i, x in enumerate(sorted(counts.items(), reverse=True, key = lambda p : p[1])[:max_vocab_size])}\n",
    "    \n",
    "    w2idx[\"<PAD>\"] = 0\n",
    "    w2idx[\"<START>\"] = 1\n",
    "    w2idx[\"<END>\"] = 2\n",
    "    w2idx[\"<UNK>\"] = 3\n",
    "    \n",
    "    idx2w = {v:k for k,v in w2idx.items()}\n",
    "    \n",
    "    Didx = [[w2idx[\"<START>\"]] + [w2idx.get(x, w2idx[\"<UNK>\"]) for x in s.split(\" \")] + [w2idx[\"<END>\"]] for s in D]\n",
    "    \n",
    "    return Didx, w2idx, idx2w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_idxs, en_w2idx, en_idx2w = to_ints(en, 100000)\n",
    "de_idxs, de_w2idx, de_idx2w = to_ints(de, 50000)\n",
    "\n",
    "en_maxlen = max(map(len, en_idxs))\n",
    "de_maxlen = max(map(len, de_idxs))\n",
    "en_vocab_size = len(en_w2idx)\n",
    "de_vocab_size = len(de_w2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_lengths = [len(x) for x in en_idxs]\n",
    "tar_lengths = [len(x) for x in de_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.preprocessing.sequence.pad_sequences(en_idxs, maxlen=en_maxlen, padding=\"post\")\n",
    "tar = tf.keras.preprocessing.sequence.pad_sequences(de_idxs, maxlen=de_maxlen, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "embedding_dim = 32\n",
    "units = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "BUFFER_SIZE = len(inp)\n",
    "N_BATCH = len(inp) // BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, inp_lengths, tar_lengths, tar)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_axis_1(data, ind):\n",
    "    \"\"\"\n",
    "    Get specified elements along the first axis of tensor.\n",
    "    :param data: Tensorflow tensor that will be subsetted.\n",
    "    :param ind: Indices to take (one for each element along axis 0 of data).\n",
    "    :return: Subsetted tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_range = tf.range(tf.shape(data)[0])\n",
    "    indices = tf.stack([batch_range, ind], axis=1)\n",
    "    res = tf.gather_nd(data, indices)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.gru = tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_state = False,\n",
    "                                        return_sequences = True,\n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden, sequence_lengths):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        outputs = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        state = extract_axis_1(outputs, sequence_lengths - 1)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.units = units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.CuDNNGRU(units,\n",
    "                                        return_sequences = True,\n",
    "                                        return_state = True,\n",
    "                                        recurrent_initializer = 'glorot_uniform')\n",
    "        \n",
    "        self.logits = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        outputs, state = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        outputs = self.logits(outputs)\n",
    "        outputs = tf.reshape(outputs, [self.batch_size, -1, self.vocab_size])\n",
    "        \n",
    "        return outputs, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.units))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(en_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(de_vocab_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    hidden_init = encoder.initialize_hidden_state()\n",
    "    \n",
    "    for (batch, (inp, inp_lengths, targ_lengths, targ)) in enumerate(dataset):\n",
    "        \n",
    "        max_inp_length = np.max(inp_lengths)\n",
    "        inp = inp[:, :max_inp_length]\n",
    "\n",
    "        max_tar_length = np.max(targ_lengths)\n",
    "        targ = targ[:, :max_tar_length]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            enc_hidden = encoder(inp, hidden_init, inp_lengths)\n",
    "            \n",
    "            dec_inputs = targ[:, :-1]\n",
    "            dec_targets = targ[:, 1:]\n",
    "            \n",
    "            logits, dec_hidden = decoder(dec_inputs, hidden = enc_hidden)\n",
    "            \n",
    "            mask = 1 - np.equal(dec_targets, 0)\n",
    "            loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=dec_targets, logits=logits) * mask\n",
    "            \n",
    "            batch_loss = tf.reduce_mean(loss_)\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(batch_loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        epoch_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.8f}'.format(epoch+1, batch, batch_loss.numpy()))\n",
    "            \n",
    "    print('Epoch loss {:.8f}'.format(epoch_loss.numpy()))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    print(\"TEST\")\n",
    "    for i in range(2):\n",
    "        print(\" \".join(en_idx2w[x.numpy()] for x in inp[i, : inp_lengths[i]]), \"   =>   \", end=\"\")\n",
    "        \n",
    "        preds = tf.argmax(logits, axis = 2)\n",
    "        \n",
    "        print(\"<START> \" + \" \".join(de_idx2w[x.numpy()] for x in preds[i]))\n",
    "        \n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_w2idx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"<START> May I go with Tom ? <END>\"\n",
    "\n",
    "print(\"Sentence: \", s)\n",
    "print(\"Unks: \", [x for x in s.split(\" \") if x not in en_w2idx])\n",
    "print() \n",
    "\n",
    "max_target_length = len(s.split(\" \")) * 2\n",
    "\n",
    "inp_length = np.array([len(s.split(\" \"))] * BATCH_SIZE)\n",
    "\n",
    "s = [[en_w2idx.get(w, en_w2idx[\"<UNK>\"]) for w in s.split(\" \")]] * BATCH_SIZE\n",
    "s = tf.keras.preprocessing.sequence.pad_sequences(s, maxlen=16, padding=\"post\")\n",
    "s = tf.convert_to_tensor(s)\n",
    "\n",
    "init_hidden = tf.zeros((BATCH_SIZE, units))\n",
    "\n",
    "enc_hidden = encoder(s, init_hidden, inp_length)\n",
    "\n",
    "dec_hidden = enc_hidden\n",
    "\n",
    "dec_input = tf.expand_dims([de_w2idx[\"<START>\"]]*BATCH_SIZE, 1)\n",
    "\n",
    "result = \"<START>\"\n",
    "for t in range(max_target_length):\n",
    "    \n",
    "    preds, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "    \n",
    "    preds = tf.reshape(preds, [BATCH_SIZE, -1])\n",
    "\n",
    "    pred = tf.argmax(preds, axis = 1)\n",
    "    \n",
    "    pred = pred[0].numpy()\n",
    "    \n",
    "    print(pred, tf.nn.softmax(preds)[0, pred].numpy())\n",
    "    \n",
    "    result += \" \" + de_idx2w[pred]\n",
    "    \n",
    "    if pred == de_w2idx[\"<END>\"]: break\n",
    "    \n",
    "    dec_input = tf.expand_dims([pred]*BATCH_SIZE,1)\n",
    "    \n",
    "\n",
    "print(\"\\nResult:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
