{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import time\n",
    "import random\n",
    "from models.rnnpb import RNNPB\n",
    "from misc.plots import plot_pbs\n",
    "from misc.dataset import Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "vocab1_max = 15000\n",
    "vocab2_max = 15000\n",
    "\n",
    "data = Dataset(\"data/deu-tok.en\", \"data/deu-tok.de\", batch_size, vocab1_max, vocab2_max, length_limit = 50, easy_subset=50000)\n",
    "\n",
    "X, X_test, X_lengths, X_test_lengths, Y, Y_test, Y_lengths, Y_test_lengths = train_test_split(data.X, data.X_lengths, data.Y, data.Y_lengths, test_size = 5)\n",
    "\n",
    "num_seqs = len(X)\n",
    "ids = list(range(num_seqs))\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((X, X_lengths, Y, Y_lengths, ids)).shuffle(len(X))\n",
    "train = train.batch(batch_size, drop_remainder = True)\n",
    "\n",
    "START = data.first.w2idx[\"<START>\"]\n",
    "END = data.first.w2idx[\"<END>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_lengths, bins='auto', log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(Y_lengths, bins='auto', log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_tensor(s, w2idx):\n",
    "    s = [w2idx.get(w, w2idx[\"<UNK>\"]) for w in s.split(\" \")]\n",
    "    x = tf.convert_to_tensor(s)\n",
    "    return x\n",
    "\n",
    "def tensor_to_sentence(x, idx2w):\n",
    "    return \" \".join(idx2w[i] for i in x)\n",
    "\n",
    "def translate(A, B, x, B_idx2w, eps=0.0001, return_pb = False):\n",
    "    pb = A.recognize(x, eps = eps)\n",
    "    y = B.generate(pb, max_length = round(1.5 * len(x)), start=START, end=END)\n",
    "    \n",
    "    s = tensor_to_sentence(y, B_idx2w)\n",
    "    \n",
    "    return s, pb[0] if return_pb else s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "units = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "recog_lr = 0.001\n",
    "num_PB = 32\n",
    "binding_strength = 1\n",
    "gradient_clip = 1.0\n",
    "bind_hard = True\n",
    "\n",
    "warmup_epochs = 0\n",
    "reset_pbs_every = 0 # epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = RNNPB(vocab_size=data.first.vocab_size, embedding_dim=embedding_dim, units=units, num_layers=num_layers, num_PB=num_PB, num_sequences=num_seqs, recog_lr = recog_lr, gradient_clip=gradient_clip, batch_size=batch_size, bind_hard=bind_hard)\n",
    "B = RNNPB(vocab_size=data.second.vocab_size, embedding_dim=embedding_dim, units=units, num_layers=num_layers, num_PB=num_PB, num_sequences=num_seqs, recog_lr = recog_lr, gradient_clip=gradient_clip, batch_size=batch_size, bind_hard=bind_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"====== EPOCH {} ======\".format(epoch))\n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    if epoch != 0 and reset_pbs_every != 0 and epoch % reset_pbs_every == 0:\n",
    "        A.reset_pbs()\n",
    "        B.reset_pbs()\n",
    "    \n",
    "    for (batch, (A_inp, A_lengths, B_inp, B_lengths,ids)) in enumerate(train):\n",
    "        \n",
    "        max_A_length = np.max(A_lengths)\n",
    "        A_inp = A_inp[:, :max_A_length]\n",
    "\n",
    "        max_B_length = np.max(B_lengths)\n",
    "        B_inp = B_inp[:, :max_B_length]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            A_inputs = A_inp[:, :-1]\n",
    "            A_targets = A_inp[:, 1:]\n",
    "            \n",
    "            B_inputs = B_inp[:, :-1]\n",
    "            B_targets = B_inp[:, 1:]\n",
    "            \n",
    "            A_outputs, _, A_pb = A(A_inputs, ids)\n",
    "            B_outputs, _, B_pb = B(B_inputs, ids)\n",
    "            \n",
    "            A_mask = 1 - np.equal(A_targets, 0)\n",
    "            A_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = A_targets, logits=A_outputs) * A_mask\n",
    "            \n",
    "            B_mask = 1 - np.equal(B_targets, 0)\n",
    "            B_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = B_targets, logits=B_outputs) * B_mask\n",
    "            \n",
    "            batch_loss = tf.reduce_mean(A_loss) + tf.reduce_mean(B_loss)\n",
    "            \n",
    "            bound_loss = batch_loss if bind == 'hard' else batch_loss + binding_strength * tf.nn.l2_loss(A_pb - B_pb)\n",
    "        \n",
    "        if epoch < warmup_epochs:\n",
    "            variables = A.variables_expt_pb() + B.variables_expt_pb()\n",
    "        else:\n",
    "            variables = A.variables + B.variables\n",
    "        \n",
    "        gradients = tape.gradient(bound_loss, variables)\n",
    "        \n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, gradient_clip)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        epoch_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Bound loss {:.4f}'.format(epoch+1, batch, batch_loss.numpy(), bound_loss.numpy()))\n",
    "    \n",
    "    print('Epoch loss {:.8f}'.format(epoch_loss.numpy()))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    if epoch >= warmup_epochs and epoch % 5 == 0:\n",
    "        print(\"Autoencoding test\")\n",
    "        A_preds = tf.argmax(A_outputs, axis = 2)[0, :A_lengths[0]-1].numpy()\n",
    "        print(tensor_to_sentence(A_inp[0, :A_lengths[0]].numpy(), data.first.idx2w), \"   =>   \", end=\"\")\n",
    "        print(\"<START> \" + tensor_to_sentence(A_preds, data.first.idx2w))\n",
    "        B_preds = tf.argmax(B_outputs, axis = 2)[0, :B_lengths[0]-1].numpy()\n",
    "        print(tensor_to_sentence(B_inp[0, :B_lengths[0]].numpy(), data.second.idx2w), \"   =>   \", end=\"\")\n",
    "        print(\"<START> \" + tensor_to_sentence(B_preds, data.second.idx2w))\n",
    "        \n",
    "        print(\"\\nPB stats:\")\n",
    "        print(\"A\")\n",
    "        A.print_pb_stats()\n",
    "        print(\"B\")\n",
    "        B.print_pb_stats()    \n",
    "        \n",
    "        \n",
    "        ids = ids[0:7]\n",
    "        A_pbs = A.get_pbs(ids)\n",
    "        B_pbs = B.get_pbs(ids)\n",
    "        \n",
    "        use_dev = (epoch % 10 == 0)\n",
    "        if use_dev:\n",
    "            print(\"\\nTesting translation using dev set\")\n",
    "            test_i = np.random.randint(0, len(X_test))\n",
    "            x = X_test[test_i, :X_test_lengths[test_i]]\n",
    "            trans, _ = translate(A, B, x, data.second.idx2w, eps=0.001, return_pb=True)\n",
    "            \n",
    "            print(tensor_to_sentence(x, data.first.idx2w), \"   =>   \", trans)\n",
    "            \n",
    "            # Only bother plotting pbs with dev example for the sake of showing soft binding\n",
    "            if bind == 'soft':\n",
    "                plot_pbs(A_pbs, B_pbs, plot_zero = True)\n",
    "            print()\n",
    "        else:\n",
    "            print(\"\\nTesting translation using training set\")\n",
    "            x = A_inp[0, :A_lengths[0]].numpy()\n",
    "            pb_journey = A.recognize(x, iters=1000, step = 100)\n",
    "            pb = pb_journey[-1]\n",
    "            \n",
    "            print(\"Target dist from origin:\", np.sqrt(np.sum(A_pbs[0]**2)))\n",
    "            print(\"Recognized pb dist from origin:\", np.sqrt(np.sum(pb**2)))\n",
    "            \n",
    "            trans = tensor_to_sentence(B.generate(pb, max_length = round(1.5 * len(x)), start=START, end=END), data.second.idx2w)\n",
    "            print(tensor_to_sentence(x, data.first.idx2w), \"   =>   \", trans)\n",
    "            \n",
    "            print(\"\\n PB plot (red square corresponds to red point)\")\n",
    "            plot_pbs(A_pbs, B_pbs, pb_journey = pb_journey, plot_zero = True)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"<START> What exactly did you do ? <END>\"\n",
    "x = sentence_to_tensor(s, en_w2idx)\n",
    "\n",
    "print(translate(A, A, x, en_idx2w))\n",
    "print(translate(A, B, x, de_idx2w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
