{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import time\n",
    "import random\n",
    "from models.rnnpb import RNNPB\n",
    "from misc.plots import plot_pbs\n",
    "from misc.dataset import Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "vocab1_max = 15000\n",
    "vocab2_max = 15000\n",
    "\n",
    "data = Dataset(\"data/deu-tok.en\", \"data/deu-tok.de\", batch_size, vocab1_max, vocab2_max, length_limit = 50, easy_subset=50000)\n",
    "#data = Dataset(\"data/deu-tok.en\", \"data/deu-tok.de\", batch_size, length_limit = 100)\n",
    "\n",
    "_, unique_inds = np.unique(data.X, axis = 0, return_index = True)\n",
    "\n",
    "#print(len(unique_inds))\n",
    "\n",
    "X = data.X[unique_inds]\n",
    "X_lengths = np.array(data.X_lengths)[unique_inds]\n",
    "\n",
    "#with open(\"data/autoenc/easy50000/en\", \"w\", encoding=\"utf-8\") as f:\n",
    "#    shuffled_inds = list(range(X.shape[0]))\n",
    "#    random.shuffle(shuffled_inds)\n",
    "#    for i in shuffled_inds:\n",
    "#        f.write(\" \".join(data.first.idx2w[w] for w in X[i, 1:X_lengths[i]-1]) + \"\\n\")\n",
    "\n",
    "X, X_test, X_lengths, X_test_lengths = train_test_split(X, X_lengths, test_size = 2000, shuffle=True)\n",
    "\n",
    "num_seqs = len(X)\n",
    "ids = list(range(num_seqs))\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((X, X_lengths, ids)).shuffle(len(X))\n",
    "train = train.batch(batch_size, drop_remainder = True)\n",
    "\n",
    "num_seqs_test = len(X_test)\n",
    "ids_test = list(range(num_seqs_test))\n",
    "test = tf.data.Dataset.from_tensor_slices((X_test, X_test_lengths, ids_test)).shuffle(len(X_test))\n",
    "test.batch(1, drop_remainder = False)\n",
    "\n",
    "START = data.first.w2idx[\"<START>\"]\n",
    "END = data.first.w2idx[\"<END>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_lengths, bins='auto', log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_tensor(s, w2idx):\n",
    "    s = [w2idx.get(w, w2idx[\"<UNK>\"]) for w in s.split(\" \")]\n",
    "    x = tf.convert_to_tensor(s)\n",
    "    return x\n",
    "\n",
    "def tensor_to_sentence(x, idx2w):\n",
    "    return \" \".join(idx2w[i] for i in x)\n",
    "\n",
    "def translate(A, B, x, B_idx2w, eps=0.0001, return_pb = False):\n",
    "    x = tf.expand_dims(x, 0)\n",
    "    \n",
    "    pb = A.recognize(x, eps = eps)[0]\n",
    "    y = B.generate(pb, max_length = round(1.5 * len(x[0])), start=START, end=END)\n",
    "    \n",
    "    s = tensor_to_sentence(y, B_idx2w)\n",
    "    return (s, pb[0]) if return_pb else s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "units = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "recog_lr = 1\n",
    "num_PB = 32\n",
    "gradient_clip = 1.0\n",
    "\n",
    "warmup_epochs = 0\n",
    "reset_pbs_every = 0 # epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = RNNPB(vocab_size=data.first.vocab_size, embedding_size=embedding_size, units=units, num_layers=num_layers, num_PB=num_PB, num_sequences=num_seqs, recog_lr = recog_lr, gradient_clip=gradient_clip, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "test_iter = iter(test)\n",
    "for epoch in range(epochs):\n",
    "    print(\"====== EPOCH {} ======\".format(epoch+1))\n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    if epoch != 0 and reset_pbs_every != 0 and epoch % reset_pbs_every == 0:\n",
    "        A.reset_pbs()\n",
    "        B.reset_pbs()\n",
    "    \n",
    "    for (batch, (A_inp, A_lengths, ids)) in enumerate(train):\n",
    "        \n",
    "        max_A_length = np.max(A_lengths)\n",
    "        A_inp = A_inp[:, :max_A_length]\n",
    "         \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            A_inputs = A_inp[:, :-1]\n",
    "            A_targets = A_inp[:, 1:]\n",
    "            \n",
    "            A_outputs, _, A_pb = A(A_inputs, ids)\n",
    "         \n",
    "            A_mask = 1 - np.equal(A_targets, 0)\n",
    "            A_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = A_targets, logits=A_outputs) * A_mask\n",
    "            \n",
    "            batch_loss = tf.reduce_mean(A_loss)\n",
    "                    \n",
    "        if epoch < warmup_epochs:\n",
    "            variables = A.variables_expt_pb()\n",
    "        else:\n",
    "            variables = A.variables\n",
    "        \n",
    "        gradients = tape.gradient(batch_loss, variables)\n",
    "        \n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, gradient_clip)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        epoch_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, batch, batch_loss.numpy()))\n",
    "    print('Epoch loss {:.8f}'.format(epoch_loss.numpy()))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    if epoch >= warmup_epochs and epoch % 5 == 0:\n",
    "        print(\"Prediction test\")\n",
    "        A_preds = tf.argmax(A_outputs, axis = 2)[0, :A_lengths[0]-1].numpy()\n",
    "        print(tensor_to_sentence(A_inp[0, :A_lengths[0]].numpy(), data.first.idx2w), \"   =>   \", end=\"\")\n",
    "        print(\"<START> \" + tensor_to_sentence(A_preds, data.first.idx2w))\n",
    "\n",
    "        print(\"\\nPB stats:\")\n",
    "        A.print_pb_stats()\n",
    "\n",
    "        print(\"\\nAutoencoding test\")\n",
    "        \n",
    "        x, x_l, _ = next(test_iter)\n",
    "        x = x[:x_l].numpy()\n",
    "        \n",
    "        trans = translate(A, A, x, data.first.idx2w)\n",
    "        \n",
    "        print(tensor_to_sentence(x, data.first.idx2w), \"   =>   \", trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"<START> What exactly did you do ? <END>\"\n",
    "x = sentence_to_tensor(s, en_w2idx)\n",
    "\n",
    "print(translate(A, A, x, en_idx2w))\n",
    "print(translate(A, B, x, de_idx2w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipy-jupyter-venv",
   "language": "python",
   "name": "myipy_jupter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
